# This file contains the manifest of SparkSQL functions provided by Glow. The definitions in this
# file are used to generate Scala and Python APIs.
#
# The top level names are groups of functions. These groups primarily matter for documentation. Each
# group contains an ordered list of functions. Most of the parameters are self-evident. 
#
# Potentially confusing function options are:
# - expr_class: The class name of the expression in Glow that implements the function. This value is
#               *not* user-visible. It's used only for client generation.
# - exclude_python: If true, the function cannot be represented in Python. This is currently only
#                   the case for functions that take lambda functions as arguments.
# 
# Potentially confusing argument options are:
# - type: The argument's type. This can currently be one of [str, double, lambda1, lambda2]. These
#         types do not correspond to any real programming language; they are just placeholders used for
#         client generation. More types can be added as needed. If an argument does not have an explicit
#         type, it is assumed to be of Spark SQL Column type.
# - is_var_args: If true, the parameters can be repeated. Only the last argument in the args list
#                can be var args.
# - is_optional: If true, the parameter can be omitted. Only the last argument in the args list can
#                 be optional.
#
# For more details about how the definitions in this file are eventually turned into clients,
# consult the generator script at project/render_template.py.
complex_type_manipulation:
  functions:
    - name: add_struct_fields
      doc: Adds fields to a struct.
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1))])
          >>> df.select(glow.add_struct_fields('struct', lit('b'), lit(2)).alias('struct')).collect()
          [Row(struct=Row(a=1, b=2))]
      expr_class: io.projectglow.sql.expressions.AddStructFields
      args:
        - name: struct
          doc: The struct to which fields will be added.
        - name: fields
          doc: The new fields to add. The arguments must alternate between string-typed literal field names and field values.
          is_var_args: true
    - name: array_summary_stats
      doc: Computes the min, max, mean, stddev for an array of numerics. The input array can be any numeric
        type, but the returned stats are always ``double`` typed.
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(arr=[1, 2, 3])])
          >>> df.select(glow.expand_struct(glow.array_summary_stats('arr'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.ArrayStatsSummary
      args:
        - name: arr
          doc: The array of numerics
    - name: array_to_dense_vector
      doc: Converts an array of numerics into a ``spark.ml`` ``DenseVector``.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.ArrayToDenseVector
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseVector
          >>> df = spark.createDataFrame([Row(arr=[1, 2, 3])])
          >>> df.select(glow.array_to_dense_vector('arr').alias('v')).collect()
          [Row(v=DenseVector([1.0, 2.0, 3.0]))]
      args:
        - name: arr
          doc: The array of numerics
    - name: array_to_sparse_vector
      doc: Converts an array of numerics into a ``spark.ml`` ``SparseVector``.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.ArrayToSparseVector
      examples:
        python: |
          >>> from pyspark.ml.linalg import SparseVector
          >>> df = spark.createDataFrame([Row(arr=[1, 0, 2, 0, 3, 0])])
          >>> df.select(glow.array_to_sparse_vector('arr').alias('v')).collect()
          [Row(v=SparseVector(6, {0: 1.0, 2: 2.0, 4: 3.0}))]
      args:
        - name: arr
          doc: The array of numerics
    - name: expand_struct
      doc: Promotes fields of a nested struct to top-level columns. Similar to using struct.* from SQL, but can be used in more contexts.
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1, b=2))])
          >>> df.select(glow.expand_struct(col('struct'))).collect()
          [Row(a=1, b=2)]
      expr_class: io.projectglow.sql.expressions.ExpandStruct
      args:
        - name: struct
          doc: The struct to expand
    - name: explode_matrix
      doc: Explodes a ``spark.ml`` matrix (sparse or dense) into multiple arrays, one per row of the matrix.
      since: 0.3.0
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> m = DenseMatrix(numRows=3, numCols=2, values=[1, 2, 3, 4, 5, 6])
          >>> df = spark.createDataFrame([Row(matrix=m)])
          >>> df.select(glow.explode_matrix('matrix').alias('row')).collect()
          [Row(row=[1.0, 4.0]), Row(row=[2.0, 5.0]), Row(row=[3.0, 6.0])]
      expr_class: io.projectglow.sql.expressions.ExplodeMatrix
      args:
        - name: matrix
          doc: The matrix to explode
    - name: subset_struct
      doc: Selects fields from a struct. The return value will be a struct containing only the desired fields.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SubsetStruct
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1, b=2, c=3))])
          >>> df.select(glow.subset_struct('struct', 'a', 'c').alias('struct')).collect()
          [Row(struct=Row(a=1, c=3))]
      args:
        - name: struct
          doc: Struct from which to select fields
        - name: fields
          doc: Fields to select
          type: str
          is_var_args: true
    - name: vector_to_array
      doc: Converts a ``spark.ml`` vector (sparse or dense) to an array of doubles.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.VectorToArray
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseVector, SparseVector
          >>> df = spark.createDataFrame([Row(v=SparseVector(3, {0: 1.0, 2: 2.0})), Row(v=DenseVector([3.0, 4.0]))])
          >>> df.select(glow.vector_to_array('v').alias('arr')).collect()
          [Row(arr=[1.0, 0.0, 2.0]), Row(arr=[3.0, 4.0])]
      args:
        - name: vector
          doc: Vector to convert

etl:
  functions:
    - name: hard_calls
      doc: Converts an array of probabilities to hard calls. See :ref:`variant-data-transformations` 
        for more details. The probabilites are assumed to be diploid.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.HardCalls
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(probs=[0.95, 0.05, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False)).alias('calls')).collect()
          [Row(calls=[0, 0])]
          >>> df = spark.createDataFrame([Row(probs=[0.05, 0.95, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False)).alias('calls')).collect()
          [Row(calls=[1, 0])]
          >>> # Use the threshold parameter to change the minimum probability required for a call
          >>> df = spark.createDataFrame([Row(probs=[0.05, 0.95, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False), threshold=0.99).alias('calls')).collect()
          [Row(calls=[-1, -1])]
      args:
        - name: probabilities
          doc: The array of probabilities to convert
        - name: numAlts
          doc: The number of alternate alleles
        - name: phased
          doc: Whether the probabilities are phased. If phased, we expect one 2 * numAlts values 
            in probabilities array. If unphased, we expect one probability per possible genotype.
        - name: threshold
          doc: The minimum probability to make a call. If no probability falls into the range 
            of [0, 1 - threshold] or [threshold, 1], a no-call (represented by -1s) will be emitted. 
            If not provided, this parameter defaults to 0.9.
          type: double
          is_optional: true
    - name: lift_over_coordinates
      doc: Performs lift over for the coordinates of a variants. To lift over alleles and add additional metadata, see :ref:`liftover`.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LiftOverCoordinatesExpr
      examples:
        python: |
          >>> df = spark.read.format('vcf').load('test-data/liftover/unlifted.test.vcf').where('start = 18210071')
          >>> chain_file = 'test-data/liftover/hg38ToHg19.over.chain.gz'
          >>> reference_file = 'test-data/liftover/hg19.chr20.fa.gz'
          >>> df.select('contigName', 'start', 'end').head()
          Row(contigName='chr20', start=18210071, end=18210072)
          >>> lifted_df = df.select(glow.expand_struct(glow.lift_over_coordinates('contigName', 'start', 'end', chain_file)))
          >>> lifted_df.head()
          Row(contigName='chr20', start=18190715, end=18190716)
      args:
        - name: contigName
          doc: The current contigName
        - name: start
          doc: The current start
        - name: end
          doc: The current end
        - name: chainFile
          doc: Location of the chain file on each node in the cluster
          type: str
        - name: minMatchRatio
          doc: Minimum fraction of bases that must remap to lift over successfully. If not provided, defaults to 0.95.
          type: double
          is_optional: true
    - name: normalize_variant
      doc: |
             Normalize the variant with a behavior similar to vt normalize or bcftools norm.
             Creates a StructType column including the normalized start, end, referenceAllele and
             alternateAlleles fields (whether they are changed or unchanged as the result of
             normalization) as well as a StructType field called normalizationStatus that
             contains the following fields:

             changed: A boolean field indicating whether the variant data was changed as a
                 result of normalization.

             errorMessage: An error message in case the attempt at normalizing the row hit an
                 error. In this case, the changed field will be set to false. If no errors occur,
                 this field will be null.

             In case of an error, the start, end, referenceAllele and alternateAlleles fields in
             the generated struct will be null.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.NormalizeVariantExpr
      examples:
        python: |
          >>> df = spark.read.format('vcf').load('test-data/variantsplitternormalizer-test/test_left_align_hg38_altered.vcf')
          >>> ref_genome = 'test-data/variantsplitternormalizer-test/Homo_sapiens_assembly38.20.21_altered.fasta'
          >>> df.select('contigName', 'start', 'end', 'referenceAllele', 'alternateAlleles').head()
          Row(contigName='chr20', start=400, end=401, referenceAllele='G', alternateAlleles=['GATCTTCCCTCTTTTCTAATATAAACACATAAAGCTCTGTTTCCTTCTAGGTAACTGGTTTGAG'])
          >>> normalized_df = df.select('contigName', glow.expand_struct(glow.normalize_variant('contigName', 'start', 'end', 'referenceAllele', 'alternateAlleles', ref_genome)))
          >>> normalized_df.head()
          Row(contigName='chr20', start=268, end=269, referenceAllele='A', alternateAlleles=['ATTTGAGATCTTCCCTCTTTTCTAATATAAACACATAAAGCTCTGTTTCCTTCTAGGTAACTGG'], normalizationStatus=Row(changed=True, errorMessage=None))
      args:
        - name: contigName
          doc: The current contigName
        - name: start
          doc: The current start
        - name: end
          doc: The current end
        - name: refAllele
          doc: Reference allele
        - name: altAlleles
          doc: Alternate alleles
        - name: refGenomePathString
          doc: |
            A path to the reference genome .fasta file. The .fasta file must
            be accompanied with a .fai index file in the same folder.
          type: str


quality_control:
  functions:
    - name: aggregate_by_index
      doc: Computes custom per-sample aggregates
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.UnwrappedAggregateByIndex
      exclude_python: true
      args:
        - name: arr
          doc: array of values.
        - name: initialValue
          doc: the initial value
        - name: update
          doc: update function
          type: lambda2
        - name: merge
          doc: merge function
          type: lambda2
        - name: evaluate
          doc: evaluate function
          type: lambda1
          is_optional: true
    - name: call_summary_stats
      doc: Computes call summary statistics for an array of genotype structs. See :ref:`variant-qc` for more details.
      since: 0.3.0
      examples:
        python: |
          >>> schema = 'genotypes: array<struct<calls: array<int>>>'
          >>> df = spark.createDataFrame([Row(genotypes=[Row(calls=[0, 0]), Row(calls=[1, 0]), Row(calls=[1, 1])])], schema)
          >>> df.select(glow.expand_struct(glow.call_summary_stats('genotypes'))).collect()
          [Row(callRate=1.0, nCalled=3, nUncalled=0, nHet=1, nHomozygous=[1, 1], nNonRef=2, nAllelesCalled=6, alleleCounts=[3, 3], alleleFrequencies=[0.5, 0.5])]
      expr_class: io.projectglow.sql.expressions.CallStats
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: dp_summary_stats
      doc: Compute summary statistics for depth field from array of genotype structs
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(genotypes=[Row(depth=1), Row(depth=2), Row(depth=3)])], 'genotypes: array<struct<depth: int>>')
          >>> df.select(glow.expand_struct(glow.dp_summary_stats('genotypes'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.DpSummaryStats
      since: 0.3.0
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: hardy_weinberg
      doc: Computes statistics relating to the Hardy Weinberg equilibrium. See :ref:`variant-qc` for more details.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.HardyWeinberg
      examples:
        python: |
          >>> genotypes = [
          ... Row(calls=[1, 1]),
          ... Row(calls=[1, 0]),
          ... Row(calls=[0, 0])]
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<calls: array<int>>>')
          >>> df.select(glow.expand_struct(glow.hardy_weinberg('genotypes'))).collect()
          [Row(hetFreqHwe=0.6, pValueHwe=0.7)]
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: gq_summary_stats
      doc: Computes summary statistics about the genotype quality field for an array of genotype structs
      since: 0.3.0
      examples:
        python: |
          >>> genotypes = [
          ... Row(conditionalQuality=1), 
          ... Row(conditionalQuality=2), 
          ... Row(conditionalQuality=3)] 
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<conditionalQuality: int>>')
          >>> df.select(glow.expand_struct(glow.gq_summary_stats('genotypes'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.GqSummaryStats
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: sample_call_summary_stats
      doc: Computes per-sample call summary statistics. See :ref:`sample-qc` for more details.
      since: 0.3.0
      examples:
        python: |
          >>> sites = [
          ... Row(refAllele='C', alternateAlleles=['G'], genotypes=[Row(sampleId='NA12878', calls=[0, 0])]),
          ... Row(refAllele='A', alternateAlleles=['G'], genotypes=[Row(sampleId='NA12878', calls=[1, 1])]),
          ... Row(refAllele='AG', alternateAlleles=['A'], genotypes=[Row(sampleId='NA12878', calls=[1, 0])])]
          >>> df = spark.createDataFrame(sites, 'alternateAlleles: array<string>, genotypes: array<struct<sampleId: string, calls: array<int>>>, refAllele: string')
          >>> df.select(glow.sample_call_summary_stats('genotypes', 'refAllele', 'alternateAlleles').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', callRate=1.0, nCalled=3, nUncalled=0, nHomRef=1, nHet=1, nHomVar=1, nSnp=2, nInsertion=0, nDeletion=1, nTransition=2, nTransversion=0, nSpanningDeletion=0, rTiTv=inf, rInsertionDeletion=0.0, rHetHomVar=1.0)])]
      expr_class: io.projectglow.sql.expressions.CallSummaryStats
      args: 
        - name: genotypes
          doc: The array of genotype structs
        - name: refAllele
          doc: The reference allele
        - name: alternateAlleles
          doc: An array of alternate alleles
    - name: sample_dp_summary_stats
      doc: Compute per-sample summary statistics about the depth field in an array of genotype structs
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SampleDpSummaryStatistics
      examples:
        python: |
          >>> sites = [
          ... Row(genotypes=[Row(sampleId='NA12878', depth=1)]),
          ... Row(genotypes=[Row(sampleId='NA12878', depth=2)]),
          ... Row(genotypes=[Row(sampleId='NA12878', depth=3)])]
          >>> df = spark.createDataFrame(sites, 'genotypes: array<struct<depth: int, sampleId: string>>')
          >>> df.select(glow.sample_dp_summary_stats('genotypes').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', mean=2.0, stdDev=1.0, min=1.0, max=3.0)])]
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: sample_gq_summary_stats
      doc: Compute per-sample summary statistics about the genotype quality field in an array of genotype structs
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SampleGqSummaryStatistics
      examples:
        python: |
          >>> sites = [
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=1)]),
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=2)]),
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=3)])]
          >>> df = spark.createDataFrame(sites, 'genotypes: array<struct<conditionalQuality: int, sampleId: string>>')
          >>> df.select(glow.sample_gq_summary_stats('genotypes').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', mean=2.0, stdDev=1.0, min=1.0, max=3.0)])]
      args:
        - name: genotypes
          doc: The array of genotype structs

gwas_functions:
  functions:
    - name: linear_regression_gwas
      doc: Performs a linear regression association test optimized for performance in a GWAS setting.
        See :ref:`linear-regression` for details.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LinearRegressionExpr
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> phenotypes = [2, 3, 4]
          >>> genotypes = [0, 1, 2]
          >>> covariates = DenseMatrix(numRows=3, numCols=1, values=[1, 1, 1])
          >>> df = spark.createDataFrame([Row(genotypes=genotypes, phenotypes=phenotypes, covariates=covariates)])
          >>> df.select(glow.expand_struct(glow.linear_regression_gwas('genotypes', 'phenotypes', 'covariates'))).collect()
          [Row(beta=0.9999999999999998, standardError=1.4901161193847656e-08, pValue=9.486373847239922e-09)]
      args: 
        - name: genotypes
          doc: An array of genotypes
        - name: phenotypes
          doc: An array of phenotypes
        - name: covariates
          doc: A spark.ml matrix of covariates
    - name: logistic_regression_gwas
      doc: Performs a logistic regression association test optimized for performance in a GWAS setting.
        See :ref:`logistic-regression` for more details.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LogisticRegressionExpr
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> phenotypes = [1, 0, 0, 1, 1]
          >>> genotypes = [0, 0, 1, 2, 2]
          >>> covariates = DenseMatrix(numRows=5, numCols=1, values=[1, 1, 1, 1, 1])
          >>> df = spark.createDataFrame([Row(genotypes=genotypes, phenotypes=phenotypes, covariates=covariates)])
          >>> df.select(glow.expand_struct(glow.logistic_regression_gwas('genotypes', 'phenotypes', 'covariates', 'Firth'))).collect()
          [Row(beta=0.7418937644793101, oddsRatio=2.09990848346903, waldConfidenceInterval=[0.2509874689201784, 17.569066925598555], pValue=0.3952193664793294)]
          >>> df.select(glow.expand_struct(glow.logistic_regression_gwas('genotypes', 'phenotypes', 'covariates', 'LRT'))).collect()
          [Row(beta=1.1658962684583645, oddsRatio=3.208797540870915, waldConfidenceInterval=[0.2970960052553798, 34.65674891673014], pValue=0.2943946848756771)]
      args:
        - name: genotypes
          doc: An array of genotype structs
        - name: phenotypes
          doc: A ``double`` array of phenotype values
        - name: covariates
          doc: M ``spark.ml`` matrix of covariates
        - name: test
          doc: Which logistic regression test to use. Can be 'LRT' or 'Firth'
          type: str
    - name: genotype_states
      doc: Gets number of alt alleles for a genotype. Returns ``-1`` if there are any ``-1``s in the input array.
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.GenotypeStates
      examples:
        python: |
          >>> genotypes = [
          ... Row(calls=[1, 1]),
          ... Row(calls=[1, 0]),
          ... Row(calls=[0, 0]),
          ... Row(calls=[-1, -1])]
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<calls: array<int>>>')
          >>> df.select(glow.genotype_states('genotypes').alias('states')).collect()
          [Row(states=[2, 1, 0, -1])]
      args:
        - name: genotypes
          doc: An array of genotype structs
