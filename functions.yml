# This file contains the manifest of SparkSQL functions provided by Glow. The definitions in this
# file are used to generate Scala and Python APIs.
#
# The top level names are groups of functions. These groups primarily matter for documentation. Each
# group contains an ordered list of functions. Most of the parameters are self-evident. 
#
# Potentially confusing function options are:
# - expr_class: The class name of the expression in Glow that implements the function. This value is
#               *not* user-visible. It's used only for client generation.
# - exclude_python: If true, the function cannot be represented in Python. This is currently only
#                   the case for functions that take lambda functions as arguments.
# 
# Potentially confusing argument options are:
# - type: The argument's type. This can currently be one of [str, double, lambda1, lambda2]. These
#         types do not correspond to any real programming language; they are just placeholders used for
#         client generation. More types can be added as needed. If an argument does not have an explicit
#         type, it is assumed to be of Spark SQL Column type.
# - is_var_args: If true, the parameters can be repeated. Only the last argument in the args list
#                can be var args.
# - is_optional: If true, the parameter can be omitted. Only the last argument in the args list can
#                 be optional.
#
# For more details about how the definitions in this file are eventually turned into clients,
# consult the generator script at project/render_template.py.
complex_type_manipulation:
  functions:
    - name: add_struct_fields
      doc: Add fields to a struct
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1))])
          >>> df.select(glow.add_struct_fields('struct', lit('b'), lit(2)).alias('struct')).collect()
          [Row(struct=Row(a=1, b=2))]
      expr_class: io.projectglow.sql.expressions.AddStructFields
      args:
        - name: struct
          doc: The struct to which fields will be added
        - name: fields
          doc: New fields
          is_var_args: true
    - name: array_summary_stats
      doc: Compute the min, max, mean, stddev for an array of numerics
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(arr=[1, 2, 3])])
          >>> df.select(glow.expand_struct(glow.array_summary_stats('arr'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.ArrayStatsSummary
      args:
        - name: arr
          doc: The array of numerics
    - name: array_to_dense_vector
      doc: Convert an array of numerics into a spark.ml DenseVector
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.ArrayToDenseVector
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseVector
          >>> df = spark.createDataFrame([Row(arr=[1, 2, 3])])
          >>> df.select(glow.array_to_dense_vector('arr').alias('v')).collect()
          [Row(v=DenseVector([1.0, 2.0, 3.0]))]
      args:
        - name: arr
          doc: The array of numerics
    - name: array_to_sparse_vector
      doc: Convert an array of numerics into a spark.ml SparseVector
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.ArrayToSparseVector
      examples:
        python: |
          >>> from pyspark.ml.linalg import SparseVector
          >>> df = spark.createDataFrame([Row(arr=[1, 0, 2, 0, 3, 0])])
          >>> df.select(glow.array_to_sparse_vector('arr').alias('v')).collect()
          [Row(v=SparseVector(6, {0: 1.0, 2: 2.0, 4: 3.0}))]
      args:
        - name: arr
          doc: The array of numerics
    - name: expand_struct
      doc: Promote fields of a nested struct to top-level columns. Similar to using struct.* from SQL, but can be used in more contexts.
      since: 0.3.0
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1, b=2))])
          >>> df.select(glow.expand_struct(col('struct'))).collect()
          [Row(a=1, b=2)]
      expr_class: io.projectglow.sql.expressions.ExpandStruct
      args:
        - name: struct
          doc: The struct to expand
    - name: explode_matrix
      doc: Explode a spark.ml Matrix into arrays of rows
      since: 0.3.0
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> m = DenseMatrix(numRows=3, numCols=2, values=[1, 2, 3, 4, 5, 6])
          >>> df = spark.createDataFrame([Row(matrix=m)])
          >>> df.select(glow.explode_matrix('matrix').alias('row')).collect()
          [Row(row=[1.0, 4.0]), Row(row=[2.0, 5.0]), Row(row=[3.0, 6.0])]
      expr_class: io.projectglow.sql.expressions.ExplodeMatrix
      args:
        - name: matrix
          doc: The matrix to explode
    - name: subset_struct
      doc: Select fields from a struct
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SubsetStruct
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(struct=Row(a=1, b=2, c=3))])
          >>> df.select(glow.subset_struct('struct', 'a', 'c').alias('struct')).collect()
          [Row(struct=Row(a=1, c=3))]
      args:
        - name: struct
          doc: Struct from which to select fields
        - name: fields
          doc: Fields to take
          type: str
          is_var_args: true
    - name: vector_to_array
      doc: Convert a spark.ml vector (sparse or dense) to an array of doubles
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.VectorToArray
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseVector, SparseVector
          >>> df = spark.createDataFrame([Row(v=SparseVector(3, {0: 1.0, 2: 2.0})), Row(v=DenseVector([3.0, 4.0]))])
          >>> df.select(glow.vector_to_array('v').alias('arr')).collect()
          [Row(arr=[1.0, 0.0, 2.0]), Row(arr=[3.0, 4.0])]
      args:
        - name: vector
          doc: Vector to convert

etl:
  functions:
    - name: hard_calls
      doc: Converts an array of probabilities to hard calls
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.HardCalls
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(probs=[0.95, 0.05, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False)).alias('calls')).collect()
          [Row(calls=[0, 0])]
          >>> df = spark.createDataFrame([Row(probs=[0.05, 0.95, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False)).alias('calls')).collect()
          [Row(calls=[1, 0])]
          >>> # Use the threshold parameter to change the minimum probability required for a call
          >>> df = spark.createDataFrame([Row(probs=[0.05, 0.95, 0.0])])
          >>> df.select(glow.hard_calls('probs', numAlts=lit(1), phased=lit(False), threshold=0.99).alias('calls')).collect()
          [Row(calls=[-1, -1])]
      args:
        - name: probabilities
          doc: Probabilities
        - name: numAlts
          doc: The number of alts
        - name: phased
          doc: Whether the probabilities are phased or not
        - name: threshold
          doc: The minimum probability to include
          type: double
          is_optional: true
    - name: lift_over_coordinates
      doc: Do liftover like Picard
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LiftOverCoordinatesExpr
      examples:
        python: |
          >>> df = spark.read.format('vcf').load('test-data/liftover/unlifted.test.vcf').where('start = 18210071')
          >>> chain_file = 'test-data/liftover/hg38ToHg19.over.chain.gz'
          >>> reference_file = 'test-data/liftover/hg19.chr20.fa.gz'
          >>> df.select('contigName', 'start', 'end').head()
          Row(contigName='chr20', start=18210071, end=18210072)
          >>> lifted_df = df.select(glow.expand_struct(glow.lift_over_coordinates('contigName', 'start', 'end', chain_file)))
          >>> lifted_df.head()
          Row(contigName='chr20', start=18190715, end=18190716)
      args:
        - name: contigName
          doc: The current contigName
        - name: start
          doc: The current start
        - name: end
          doc: The current end
        - name: chainFile
          doc: Location of the chain file on each node in the cluster
          type: str
        - name: minMatchRatio
          doc: Minimum fraction of bases that must remap to lift over successfully
          type: double
          is_optional: true
          

quality_control:
  functions:
    - name: aggregate_by_index
      doc: Compute custom per-sample aggregates
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.UnwrappedAggregateByIndex
      exclude_python: true
      args:
        - name: arr
          doc: array of values.
        - name: initialValue
          doc: the initial value
        - name: update
          doc: update function
          type: lambda2
        - name: merge
          doc: merge function
          type: lambda2
        - name: evaluate
          doc: evaluate function
          type: lambda1
          is_optional: true
    - name: call_summary_stats
      doc: Compute call stats for an array of genotype structs
      since: 0.3.0
      examples:
        python: |
          >>> schema = 'genotypes: array<struct<calls: array<int>>>'
          >>> df = spark.createDataFrame([Row(genotypes=[Row(calls=[0, 0]), Row(calls=[1, 0]), Row(calls=[1, 1])])], schema)
          >>> df.select(glow.expand_struct(glow.call_summary_stats('genotypes'))).collect()
          [Row(callRate=1.0, nCalled=3, nUncalled=0, nHet=1, nHomozygous=[1, 1], nNonRef=2, nAllelesCalled=6, alleleCounts=[3, 3], alleleFrequencies=[0.5, 0.5])]
      expr_class: io.projectglow.sql.expressions.CallStats
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: dp_summary_stats
      doc: Compute summary statistics for depth field from array of genotype structs
      examples:
        python: |
          >>> df = spark.createDataFrame([Row(genotypes=[Row(depth=1), Row(depth=2), Row(depth=3)])], 'genotypes: array<struct<depth: int>>')
          >>> df.select(glow.expand_struct(glow.dp_summary_stats('genotypes'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.DpSummaryStats
      since: 0.3.0
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: hardy_weinberg
      doc: Compute statistics relating to the Hardy Weinberg equilibrium
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.HardyWeinberg
      examples:
        python: |
          >>> genotypes = [
          ... Row(calls=[1, 1]),
          ... Row(calls=[1, 0]),
          ... Row(calls=[0, 0])]
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<calls: array<int>>>')
          >>> df.select(glow.expand_struct(glow.hardy_weinberg('genotypes'))).collect()
          [Row(hetFreqHwe=0.6, pValueHwe=0.7)]
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: gq_summary_stats
      doc: Compute summary statistics about the genotype quality field for an array of genotype structs
      since: 0.3.0
      examples:
        python: |
          >>> genotypes = [
          ... Row(conditionalQuality=1), 
          ... Row(conditionalQuality=2), 
          ... Row(conditionalQuality=3)] 
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<conditionalQuality: int>>')
          >>> df.select(glow.expand_struct(glow.gq_summary_stats('genotypes'))).collect()
          [Row(mean=2.0, stdDev=1.0, min=1.0, max=3.0)]
      expr_class: io.projectglow.sql.expressions.GqSummaryStats
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: sample_call_summary_stats
      doc: Compute per-sample call stats
      since: 0.3.0
      examples:
        python: |
          >>> sites = [
          ... Row(refAllele='C', alternateAlleles=['G'], genotypes=[Row(sampleId='NA12878', calls=[0, 0])]),
          ... Row(refAllele='A', alternateAlleles=['G'], genotypes=[Row(sampleId='NA12878', calls=[1, 1])]),
          ... Row(refAllele='AG', alternateAlleles=['A'], genotypes=[Row(sampleId='NA12878', calls=[1, 0])])]
          >>> df = spark.createDataFrame(sites, 'alternateAlleles: array<string>, genotypes: array<struct<sampleId: string, calls: array<int>>>, refAllele: string')
          >>> df.select(glow.sample_call_summary_stats('genotypes', 'refAllele', 'alternateAlleles').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', callRate=1.0, nCalled=3, nUncalled=0, nHomRef=1, nHet=1, nHomVar=1, nSnp=2, nInsertion=0, nDeletion=1, nTransition=2, nTransversion=0, nSpanningDeletion=0, rTiTv=inf, rInsertionDeletion=0.0, rHetHomVar=1.0)])]
      expr_class: io.projectglow.sql.expressions.CallSummaryStats
      args: 
        - name: genotypes
          doc: The array of genotype structs
        - name: refAllele
          doc: The reference allele
        - name: alternateAlleles
          doc: An array of alternate alleles
    - name: sample_dp_summary_stats
      doc: Compute per-sample summary statistics about the depth field in an array of genotype structs
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SampleDpSummaryStatistics
      examples:
        python: |
          >>> sites = [
          ... Row(genotypes=[Row(sampleId='NA12878', depth=1)]),
          ... Row(genotypes=[Row(sampleId='NA12878', depth=2)]),
          ... Row(genotypes=[Row(sampleId='NA12878', depth=3)])]
          >>> df = spark.createDataFrame(sites, 'genotypes: array<struct<depth: int, sampleId: string>>')
          >>> df.select(glow.sample_dp_summary_stats('genotypes').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', mean=2.0, stdDev=1.0, min=1.0, max=3.0)])]
      args:
        - name: genotypes
          doc: The array of genotype structs
    - name: sample_gq_summary_stats
      doc: Compute per-sample summary statistics about the genotype quality field in an array of genotype structs
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.SampleGqSummaryStatistics
      examples:
        python: |
          >>> sites = [
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=1)]),
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=2)]),
          ... Row(genotypes=[Row(sampleId='NA12878', conditionalQuality=3)])]
          >>> df = spark.createDataFrame(sites, 'genotypes: array<struct<conditionalQuality: int, sampleId: string>>')
          >>> df.select(glow.sample_gq_summary_stats('genotypes').alias('stats')).collect()
          [Row(stats=[Row(sampleId='NA12878', mean=2.0, stdDev=1.0, min=1.0, max=3.0)])]
      args:
        - name: genotypes
          doc: The array of genotype structs

gwas_functions:
  functions:
    - name: linear_regression_gwas
      doc: A linear regression GWAS function
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LinearRegressionExpr
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> phenotypes = [2, 3, 4]
          >>> genotypes = [0, 1, 2]
          >>> covariates = DenseMatrix(numRows=3, numCols=1, values=[1, 1, 1])
          >>> df = spark.createDataFrame([Row(genotypes=genotypes, phenotypes=phenotypes, covariates=covariates)])
          >>> df.select(glow.expand_struct(glow.linear_regression_gwas('genotypes', 'phenotypes', 'covariates'))).collect()
          [Row(beta=0.9999999999999998, standardError=1.4901161193847656e-08, pValue=9.486373847239922e-09)]
      args: 
        - name: genotypes
          doc: An array of genotypes
        - name: phenotypes
          doc: An array of phenotypes
        - name: covariates
          doc: A Spark matrix of covariates
    - name: logistic_regression_gwas
      doc: A logistic regression function
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.LogisticRegressionExpr
      examples:
        python: |
          >>> from pyspark.ml.linalg import DenseMatrix
          >>> phenotypes = [1, 0, 0, 1, 1]
          >>> genotypes = [0, 0, 1, 2, 2]
          >>> covariates = DenseMatrix(numRows=5, numCols=1, values=[1, 1, 1, 1, 1])
          >>> df = spark.createDataFrame([Row(genotypes=genotypes, phenotypes=phenotypes, covariates=covariates)])
          >>> df.select(glow.expand_struct(glow.logistic_regression_gwas('genotypes', 'phenotypes', 'covariates', 'Firth'))).collect()
          [Row(beta=0.7418937644793101, oddsRatio=2.09990848346903, waldConfidenceInterval=[0.2509874689201784, 17.569066925598555], pValue=0.3952193664793294)]
          >>> df.select(glow.expand_struct(glow.logistic_regression_gwas('genotypes', 'phenotypes', 'covariates', 'LRT'))).collect()
          [Row(beta=1.1658962684583645, oddsRatio=3.208797540870915, waldConfidenceInterval=[0.2970960052553798, 34.65674891673014], pValue=0.2943946848756771)]
      args:
        - name: genotypes
          doc: An array of genotypes
        - name: phenotypes
          doc: An array of phenotype values
        - name: covariates
          doc: a matrix of covariates
        - name: test
          doc: Which logistic regression test to use. Can be 'LRT' or 'Firth'
          type: str
    - name: genotype_states
      doc: Get number of alt alleles for a genotype
      since: 0.3.0
      expr_class: io.projectglow.sql.expressions.GenotypeStates
      examples:
        python: |
          >>> genotypes = [
          ... Row(calls=[1, 1]),
          ... Row(calls=[1, 0]),
          ... Row(calls=[0, 0]),
          ... Row(calls=[-1, -1])]
          >>> df = spark.createDataFrame([Row(genotypes=genotypes)], 'genotypes: array<struct<calls: array<int>>>')
          >>> df.select(glow.genotype_states('genotypes').alias('states')).collect()
          [Row(states=[2, 1, 0, -1])]
      args:
        - name: genotypes
          doc: An array of genotype structs
