setup_2xl_base: &setup_2xl_base
  working_directory: ~/glow
  docker:
    - image: cimg/openjdk:8.0.392
  resource_class: xlarge

setup_xl_base: &setup_xl_base
  working_directory: ~/glow
  docker:
    - image: cimg/openjdk:8.0.392
  resource_class: xlarge

setup_l_base: &setup_l_base
  working_directory: ~/glow
  docker:
    - image: cimg/openjdk:8.0.392
  resource_class: large

setup_m_base: &setup_m_base
  working_directory: ~/glow
  docker:
    - image: cimg/openjdk:8.0.392
  resource_class: medium+

install_conda: &install_conda
  run:
    name: Install conda
    command: |
      if [ ! -d "/home/circleci/conda" ]; then
        wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.11.0-2-Linux-x86_64.sh
        /bin/bash Miniconda3-py38_23.11.0-2-Linux-x86_64.sh -b -p $HOME/conda
      else
        echo "Conda already installed"
      fi
create_python_env: &create_python_env
  run:
    name: Create conda environment for Python dependencies
    command: |
      if [ ! -d "/home/circleci/conda/envs/glow" ]; then
        export PATH=$HOME/conda/bin:$PATH
        conda install -n base conda-libmamba-solver
        conda config --set solver libmamba
        conda env create -f python/environment.yml
      fi
create_docs_env: &create_docs_env
  run:
    name: Create minimal conda environment for docs dependencies
    command: |
      if [ ! -d "/home/circleci/conda/envs/glow-docs" ]; then
        export PATH=$HOME/conda/bin:$PATH
        conda env create -f docs/source/environment.yml
      fi
check_clean_repo: &check_clean_repo
  run:
    name: Verify that repo is clean
    command: |
      if [[ -n $(git status --short) ]]; then
        echo "Working directory was not clean!"
        git status
        false
      else
        true
      fi
version: 2.1
orbs:
  codecov: codecov/codecov@3.2.2
  jq: circleci/jq@2.2.0
jobs:

  check-docs:
    <<: *setup_m_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}-{{ checksum "docs/source/environment.yml" }}
      - *install_conda
      - *create_python_env
      - *create_docs_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}-{{ checksum "docs/source/environment.yml" }}
      - run:
          name: Check docs links
          command: |
            export PATH=$HOME/conda/envs/glow-docs/bin:$PATH
            cd docs
            make linkcheck
      - run:
          name: Configure Databricks CLI
          command: |
            printf "[docs-ci]\nhost = https://adb-984752964297111.11.azuredatabricks.net\ntoken = ${DATABRICKS_API_TOKEN}\njobs-api-version = 2.1\n" > ~/.databrickscfg
      - run:
          name: Generate notebook source files
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            for f in $(find docs/source/_static/notebooks -type f -name '*.html'); do
                python docs/dev/gen-nb-src.py --html "${f}" --cli-profile docs-ci
            done
      - *check_clean_repo

  spark-3-4-tests:
    <<: *setup_2xl_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Prepare CircleCI environment for dependency cache - issue #301
          environment:
          command: sudo apt-get install -y ca-certificates
      - run:
          name: Run Scala tests
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.4.1"
            export SCALA_VERSION="2.12.15"
            sbt core/test exit
      - run:
          name: Run docs tests
          no_output_timeout: 120m
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            pip install pyspark==3.4.1
            export SPARK_VERSION="3.4.1"
            export SCALA_VERSION="2.12.15"
            sbt docs/test exit
      - run:
          name: Run Python tests
          no_output_timeout: 90m
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.4.1"
            export SCALA_VERSION="2.12.15"
            sbt python/test exit
      - *check_clean_repo
      - store_artifacts:
          path: ~/glow/unit-tests.log
          destination: unit-tests.log
      - store_test_results:
          path: ~/glow/core/target/scala-2.12/test-reports

  spark-3-5-tests:
    <<: *setup_2xl_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Prepare CircleCI environment for dependency cache - issue #301
          environment:
          command: sudo apt-get install -y ca-certificates
      - run:
          name: Run Scala tests
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.5.0"
            export SCALA_VERSION="2.12.15"
            sbt core/test exit
      - run:
          name: Run docs tests
          no_output_timeout: 120m
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.5.0"
            export SCALA_VERSION="2.12.15"
            sbt docs/test exit
      - run:
          name: Run Python tests
          no_output_timeout: 90m
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.5.0"
            export SCALA_VERSION="2.12.15"
            sbt python/test exit
      - *check_clean_repo
      - store_artifacts:
          path: ~/glow/unit-tests.log
          destination: unit-tests.log
      - store_test_results:
          path: ~/glow/core/target/scala-2.12/test-reports

  all-notebook-tests:
    <<: *setup_2xl_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Configure Databricks CLI
          command: |
            printf "[docs-ci]\nhost = https://adb-984752964297111.11.azuredatabricks.net\ntoken = ${DATABRICKS_API_TOKEN}\njobs-api-version = 2.1\n" > ~/.databrickscfg
      - run:
          name: Run all notebook tests
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            if [[ ! -z $CIRCLE_PULL_REQUEST ]]; then
              echo 'export repo_url=$(curl -s https://api.github.com/repos/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/pulls/${CIRCLE_PR_NUMBER} | jq -r '.head.repo.html_url')'>> $BASH_ENV
              echo 'export pr_branch=$(curl -s https://api.github.com/repos/${CIRCLE_PROJECT_USERNAME}/${CIRCLE_PROJECT_REPONAME}/pulls/${CIRCLE_PR_NUMBER} | jq -r '.head.ref')' >> $BASH_ENV
              source $BASH_ENV
              python docs/dev/run-nb-test.py --cli-profile docs-ci \
                                             --repos-url $repo_url \
                                             --branch $pr_branch \
                                             --dockerhub_password ${PROJECTGLOW_DOCKERHUB_PASSWORD}
            else
              python docs/dev/run-nb-test.py --cli-profile docs-ci \
                                             --dockerhub_password ${PROJECTGLOW_DOCKERHUB_PASSWORD}
            fi
workflows:
  version: 2
  test:
    jobs:
      - check-docs
      - spark-3-4-tests
      - spark-3-5-tests
      - all-notebook-tests
  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - check-docs
      - spark-3-5-tests
      - all-notebook-tests