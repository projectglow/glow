setup_base: &setup_base
  working_directory: ~/glow
  docker:
    - image: circleci/openjdk:8

install_conda: &install_conda
  run:
    name: Install conda
    command: |
      if [ ! -d "/home/circleci/conda" ]; then
        wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh
        /bin/bash Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -p $HOME/conda
      else
        echo "Conda already installed"
      fi

create_python_env: &create_python_env
  run:
    name: Create conda environment for Python dependencies
    command: |
      if [ ! -d "/home/circleci/conda/envs/glow" ]; then
        export PATH=$HOME/conda/bin:$PATH
        conda env create -f python/environment.yml
      fi

create_docs_env: &create_docs_env
  run:
    name: Create minimal conda environment for docs dependencies
    command: |
      if [ ! -d "/home/circleci/conda/envs/glow-docs" ]; then
        export PATH=$HOME/conda/bin:$PATH
        conda env create -f docs/source/environment.yml
      fi

install_pyspark2: &install_pyspark2
  run: # Evict PySpark 3.0.0 in favor of PySpark 2.4.5
    name: Install PySpark 2.4.5
    command: |
      export PATH=$HOME/conda/bin:$PATH
      source activate glow
      conda remove -y pyspark
      pip install pyspark==2.4.5


check_clean_repo: &check_clean_repo
  run:
    name: Verify that repo is clean
    environment:
    command: |
      if [[ -n $(git status --short) ]]; then
        echo "Working directory was not clean!"
        git status
        false
      else
        true
      fi


version: 2.1
orbs:
  codecov: codecov/codecov@1.0.5
jobs:

  check-docs:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}-{{ checksum "docs/source/environment.yml" }}
      - *install_conda
      - *create_python_env
      - *create_docs_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}-{{ checksum "docs/source/environment.yml" }}
      - run:
          name: Check docs links
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow-docs/bin:$PATH
            cd docs
            make linkcheck
      - run:
          name: Configure Databricks CLI
          command: |
            printf "[docs-ci]\nhost = https://westus2.azuredatabricks.net\ntoken = ${DATABRICKS_API_TOKEN}\n" > ~/.databrickscfg
      - run:
          name: Generate notebook source files
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            for f in $(find docs/source/_static/notebooks -type f -name '*.html'); do
                python docs/dev/gen-nb-src.py --html "${f}" --cli-profile docs-ci
            done
      - *check_clean_repo

  scala-2_11-tests:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_pyspark2
      - run:
          name: Run Scala tests
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.11.12"
            sbt coverage core/test coverageReport exit
      - run:
          name: Run docs tests
          no_output_timeout: 30m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.11.12"
            sbt docs/test exit
      - run:
          name: Run Python tests
          no_output_timeout: 90m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.11.12"
            sbt python/test exit
      - run:
          name: Run Hail tests
          no_output_timeout: 30m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$HOME/conda/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.11.12"
            sudo apt-get update
            sudo apt-get -y install rsync
            sbt installHail hail/test uninstallHail exit
      - *check_clean_repo
      - store_artifacts:
          path: ~/glow/unit-tests.log
          destination: unit-tests.log
      - store_test_results:
          path: ~/glow/core/target/scala-2.11/test-reports
      - codecov/upload:
          file: "core/target/scala-2.11/scoverage-report/scoverage.xml"

  scala-2_12-tests:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_pyspark2
      - run:
          name: Run Scala tests
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.12.8"
            sbt core/test exit
      - run:
          name: Run docs tests
          no_output_timeout: 30m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.12.8"
            sbt docs/test exit
      - run:
          name: Run Python tests
          no_output_timeout: 90m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.12.8"
            sbt python/test exit
      - run:
          name: Run Hail tests
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$HOME/conda/bin:$PATH
            export SPARK_VERSION="2.4.5"
            export SCALA_VERSION="2.12.8"
            sudo apt-get update
            sudo apt-get -y install rsync
            sbt installHail hail/test uninstallHail exit

  spark-3-tests:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Run Scala tests
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.0.0"
            export SCALA_VERSION="2.12.8"
            sbt core/test exit
      - run:
          name: Run docs tests
          no_output_timeout: 30m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.0.0"
            export SCALA_VERSION="2.12.8"
            sbt docs/test exit
      - run:
          name: Run Python tests
          no_output_timeout: 90m
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            export SPARK_VERSION="3.0.0"
            export SCALA_VERSION="2.12.8"
            sbt python/test exit
      - run:
          name: Run Hail tests
          environment:
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$HOME/conda/bin:$PATH
            export SPARK_VERSION="3.0.0"
            export SCALA_VERSION="2.12.8"
            sudo apt-get update
            sudo apt-get -y install rsync
            sbt installHail hail/test uninstallHail exit
      - *check_clean_repo
      - store_artifacts:
          path: ~/glow/unit-tests.log
          destination: unit-tests.log
      - store_test_results:
          path: ~/glow/core/target/scala-2.12/test-reports

  all-notebook-tests:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Configure Databricks CLI
          command: |
            printf "[docs-ci]\nhost = https://westus2.azuredatabricks.net\ntoken = ${DATABRICKS_API_TOKEN}\n" > ~/.databrickscfg
      - run:
          name: Run all notebook tests
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            python docs/dev/run-nb-test.py --cli-profile docs-ci

  changed-notebook-tests:
    <<: *setup_base
    steps:
      - checkout
      - restore_cache:
          keys:
            - conda-deps-v1-{{ checksum "python/environment.yml" }}
      - *install_conda
      - *create_python_env
      - save_cache:
          paths:
            - /home/circleci/conda
          key: conda-deps-v1-{{ checksum "python/environment.yml" }}
      - run:
          name: Configure Databricks CLI
          command: |
            printf "[docs-ci]\nhost = https://westus2.azuredatabricks.net\ntoken = ${DATABRICKS_API_TOKEN}\n" > ~/.databrickscfg
      - run:
          name: Run changed notebook tests
          command: |
            export PATH=$HOME/conda/envs/glow/bin:$PATH
            if [[ -n $(git diff --name-only master --relative docs/source/_static/) ]]; then
              echo "Testing changed notebooks!"
              python docs/dev/run-nb-test.py --cli-profile docs-ci $(git diff --name-only master --relative docs/source/_static/zzz_GENERATED_NOTEBOOK_SOURCE | cut -c 51- | cut -f1 -d"." | sed -e 's/^/--nbs /')
            else
              echo "No notebooks changed."
            fi

workflows:
  version: 2
  test:
    jobs:
      - check-docs
      - scala-2_11-tests
      - scala-2_12-tests
      - spark-3-tests
      - changed-notebook-tests
  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - check-docs
      - spark-3-tests
      - all-notebook-tests
