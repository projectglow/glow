/*
 * Copyright 2019 The Glow Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.projectglow.transformers.splitmultiallelics

import io.projectglow.Glow
import io.projectglow.common.VariantSchemas._
import io.projectglow.common.{CommonOptions, GlowLogging}
import io.projectglow.sql.GlowBaseTest
import org.apache.spark.SparkConf
import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute

import io.projectglow.sql.expressions.GenotypeStates
import io.projectglow.transformers.splitmultiallelics.SplitMultiallelicsTransformer._

class SplitMultiallelicsTransformerSuite extends GlowBaseTest with GlowLogging {

  lazy val sourceName: String = "vcf"
  lazy val testFolder: String = s"$testDataHome/variantsplitternormalizer-test"

  // gatk test file (multiallelic)
  // The base of vcfs and reference in these test files were taken from gatk
  // LeftTrimAndLeftAlign test suite. The reference genome was trimmed to +/-400 bases around
  // each variant to generate a small reference fasta. The vcf variants were modified accordingly.

  lazy val gatkTestVcf =
    s"$testFolder/test_left_align_hg38_altered.vcf"

  lazy val gatkTestVcfExpectedSplit =
    s"$testFolder/test_left_align_hg38_altered_vtdecompose.vcf"

  // These files are similar to above but contain symbolic variants.
  lazy val gatkTestVcfSymbolic =
    s"$testFolder/test_left_align_hg38_altered_symbolic.vcf"

  lazy val gatkTestVcfSymbolicExpectedSplit =
    s"$testFolder/test_left_align_hg38_altered_symbolic_vtdecompose.vcf"

  // vt test files
  // The base of vcfs and reference in these test files were taken from vt
  // (https://genome.sph.umich.edu/wiki/Vt) normalization test suite. The vcf in this test suite
  // is biallelic. The reference genome was trimmed to +/-100 bases around each variant to
  // generate a small reference fasta. The vcf variants were modified accordingly.
  //
  // The multialleleic versions were generated by artificially adding more alleles and
  // corresponding genotypes to some of the variants.

  lazy val vtTestVcfBiallelic =
    s"$testFolder/01_IN_altered_biallelic.vcf"

  lazy val vtTestVcfBiallelicExpectedSplit =
    s"$testFolder/01_IN_altered_biallelic_vtdecompose.vcf"

  lazy val vtTestVcfMultiAllelic =
    s"$testFolder/01_IN_altered_multiallelic.vcf"

  lazy val vtTestVcfMultiAllelicExpectedSplit =
    s"$testFolder/01_IN_altered_multiallelic_vtdecompose.vcf"

  override def sparkConf: SparkConf = {
    super
      .sparkConf
      .set(
        "spark.hadoop.io.compression.codecs",
        "org.seqdoop.hadoop_bam.util.BGZFCodec"
      )
  }

  /**
   *  Tests whether the transformed VCF matches the expected VCF
   */
  def testSplitvsExpected(
      originalVCFFileName: String,
      expectedVCFFileName: String,
      includeSampleIds: Boolean
  ): Unit = {

    val dfOriginal = spark
      .read
      .format(sourceName)
      .options(Map(CommonOptions.INCLUDE_SAMPLE_IDS -> includeSampleIds.toString))
      .load(originalVCFFileName)

    val dfSplit = Glow
      .transform(
        SPLITTER_TRANSFORMER_NAME,
        dfOriginal
      )
      .orderBy(contigNameField.name, startField.name, endField.name)

    val dfExpected = spark
      .read
      .format(sourceName)
      .options(Map(CommonOptions.INCLUDE_SAMPLE_IDS -> includeSampleIds.toString))
      .load(expectedVCFFileName)
      .orderBy(contigNameField.name, startField.name, endField.name)

    val dfExpectedColumns =
      dfExpected.columns.map(name => if (name.contains(".")) s"`${name}`" else name)

    assert(dfSplit.count() == dfExpected.count())

    dfExpected
      .drop(splitFromMultiAllelicField.name)
      .collect
      .zip(
        dfSplit
          .select(dfExpectedColumns.head, dfExpectedColumns.tail: _*) // make order of columns the same
          .drop(splitFromMultiAllelicField.name)
          .collect
      )
      .foreach {
        case (rowExp, rowNorm) =>
          assert(rowExp.equals(rowNorm), s"Expected\n$rowExp\nNormalized\n$rowNorm")
      }
  }

  def testSplitvsExpected(
      originalVCFFileName: String,
      expectedVCFFileName: String
  ): Unit = {
    testSplitvsExpected(originalVCFFileName, expectedVCFFileName, true)
  }

  test("split multiallelics transform") {

    testSplitvsExpected(
      vtTestVcfBiallelic,
      vtTestVcfBiallelicExpectedSplit
    )

    testSplitvsExpected(
      vtTestVcfMultiAllelic,
      vtTestVcfMultiAllelicExpectedSplit
    )

    // without sampleIds
    testSplitvsExpected(
      vtTestVcfMultiAllelic,
      vtTestVcfMultiAllelicExpectedSplit,
      false
    )

    testSplitvsExpected(
      gatkTestVcf,
      gatkTestVcfExpectedSplit
    )

    testSplitvsExpected(
      gatkTestVcfSymbolic,
      gatkTestVcfSymbolicExpectedSplit
    )

  }
}
