[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.plink[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{EOFException, FileNotFoundException}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.{DebugFilesystem, SparkException}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.Row[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.ScalaReflection[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.expr[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.StructType[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{PlinkGenotype, PlinkRow, VariantSchemas}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass PlinkReaderSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val testRoot = s"$testDataHome/plink"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val fiveSamplesFiveVariants = s"$testRoot/five-samples-five-variants"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val bedBimFam = s"$fiveSamplesFiveVariants/bed-bim-fam"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val sourceName = "plink"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Read PLINK files") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val plinkRows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sort("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[PlinkRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(plinkRows.length == 5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    DebugFilesystem.assertNoOpenStreams()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val snp1 = plinkRows.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      snp1 == PlinkRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        contigName = "1",[0m
[0m[[0m[0mdebug[0m] [0m[0m        position = 0.0,[0m
[0m[[0m[0mdebug[0m] [0m[0m        start = 9,[0m
[0m[[0m[0mdebug[0m] [0m[0m        end = 10,[0m
[0m[[0m[0mdebug[0m] [0m[0m        names = Seq("snp1"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        referenceAllele = "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m        alternateAlleles = Seq("C"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        genotypes = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam1_ind1",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam2_ind2",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam3_ind3",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(1, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam4_ind4",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(-1, -1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam5_ind5",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(-1, -1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          )[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      ))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val snp5 = plinkRows.last[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      snp5 == PlinkRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        contigName = "26", // MT[0m
[0m[[0m[0mdebug[0m] [0m[0m        position = 2.5,[0m
[0m[[0m[0mdebug[0m] [0m[0m        start = 49,[0m
[0m[[0m[0mdebug[0m] [0m[0m        end = 50,[0m
[0m[[0m[0mdebug[0m] [0m[0m        names = Seq("snp5"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        referenceAllele = "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m        alternateAlleles = Seq("C"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        genotypes = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam1_ind1",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam2_ind2",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam3_ind3",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam4_ind4",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          PlinkGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m            sampleId = "fam5_ind5",[0m
[0m[[0m[0mdebug[0m] [0m[0m            calls = Seq(0, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m          )[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      ))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Missing FAM") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$fiveSamplesFiveVariants/no-fam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[FileNotFoundException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    DebugFilesystem.assertNoOpenStreams()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Missing BIM") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$fiveSamplesFiveVariants/no-bim/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[FileNotFoundException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    DebugFilesystem.assertNoOpenStreams()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Wrong BIM delimiter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("bimDelimiter", " ")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .sort("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.getMessage.contains("Failed while parsing BIM file"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Wrong FAM delimiter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("famDelimiter", "\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Failed while parsing FAM file"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Read subset of columns") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select("contigName", "start", "genotypes.sampleId")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      rows.head == Row([0m
[0m[[0m[0mdebug[0m] [0m[0m        "1",[0m
[0m[[0m[0mdebug[0m] [0m[0m        9,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq("fam1_ind1", "fam2_ind2", "fam3_ind3", "fam4_ind4", "fam5_ind5")))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Read compared to VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // The PLINK schema is a sub-set of the VCF schema, with the addition of a position field[0m
[0m[[0m[0mdebug[0m] [0m[0m    val commonVcfPlinkSchema =[0m
[0m[[0m[0mdebug[0m] [0m[0m      StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m        VariantSchemas.plinkSchema(true).filter(_.name != VariantSchemas.positionField.name))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfRows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(commonVcfPlinkSchema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$fiveSamplesFiveVariants/vcf/test.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sort("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val plinkRows =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .schema(commonVcfPlinkSchema)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .sort("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfRows.collect.sameElements(plinkRows.collect))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Read BED without magic bytes") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$bedBimFam/test.fam")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Magic bytes were not 006c,001b,0001"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Read prematurely truncated BED") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$fiveSamplesFiveVariants/malformed/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[EOFException])[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Use IID only for sample ID") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("mergeFidIid", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select("genotypes.sampleId")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[String]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .limit(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleIds == Seq("ind1", "ind2", "ind3", "ind4", "ind5"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Invalid arg for mergeFidIid") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("mergeFidIid", "hello")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      e.getCause[0m
[0m[[0m[0mdebug[0m] [0m[0m        .getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains("Value for mergeFidIid must be [true, false]. Provided: hello"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("PLINK file format does not support writing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[UnsupportedOperationException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.write.format(sourceName).save("noop")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("PLINK data source does not support writing"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class WeirdGenotype(animal: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class WeirdVariant(species: String, genotypes: Seq[WeirdGenotype])[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Be permissive if schema includes fields that can't be derived from PLINK files") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(ScalaReflection.schemaFor[WeirdVariant].dataType.asInstanceOf[StructType])[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    rows.foreach { r =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(r == Row(null, Seq(Row(null), Row(null), Row(null), Row(null), Row(null))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Accept glob path") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = spark.read.format(sourceName).load(s"$bedBimFam/*.bed").collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rows.length == 5)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Support not having sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$bedBimFam/test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("g", expr("genotypes[0]"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("g.*")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!schema.exists(_.name == "sampleId"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def getVariantIdxStartNum(fileStart: Long, fileLength: Long, blockSize: Int): (Int, Int, Int) = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val firstVariantIdx = PlinkFileFormat.getFirstVariantIdx(fileStart, blockSize)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val firstVariantStart = PlinkFileFormat.getVariantStart(firstVariantIdx, blockSize)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val numVariants =[0m
[0m[[0m[0mdebug[0m] [0m[0m      PlinkFileFormat.getNumVariants(fileStart, fileLength, firstVariantStart, blockSize)[0m
[0m[[0m[0mdebug[0m] [0m[0m    (firstVariantIdx, firstVariantStart, numVariants)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Full file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(0, 13, 2) == (0, 3, 5))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Start in magic block") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(1, 12, 5) == (0, 3, 2))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Start at beginning of first block") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(3, 10, 2) == (0, 3, 5))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Start in middle of first block") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(4, 9, 2) == (1, 5, 4))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Zero-variant slice") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(6, 1, 2) == (2, 7, 0))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("One-variant slice") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(7, 1, 2) == (2, 7, 1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Multi-variant slice") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getVariantIdxStartNum(7, 6, 2) == (2, 7, 3))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Get block size") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    (1 to 4).foreach { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(PlinkFileFormat.getBlockSize(s) == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    (5 to 8).foreach { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(PlinkFileFormat.getBlockSize(s) == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.common[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{FileInputStream, IOException}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport com.google.common.util.concurrent.Striped[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.WithUtils._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass WithUtilsSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val txtFile = s"$testDataHome/saige_output.txt"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("closes after successful function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new FileInputStream(txtFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    withCloseable(stream) { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      s.read() // Should succeed[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IOException](stream.read())[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("closes after failed function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new FileInputStream(txtFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withCloseable(stream) { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        s.read()[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new IllegalArgumentException("Non fatal exception")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Non fatal exception"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IOException](stream.read())[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unlocks after successful function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val striped = Striped.readWriteLock(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val readLock = striped.get(txtFile).readLock()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writeLock = striped.get(txtFile).writeLock()[0m
[0m[[0m[0mdebug[0m] [0m[0m    withLock(readLock) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(!writeLock.tryLock())[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(writeLock.tryLock())[0m
[0m[[0m[0mdebug[0m] [0m[0m    writeLock.unlock()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unlocks after failed function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val striped = Striped.readWriteLock(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val readLock = striped.get(txtFile).readLock()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writeLock = striped.get(txtFile).writeLock()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withLock(readLock) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new IllegalArgumentException("Non fatal exception")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Non fatal exception"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(writeLock.tryLock())[0m
[0m[[0m[0mdebug[0m] [0m[0m    writeLock.unlock()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uncaches RDD after successful function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rdd = spark.sparkContext.parallelize(Seq(1, 2))[0m
[0m[[0m[0mdebug[0m] [0m[0m    withCachedRDD(rdd) { r =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(r.count == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sparkContext.getPersistentRDDs.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sparkContext.getPersistentRDDs.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uncaches RDD after failed function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rdd = spark.sparkContext.parallelize(Seq(1, 2))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withCachedRDD(rdd) { _ =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new IllegalArgumentException("Non fatal exception")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Non fatal exception"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sparkContext.getPersistentRDDs.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uncaches Dataset after successful function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark.sparkContext.parallelize(Seq(1, 2)).toDF[0m
[0m[[0m[0mdebug[0m] [0m[0m    withCachedDataset(ds) { d =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(d.count == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(!spark.sharedState.cacheManager.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sharedState.cacheManager.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uncaches Dataset after failed function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark.sparkContext.parallelize(Seq(1, 2)).toDF[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withCachedDataset(ds) { _ =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new IllegalArgumentException("Non fatal exception")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Non fatal exception"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sharedState.cacheManager.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.common[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.scalatest.exceptions.TestFailedException[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mtrait TestUtils {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val ABS_TOL_MSG = " using absolute tolerance"[0m
[0m[[0m[0mdebug[0m] [0m[0m  val REL_TOL_MSG = " using relative tolerance"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Private helper function for comparing two values using relative tolerance.[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Note that if x or y is extremely close to zero, i.e., smaller than Double.MinPositiveValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m   * the relative tolerance is meaningless, so the exception will be raised to warn users.[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def RelativeErrorComparison(x: Double, y: Double, eps: Double): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val absX = math.abs(x)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val absY = math.abs(y)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val diff = math.abs(x - y)[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (x == y) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else if (absX <= Double.MinPositiveValue && absY <= Double.MinPositiveValue) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else if (absX < Double.MinPositiveValue || absY < Double.MinPositiveValue) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      throw new TestFailedException([0m
[0m[[0m[0mdebug[0m] [0m[0m        s"$x or $y is extremely close to zero, so the relative tolerance is meaningless.",[0m
[0m[[0m[0mdebug[0m] [0m[0m        0[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      diff < eps * math.min(absX, absY)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Private helper function for comparing two values using absolute tolerance.[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def AbsoluteErrorComparison(x: Double, y: Double, eps: Double): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    math.abs(x - y) < eps[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class CompareDoubleRightSide([0m
[0m[[0m[0mdebug[0m] [0m[0m      fun: (Double, Double, Double) => Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      y: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m      eps: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m      method: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Implicit class for comparing two double values using relative tolerance or absolute tolerance.[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  implicit class DoubleWithAlmostEquals(val x: Double) {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * When the difference of two values are within eps, returns true; otherwise, returns false.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def ~=(r: CompareDoubleRightSide): Boolean = r.fun(x, r.y, r.eps)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * When the difference of two values are within eps, returns false; otherwise, returns true.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def !~=(r: CompareDoubleRightSide): Boolean = !r.fun(x, r.y, r.eps)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * Throws exception when the difference of two values are NOT within eps;[0m
[0m[[0m[0mdebug[0m] [0m[0m     * otherwise, returns true.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def ~==(r: CompareDoubleRightSide): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m      if (!r.fun(x, r.y, r.eps)) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new TestFailedException([0m
[0m[[0m[0mdebug[0m] [0m[0m          s"Expected $x and ${r.y} to be within ${r.eps}${r.method}.",[0m
[0m[[0m[0mdebug[0m] [0m[0m          0[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * Throws exception when the difference of two values are within eps; otherwise, returns true.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def !~==(r: CompareDoubleRightSide): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m      if (r.fun(x, r.y, r.eps)) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw new TestFailedException([0m
[0m[[0m[0mdebug[0m] [0m[0m          s"Did not expect $x and ${r.y} to be within ${r.eps}${r.method}.",[0m
[0m[[0m[0mdebug[0m] [0m[0m          0[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * Comparison using absolute tolerance.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def absTol(eps: Double): CompareDoubleRightSide =[0m
[0m[[0m[0mdebug[0m] [0m[0m      CompareDoubleRightSide(AbsoluteErrorComparison, x, eps, ABS_TOL_MSG)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /**[0m
[0m[[0m[0mdebug[0m] [0m[0m     * Comparison using relative tolerance.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    def relTol(eps: Double): CompareDoubleRightSide =[0m
[0m[[0m[0mdebug[0m] [0m[0m      CompareDoubleRightSide(RelativeErrorComparison, x, eps, REL_TOL_MSG)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    override def toString: String = x.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.File[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport breeze.linalg.{DenseMatrix, DenseVector}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.io.FileUtils[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.encoders.ExpressionEncoder[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.{expr, monotonically_increasing_id}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{AnalysisException, Encoders}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.expressions.{LogisticRegressionGwas, LogitTestResults, NewtonResult}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.tertiary.RegressionTestUtils._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass LogisticRegressionSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private lazy val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def runLRT(testData: TestData, onSpark: Boolean): Seq[LogitTestResults] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    runTest("LRT", testData, onSpark)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def runTest([0m
[0m[[0m[0mdebug[0m] [0m[0m      logitTest: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      testData: TestData,[0m
[0m[[0m[0mdebug[0m] [0m[0m      onSpark: Boolean): Seq[LogitTestResults] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (onSpark) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      val rows = testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m          g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m          testData.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m          twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("id", monotonically_increasing_id())[0m
[0m[[0m[0mdebug[0m] [0m[0m        .repartition(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m          "logit",[0m
[0m[[0m[0mdebug[0m] [0m[0m          expr(s"logistic_regression_gwas(genotypes, phenotypes, covariates, '$logitTest')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .orderBy("id")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("expand_struct(logit)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[LogitTestResults][0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m        .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val encoder = Encoders[0m
[0m[[0m[0mdebug[0m] [0m[0m        .product[LogitTestResults][0m
[0m[[0m[0mdebug[0m] [0m[0m        .asInstanceOf[ExpressionEncoder[LogitTestResults]][0m
[0m[[0m[0mdebug[0m] [0m[0m        .resolveAndBind()[0m
[0m[[0m[0mdebug[0m] [0m[0m      val covariatesMatrix = twoDArrayToSparkMatrix(testData.covariates.toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m      val t = LogisticRegressionGwas.logitTests(logitTest)[0m
[0m[[0m[0mdebug[0m] [0m[0m      val nullFit = if (t.canReuseNullFit) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        Some(t.fitNullModel(testData.phenotypes.toArray, covariatesMatrix))[0m
[0m[[0m[0mdebug[0m] [0m[0m      } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m        None[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val x = new DenseMatrix[Double]([0m
[0m[[0m[0mdebug[0m] [0m[0m          covariatesMatrix.numRows,[0m
[0m[[0m[0mdebug[0m] [0m[0m          covariatesMatrix.numCols + 1,[0m
[0m[[0m[0mdebug[0m] [0m[0m          covariatesMatrix.values ++ g)[0m
[0m[[0m[0mdebug[0m] [0m[0m        val y = new DenseVector[Double](testData.phenotypes.toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m        val internalRow = t[0m
[0m[[0m[0mdebug[0m] [0m[0m          .runTest(x, y, nullFit)[0m
[0m[[0m[0mdebug[0m] [0m[0m        encoder.fromRow(internalRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Data from https://stats.idre.ucla.edu/stat/data/binary.csv[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Check if fitting with (gre + gpa + rank) is significantly better than fitting with (gre + gpa)[0m
[0m[[0m[0mdebug[0m] [0m[0m  // R commands:[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    fiftyStudents <- head(df, 50)[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    fullFit <- glm(admit ~ gre + gpa + rank, family = binomial, fiftyStudents)[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    nullFit <- glm(admit ~ gre + gpa, family = binomial, fiftyStudents)[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    oddsRatio <- exp(coef(fullFit))[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    confInt <- exp(confint.default(fullFit))[0m
[0m[[0m[0mdebug[0m] [0m[0m  //    lrtPValue <- anova(nullFit, fullFit, test="LRT")[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val admitStudents: TestData = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s = """[0m
[0m[[0m[0mdebug[0m] [0m[0m      |   admit gre  gpa rank[0m
[0m[[0m[0mdebug[0m] [0m[0m      |1      0 380 3.61    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |2      1 660 3.67    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |3      1 800 4.00    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |4      1 640 3.19    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |5      0 520 2.93    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |6      1 760 3.00    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |7      1 560 2.98    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |8      0 400 3.08    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |9      1 540 3.39    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |10     0 700 3.92    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |11     0 800 4.00    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |12     0 440 3.22    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |13     1 760 4.00    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |14     0 700 3.08    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |15     1 700 4.00    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |16     0 480 3.44    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |17     0 780 3.87    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |18     0 360 2.56    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |19     0 800 3.75    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |20     1 540 3.81    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |21     0 500 3.17    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |22     1 660 3.63    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |23     0 600 2.82    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |24     0 680 3.19    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |25     1 760 3.35    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |26     1 800 3.66    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |27     1 620 3.61    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |28     1 520 3.74    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |29     1 780 3.22    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |30     0 520 3.29    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |31     0 540 3.78    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |32     0 760 3.35    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |33     0 600 3.40    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |34     1 800 4.00    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |35     0 360 3.14    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |36     0 400 3.05    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |37     0 580 3.25    1[0m
[0m[[0m[0mdebug[0m] [0m[0m      |38     0 520 2.90    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |39     1 500 3.13    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |40     1 520 2.68    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |41     0 560 2.42    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |42     1 580 3.32    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |43     1 600 3.15    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |44     0 500 3.31    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |45     0 700 2.94    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |46     1 460 3.45    3[0m
[0m[[0m[0mdebug[0m] [0m[0m      |47     1 580 3.46    2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |48     0 500 2.97    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |49     0 440 2.48    4[0m
[0m[[0m[0mdebug[0m] [0m[0m      |50     0 400 3.35    3[0m
[0m[[0m[0mdebug[0m] [0m[0m    """.stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val parsed = s[0m
[0m[[0m[0mdebug[0m] [0m[0m      .trim()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .split("\n")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map(l => l.split("\\s+").drop(1).map(_.toDouble))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val phenotypes = parsed.map(_(0))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariates = parsed.map { r =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1d, r(1), r(2))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = parsed.map(_(3))[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestData(Seq(genotypes), phenotypes, covariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val admitStudentsStats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults(-0.611263, 0.54266503, Seq(2.901759e-01, 1.014851), 0.04693173)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV1 = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Seq(0, 1, 2, 0, 0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Array(1), Array(1), Array(1), Array(1), Array(1), Array(1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV1Stats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults(0.4768, 1.610951, Seq(0.1403952, 18.48469), 0.6935)[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV1FirthStats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults([0m
[0m[[0m[0mdebug[0m] [0m[0m      0.2434646,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Math.exp(0.2434646),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(-2.020431, 2.507360).map(Math.exp),[0m
[0m[[0m[0mdebug[0m] [0m[0m      0.796830)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV2 = interceptOnlyV1.copy(phenotypes = Seq(0, 0, 1, 0, 1, 1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV2Stats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults(1.4094, 4.0936366, Seq(0.26608762, 62.978730), 0.2549)[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val interceptOnlyV2FirthStats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults([0m
[0m[[0m[0mdebug[0m] [0m[0m      0.8731197,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Math.exp(0.8731197),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(-1.495994, 3.242234).map(Math.exp),[0m
[0m[[0m[0mdebug[0m] [0m[0m      0.3609153)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val gtsAndCovariates = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Seq(0, 1, 2, 0, 0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, 0, -1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, 2, 3),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, 1, 5),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, -2, 0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, -2, -4),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(1, 4, 3)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val gtsAndCovariatesStats =[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogitTestResults(3.1776, 23.9884595, Seq(0.007623126, 75486.900886), 0.35)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // https://en.wikipedia.org/wiki/Separation_(statistics)[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val completeSeparation = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Seq(0, 1, 2, 0, 0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Array(0), Array(0), Array(1), Array(1), Array(1), Array(1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val linearlyDependentCovariates = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Seq(0, 1, 2, 0, 0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Array(0, 0), Array(1, 2), Array(1, 2), Array(1, 2), Array(1, 2), Array(1, 2)))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class TestDataAndGoldenStats(testData: TestData, lrtStats: LogitTestResults)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val TEST_DATA_AND_GOLDEN_STATS = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    TestDataAndGoldenStats(admitStudents, admitStudentsStats),[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestDataAndGoldenStats(interceptOnlyV1, interceptOnlyV1Stats),[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestDataAndGoldenStats(interceptOnlyV2, interceptOnlyV2Stats),[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestDataAndGoldenStats(gtsAndCovariates, gtsAndCovariatesStats)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareLogitTestResults(s1: LogitTestResults, s2: LogitTestResults): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.beta ~== s2.beta relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.oddsRatio ~== s2.oddsRatio relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.waldConfidenceInterval.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s2.waldConfidenceInterval.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    s1.waldConfidenceInterval.zip(s2.waldConfidenceInterval).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (s1ci, s2ci) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(s1ci ~== s2ci relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.pValue ~== s2.pValue relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def runNewtonIterations(testData: TestData): NewtonResult = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    LogisticRegressionGwas.newtonIterations([0m
[0m[[0m[0mdebug[0m] [0m[0m      twoDArrayToBreezeMatrix(testData.covariates.toArray),[0m
[0m[[0m[0mdebug[0m] [0m[0m      DenseVector(testData.phenotypes.toArray),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Does not converge in case of complete separation") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val nonConvergedNewton = runNewtonIterations(completeSeparation)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!nonConvergedNewton.converged)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!nonConvergedNewton.exploded)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(nonConvergedNewton.nIter == 26)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Explodes in case of linearly dependent covariates") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val nonConvergedNewton = runNewtonIterations(linearlyDependentCovariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!nonConvergedNewton.converged)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(nonConvergedNewton.exploded)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(nonConvergedNewton.nIter == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw error if test is not foldable") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = admitStudents.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        admitStudents.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(admitStudents.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .repartition(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("test", monotonically_increasing_id())[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m          "logit",[0m
[0m[[0m[0mdebug[0m] [0m[0m          expr("logistic_regression_gwas(genotypes, phenotypes, covariates, test)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Test must be a constant value"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw error if test is not supported") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = admitStudents.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        admitStudents.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(admitStudents.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m          "logit",[0m
[0m[[0m[0mdebug[0m] [0m[0m          expr("logistic_regression_gwas(genotypes, phenotypes, covariates, 'fakeTest')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("expand_struct(logit)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.toLowerCase.contains("supported tests are currently"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  Seq(true, false).foreach { onSpark =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    gridTest(s"Likelihood ratio test against R (on Spark: $onSpark)")(TEST_DATA_AND_GOLDEN_STATS) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      testCase =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val ourStats = runLRT(testCase.testData, onSpark).head[0m
[0m[[0m[0mdebug[0m] [0m[0m        compareLogitTestResults(testCase.lrtStats, ourStats)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkAllNan(lrtStats: LogitTestResults): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(lrtStats.beta.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(lrtStats.oddsRatio.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m    lrtStats.waldConfidenceInterval.foreach { c =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(c.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(lrtStats.pValue.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Return NaNs if null fit didn't converge") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ourStats = runLRT(completeSeparation, false).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkAllNan(ourStats)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Return NaNs if full fit didn't converge") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ourStats = runLRT([0m
[0m[[0m[0mdebug[0m] [0m[0m      TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(completeSeparation.covariates.flatten),[0m
[0m[[0m[0mdebug[0m] [0m[0m        completeSeparation.phenotypes,[0m
[0m[[0m[0mdebug[0m] [0m[0m        completeSeparation.genotypes.head.map(Array(_))),[0m
[0m[[0m[0mdebug[0m] [0m[0m      false).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkAllNan(ourStats)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val allLogitTests = LogisticRegressionGwas.logitTests.keys.toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("Check sample number matches between phenos and covars")(allLogitTests) { testName =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fewerPhenoSamples = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Seq(0, 1, 2, 0, 0)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0, 0, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Array(1), Array(1), Array(1), Array(1), Array(1), Array(1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      runTest(testName, fewerPhenoSamples, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      ex.getCause[0m
[0m[[0m[0mdebug[0m] [0m[0m        .getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains("Number of samples do not match between phenotype vector and covariate matrix"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("Check sample number matches between genos and phenos")(allLogitTests) { testName =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fewerPhenoSamples = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Seq(0, 1, 2, 0, 0)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Array(1), Array(1), Array(1), Array(1), Array(1), Array(1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      runTest(testName, fewerPhenoSamples, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      ex.getCause[0m
[0m[[0m[0mdebug[0m] [0m[0m        .getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains("Number of samples differs between genotype and phenotype arrays"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("Checks for non-zero covariates")(allLogitTests) { testName =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fewerPhenoSamples = TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Seq(0, 1, 2, 0, 0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0, 0, 1, 1, 1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(Array.empty, Array.empty, Array.empty, Array.empty, Array.empty, Array.empty))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      runLRT(fewerPhenoSamples, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.toLowerCase.contains("must have at least one column"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Run multiple regressions") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = testDataToRows(interceptOnlyV1) ++ testDataToRows(interceptOnlyV2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ourStats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("id", monotonically_increasing_id())[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "logit",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr("logistic_regression_gwas(genotypes, phenotypes, covariates, 'LRT')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("id")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(logit)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[LogitTestResults][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(interceptOnlyV1Stats, interceptOnlyV2Stats).zip(ourStats).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (golden, our) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        compareLogitTestResults(golden, our)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("firth regression vs R onSpark=")(Seq(true, false)) { onSpark =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s = FileUtils.readFileToString(new File(s"$testDataHome/r/sex2.txt"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val parsed = s[0m
[0m[[0m[0mdebug[0m] [0m[0m      .trim()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .split("\n")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map(l => l.split("\\s+").map(_.toDouble))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val phenotypes = parsed.map(_(0))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariates = parsed.map { r =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      1d +: r.tail.init[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = parsed.map(_.last)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = TestData(Seq(genotypes), phenotypes, covariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ours = runTest("firth", testData, onSpark).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    // golden stats are from R's logistf package[0m
[0m[[0m[0mdebug[0m] [0m[0m    // data(sex2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // fit<-logistf(case ~ age+oc+vic+vicl+vis+dia, data=sex2, pl=TRUE)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // summary(fit)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // To get Wald confidence intervals, set pl=FALSE[0m
[0m[[0m[0mdebug[0m] [0m[0m    val golden = LogitTestResults([0m
[0m[[0m[0mdebug[0m] [0m[0m      3.09601263,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Math.exp(3.09601263),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(-0.1869446, 6.37896984).map(Math.exp),[0m
[0m[[0m[0mdebug[0m] [0m[0m      4.951873e-03[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLogitTestResults(golden, ours)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("multiple firth regressions") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = testDataToRows(interceptOnlyV1) ++ testDataToRows(interceptOnlyV2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ourStats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("id", monotonically_increasing_id())[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "logit",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr("logistic_regression_gwas(genotypes, phenotypes, covariates, 'firth')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("id")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(logit)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[LogitTestResults][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(interceptOnlyV1FirthStats, interceptOnlyV2FirthStats).zip(ourStats).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (golden, our) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        compareLogitTestResults(golden, our)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("firth returns nan if model can't be fit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val result = runTest("firth", linearlyDependentCovariates, onSpark = false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkAllNan(result.head)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("tests are case insensitive") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      LogisticRegressionGwas.logitTests.get("firth") ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      LogisticRegressionGwas.logitTests.get("FIRTH"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(LogisticRegressionGwas.logitTests.get("monkey").isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{AnalysisException, DataFrame}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.expr[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{LongType, StringType, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass LiftOverCoordinatesExprSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  val requiredBaseSchema: StructType = new StructType()[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("contigName", StringType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("start", LongType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("end", LongType)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val chainFile = s"$testDataHome/liftover/hg38ToHg19.over.chain.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def liftOverAndCompare([0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossMappedDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossUnmappedDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt: Option[Double]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = if (minMatchOpt.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("minMatchRatio", expr(s"${minMatchOpt.get}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m          "lifted",[0m
[0m[[0m[0mdebug[0m] [0m[0m          expr(s"lift_over_coordinates(contigName, start, end, '$chainFile', minMatchRatio)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .drop("minMatchRatio")[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("lifted", expr(s"lift_over_coordinates(contigName, start, end, '$chainFile')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val liftedDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .filter("lifted is not null")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("contigName", $"lifted.contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("start", $"lifted.start")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("end", $"lifted.end")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .drop("lifted")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val unliftedDf = outputDf.filter("lifted is null").drop("lifted")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val liftedRows = liftedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossMappedRows = crossMappedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(liftedRows.length == crossMappedRows.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    liftedRows.zip(crossMappedRows).foreach { case (r1, r2) => assert(r1 == r2) }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val unliftedRows = unliftedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossUnmappedRows = crossUnmappedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(unliftedRows.length == crossUnmappedRows.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    unliftedRows.zip(crossUnmappedRows).foreach { case (r1, r2) => assert(r1 == r2) }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def readVcf(vcfFile: String): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcfFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select("contigName", "start", "end")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      testVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossMappedVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossUnmappedVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt: Option[Double] = None): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossMappedDf = readVcf(crossMappedVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossUnmappedDf = readVcf(crossUnmappedVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    liftOverAndCompare(inputDf, crossMappedDf, crossUnmappedDf, minMatchOpt)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def readBed(bedFile: String): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("csv")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("delimiter", "\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("header", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(requiredBaseSchema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(bedFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareLiftedBed([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBed: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossMappedBed: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      crossUnmappedBed: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt: Option[Double] = None): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readBed(testBed)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossMappedDf = readBed(crossMappedBed)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val crossUnmappedDf = readBed(crossUnmappedBed)[0m
[0m[[0m[0mdebug[0m] [0m[0m    liftOverAndCompare(inputDf, crossMappedDf, crossUnmappedDf, minMatchOpt)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Basic") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/combined.chr20_18210071_18210093.g.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.combined.chr20_18210071_18210093.g.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.combined.chr20_18210071_18210093.g.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      None[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some failures") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/unlifted.test.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.test.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.test.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      None)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("High min match") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedBed([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/unlifted.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.minMatch05.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.minMatch05.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(0.5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Low min match") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedBed([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/unlifted.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.minMatch001.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.minMatch001.test.bed",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(0.01)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Do not cache chain file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    inputDf.selectExpr(s"lift_over_coordinates(contigName, start, end, '$chainFile')").collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.selectExpr("lift_over_coordinates(contigName, start, end, 'fakeChainFile')").collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.contains("htsjdk.samtools.SAMException: Cannot read non-existent file"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Null contigName") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.withColumn("lifted", expr(s"lift_over_coordinates(null, start, end, '$chainFile')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("lifted is null").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Null start") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "lifted",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr(s"lift_over_coordinates(contigName, null, end, '$chainFile')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("lifted is null").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Null end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "lifted",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr(s"lift_over_coordinates(contigName, start, null, '$chainFile')"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("lifted is null").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Null minMatchRatio") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "lifted",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr(s"lift_over_coordinates(contigName, start, end, '$chainFile', null)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("lifted is null").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Chain file must be constant") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readBed(s"$testDataHome/liftover/unlifted.test.bed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf.selectExpr(s"lift_over_coordinates(contigName, start, end, rand())").collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage().contains("Chain file must be a constant value"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.expressions.MomentAggState[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass MomentAggStateSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("merge") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s1 = MomentAggState(5, 0, 10, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s2 = MomentAggState(3, 1, 11, 1, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val merged = MomentAggState.merge(s1, s2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = MomentAggState(8, 0, 11, 1.625, 1 + 2 + -1 * -0.125 * 5 * 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(merged == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("merge (count == 0)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s1 = MomentAggState()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s2 = MomentAggState()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val merged = MomentAggState.merge(s1, s2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = MomentAggState()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(merged == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("merge (left count == 0)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s1 = MomentAggState(0, 0, 10, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s2 = MomentAggState(1, 1, 11, 1, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val merged = MomentAggState.merge(s1, s2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = s2[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(merged == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("merge (right count == 0)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s1 = MomentAggState(1, 0, 10, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s2 = MomentAggState(0, 1, 11, 1, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val merged = MomentAggState.merge(s1, s2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = s1[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(merged == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("update") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val state = MomentAggState(1, 1, 1, 1, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    state.update(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = MomentAggState(2, 1, 10, 5.5, 40.5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(state == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport breeze.linalg.{DenseMatrix => BreezeDenseMatrix}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.ml.linalg.{DenseMatrix => SparkDenseMatrix}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m    genotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    phenotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    covariates: SparkDenseMatrix)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class TestData([0m
[0m[[0m[0mdebug[0m] [0m[0m    genotypes: Seq[Seq[Double]],[0m
[0m[[0m[0mdebug[0m] [0m[0m    phenotypes: Seq[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    covariates: Seq[Array[Double]])[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mobject RegressionTestUtils {[0m
[0m[[0m[0mdebug[0m] [0m[0m  def twoDArrayToSparkMatrix(input: Array[Array[Double]]): SparkDenseMatrix = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    new SparkDenseMatrix([0m
[0m[[0m[0mdebug[0m] [0m[0m      input.length,[0m
[0m[[0m[0mdebug[0m] [0m[0m      input.head.length,[0m
[0m[[0m[0mdebug[0m] [0m[0m      input(0).indices.flatMap(i => input.map(_(i))).toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def twoDArrayToBreezeMatrix(input: Array[Array[Double]]): BreezeDenseMatrix[Double] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    new BreezeDenseMatrix([0m
[0m[[0m[0mdebug[0m] [0m[0m      input.length,[0m
[0m[[0m[0mdebug[0m] [0m[0m      input.head.length,[0m
[0m[[0m[0mdebug[0m] [0m[0m      input(0).indices.flatMap(i => input.map(_(i))).toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testDataToRows(testData: TestData): Seq[RegressionRow] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        testData.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.util.Random[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.AnalysisException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GenotypeFields, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VariantQcExprsSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testVcf = s"$testDataHome/1kg_sample.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy private val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val targetSite = col("contigName") === 1 && col("start") === 904164[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(targetSite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("explode(genotypes)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(col)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Golden values are pulled from Hail[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("hardy weinberg") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val hw = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(targetSite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(hardy_weinberg(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[HardyWeinbergStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(hw.hetFreqHwe ~== 0.19938860890353427 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(hw.pValueHwe ~== 2.8753895001390113e-07 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("hardy weinberg doesn't crash if there are no homozygous") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataset(Seq(rowWithCalls(Seq(Seq(0, 1)))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(hardy_weinberg(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect() // no crash[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Golden values are pulled from Hail[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("call stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("contigName", "start", "expand_struct(call_summary_stats(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(targetSite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[CStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.callRate ~== 0.9915493130683899 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nCalled == 704)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nUncalled == 6)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nHet == 110)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nNonRef == 134)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nAllelesCalled == 1408)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.alleleCounts == Seq(1250, 158))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.alleleFrequencies(0) ~== 0.8877840909090909 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.alleleFrequencies(1) ~== 0.11221590909090909 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def rowWithCalls(calls: Seq[Seq[Int]]): VCFRow = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      "monkey",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("G"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      calls.map { call =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        GenotypeFields([0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(call),[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("call stats (missing genotypes)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataset(Seq(rowWithCalls(Seq(Seq()))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(call_summary_stats(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[CStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = CStats(0, 0, 1, 0, Seq.empty, 0, 0, Seq.empty, Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("call stats (haploid)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataset(Seq(rowWithCalls(Seq(Seq(0)))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(call_summary_stats(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[CStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = CStats(1, 1, 0, 0, Seq(1), 0, 1, Seq(1), Seq(1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("call stats (weird genotype struct)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(rowWithCalls(Seq(Seq(0, 1)))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val transformed = df.selectExpr([0m
[0m[[0m[0mdebug[0m] [0m[0m      "transform(genotypes, g -> struct(g.depth as depth, g.calls as calls)) as newG"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = transformed[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(call_summary_stats(newG))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[CStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.nCalled == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("array stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(Datum(Array(0, 1, 2))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(array_summary_stats(numbers))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("array stats (empty)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(Datum(Array.empty)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(array_summary_stats(numbers))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = ArraySummaryStats(None, None, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("array stats (1 element)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(Datum(Array(1))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(array_summary_stats(numbers))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("array stats (contains null)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.range(1).withColumn("numbers", array(lit(1d), lit(3d)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(array_summary_stats(numbers))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == ArraySummaryStats(Some(2), Some(Math.sqrt(2)), Some(1), Some(3)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("array stats (negative values)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.range(1).withColumn("numbers", array(lit(-1d), lit(-3d)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(array_summary_stats(numbers))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == ArraySummaryStats(Some(-2), Some(Math.sqrt(2)), Some(-3), Some(-1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Golden values are pulled from Hail[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("dp stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(targetSite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(dp_summary_stats(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get ~== 7.019886363636361 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get ~== 3.9050742032055332 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get == 23)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Golden values are pulled from Hail[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("gq stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(targetSite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(gq_summary_stats(genotypes))")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get ~== 26.856534090909086 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get ~== 22.18115337984482 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get == 99)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("write to parquet") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tmpFile = s"/tmp/${Random.alphanumeric.take(10).mkString}"[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("stats", expr("call_summary_stats(genotypes)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("hw", expr("hardy_weinberg(genotypes)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("parquet")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tmpFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("analysis error when genotype doesn't exist for call stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(Seq(Datum(Array(1))))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("call_summary_stats(numbers)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Genotypes field must be an array of structs"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("analysis error when genotype doesn't exist for hardy weinberg") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(Seq(Datum(Array(1))))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("hardy_weinberg(numbers)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Genotypes field must be an array of structs"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("analysis error when genotype is missing calls for call stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("transform(genotypes, gt -> subset_struct(gt, 'sampleId')) as callFreeGts")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("call_summary_stats(callFreeGts)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      e.getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains([0m
[0m[[0m[0mdebug[0m] [0m[0m          "Genotype struct was missing required fields: (name: calls, type: ArrayType(IntegerType,true))"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class ArraySummaryStats([0m
[0m[[0m[0mdebug[0m] [0m[0m    mean: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    stdDev: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    min: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    max: Option[Double])[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class Datum(numbers: Array[Double])[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class HardyWeinbergStats(hetFreqHwe: Double, pValueHwe: Double)[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class CStats([0m
[0m[[0m[0mdebug[0m] [0m[0m    callRate: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nCalled: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nUncalled: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nHet: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nHomozygous: Seq[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    nNonRef: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nAllelesCalled: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m    alleleCounts: Seq[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    alleleFrequencies: Seq[Double])[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.concurrent.duration._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.util.Random[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.math3.distribution.TDistribution[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.math3.linear.SingularMatrixException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.ml.linalg.DenseMatrix[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport RegressionTestUtils._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.expressions.{ComputeQR, LinearRegressionGwas, RegressionStats}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass LinearRegressionSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private lazy val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m  private lazy val random = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val seed = System.currentTimeMillis()[0m
[0m[[0m[0mdebug[0m] [0m[0m    logger.info(s"Using random seed $seed")[0m
[0m[[0m[0mdebug[0m] [0m[0m    new Random(seed)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def runRegression([0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      phenotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      covariates: Array[Array[Double]]): RegressionStats = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariateQR = ComputeQR.computeQR(twoDArrayToSparkMatrix(covariates))[0m
[0m[[0m[0mdebug[0m] [0m[0m    LinearRegressionGwas.runRegression(genotypes.clone(), phenotypes.clone(), covariateQR)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def olsBaseline([0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      phenotypes: Array[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      covariates: Array[Array[Double]]): RegressionStats = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // transform the data in to design matrix and y matrix compatible with OLSMultipleLinearRegression[0m
[0m[[0m[0mdebug[0m] [0m[0m    val observationLength = covariates(0).length + 1[0m
[0m[[0m[0mdebug[0m] [0m[0m    val numObservations = genotypes.length[0m
[0m[[0m[0mdebug[0m] [0m[0m    val x = new Array[Array[Double]](numObservations)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // iterate over observations, copying correct elements into sample array and filling the x matrix.[0m
[0m[[0m[0mdebug[0m] [0m[0m    // the first element of each sample in x is the coded genotype and the rest are the covariates.[0m
[0m[[0m[0mdebug[0m] [0m[0m    var sample = new Array[Double](observationLength)[0m
[0m[[0m[0mdebug[0m] [0m[0m    for (i <- 0 until numObservations) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      sample = new Array[Double](observationLength)[0m
[0m[[0m[0mdebug[0m] [0m[0m      for (j <- covariates(i).indices) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        sample(j) = covariates(i)(j)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m      sample(sample.length - 1) = genotypes(i)[0m
[0m[[0m[0mdebug[0m] [0m[0m      x(i) = sample[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // create linear model[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ols = new OLSMultipleLinearRegression()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ols.setNoIntercept(true) // We manually pass in the intercept[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // input sample data[0m
[0m[[0m[0mdebug[0m] [0m[0m    ols.newSampleData(phenotypes, x)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // calculate coefficients[0m
[0m[[0m[0mdebug[0m] [0m[0m    val beta = ols.estimateRegressionParameters().last[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // compute the regression parameters standard errors[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genoSE = ols.estimateRegressionParametersStandardErrors().last[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // test statistic t for jth parameter is equal to bj/SEbj, the parameter estimate divided by its standard error[0m
[0m[[0m[0mdebug[0m] [0m[0m    val t = beta / genoSE[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    /* calculate p-value and report:[0m
[0m[[0m[0mdebug[0m] [0m[0m     Under null hypothesis (i.e. the j'th element of weight vector is 0) the relevant distribution is[0m
[0m[[0m[0mdebug[0m] [0m[0m     a t-distribution with N-p-1 degrees of freedom.[0m
[0m[[0m[0mdebug[0m] [0m[0m     */[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tDist = new TDistribution(numObservations - observationLength)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val pvalue = 2 * tDist.cumulativeProbability(-Math.abs(t))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    RegressionStats(beta, genoSE, pvalue)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def generateTestData([0m
[0m[[0m[0mdebug[0m] [0m[0m      nSamples: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      nVariants: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      nRandomCovariates: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      includeIntercept: Boolean = true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      multiplier: Int = 1): TestData = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = Range(0, nVariants).map { _ =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      Range(0, nSamples).map(_ => multiplier * random.nextDouble())[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val phenotypes = Range(0, nSamples).map(_ => multiplier * random.nextDouble())[0m
[0m[[0m[0mdebug[0m] [0m[0m    val nCovariates = if (includeIntercept) nRandomCovariates + 1 else nRandomCovariates[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariates = Range(0, nSamples).map(_ => new Array[Double](nCovariates))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val startIdx = if (includeIntercept) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      covariates.foreach(_(0) = 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      1[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      0[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Range(startIdx, nCovariates).foreach { i =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      covariates.foreach(_(i) = multiplier * random.nextDouble())[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestData(genotypes, phenotypes, covariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def timeIt[T](opName: String)(f: => T): T = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val start = System.nanoTime()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ret = f[0m
[0m[[0m[0mdebug[0m] [0m[0m    val end = System.nanoTime()[0m
[0m[[0m[0mdebug[0m] [0m[0m    logger.info(s"Completed '$opName' in ${(end - start).nanos.toMillis}ms")[0m
[0m[[0m[0mdebug[0m] [0m[0m    ret[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareRegressionStats(s1: RegressionStats, s2: RegressionStats): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.beta ~== s2.beta relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.standardError ~== s2.standardError relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.pValue ~== s2.pValue relTol 0.02)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareToApacheOLS(testData: TestData, useSpark: Boolean): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val apacheResults = timeIt("Apache linreg") {[0m
[0m[[0m[0mdebug[0m] [0m[0m      testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        olsBaseline(g.toArray, testData.phenotypes.toArray, testData.covariates.toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ourResults = timeIt("DB linreg") {[0m
[0m[[0m[0mdebug[0m] [0m[0m      if (useSpark) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        val rows = testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m            g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m            testData.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m            twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        // Add id to preserve sorting[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark[0m
[0m[[0m[0mdebug[0m] [0m[0m          .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m          .withColumn("id", monotonically_increasing_id())[0m
[0m[[0m[0mdebug[0m] [0m[0m          .repartition(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m          .withColumn("linreg", expr("linear_regression_gwas(genotypes, phenotypes, covariates)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m          .orderBy("id")[0m
[0m[[0m[0mdebug[0m] [0m[0m          .selectExpr("expand_struct(linreg)")[0m
[0m[[0m[0mdebug[0m] [0m[0m          .as[RegressionStats][0m
[0m[[0m[0mdebug[0m] [0m[0m          .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m          .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m      } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m        val gwasContext = ComputeQR.computeQR(twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m        testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          LinearRegressionGwas.runRegression(g.toArray, testData.phenotypes.toArray, gwasContext)[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    apacheResults.zip(ourResults).map {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (ols, db) => compareRegressionStats(ols, db)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // The cars dataset built into R[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val cars: TestData = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val s =[0m
[0m[[0m[0mdebug[0m] [0m[0m      """[0m
[0m[[0m[0mdebug[0m] [0m[0m        |   speed dist[0m
[0m[[0m[0mdebug[0m] [0m[0m        |1      4    2[0m
[0m[[0m[0mdebug[0m] [0m[0m        |2      4   10[0m
[0m[[0m[0mdebug[0m] [0m[0m        |3      7    4[0m
[0m[[0m[0mdebug[0m] [0m[0m        |4      7   22[0m
[0m[[0m[0mdebug[0m] [0m[0m        |5      8   16[0m
[0m[[0m[0mdebug[0m] [0m[0m        |6      9   10[0m
[0m[[0m[0mdebug[0m] [0m[0m        |7     10   18[0m
[0m[[0m[0mdebug[0m] [0m[0m        |8     10   26[0m
[0m[[0m[0mdebug[0m] [0m[0m        |9     10   34[0m
[0m[[0m[0mdebug[0m] [0m[0m        |10    11   17[0m
[0m[[0m[0mdebug[0m] [0m[0m        |11    11   28[0m
[0m[[0m[0mdebug[0m] [0m[0m        |12    12   14[0m
[0m[[0m[0mdebug[0m] [0m[0m        |13    12   20[0m
[0m[[0m[0mdebug[0m] [0m[0m        |14    12   24[0m
[0m[[0m[0mdebug[0m] [0m[0m        |15    12   28[0m
[0m[[0m[0mdebug[0m] [0m[0m        |16    13   26[0m
[0m[[0m[0mdebug[0m] [0m[0m        |17    13   34[0m
[0m[[0m[0mdebug[0m] [0m[0m        |18    13   34[0m
[0m[[0m[0mdebug[0m] [0m[0m        |19    13   46[0m
[0m[[0m[0mdebug[0m] [0m[0m        |20    14   26[0m
[0m[[0m[0mdebug[0m] [0m[0m        |21    14   36[0m
[0m[[0m[0mdebug[0m] [0m[0m        |22    14   60[0m
[0m[[0m[0mdebug[0m] [0m[0m        |23    14   80[0m
[0m[[0m[0mdebug[0m] [0m[0m        |24    15   20[0m
[0m[[0m[0mdebug[0m] [0m[0m        |25    15   26[0m
[0m[[0m[0mdebug[0m] [0m[0m        |26    15   54[0m
[0m[[0m[0mdebug[0m] [0m[0m        |27    16   32[0m
[0m[[0m[0mdebug[0m] [0m[0m        |28    16   40[0m
[0m[[0m[0mdebug[0m] [0m[0m        |29    17   32[0m
[0m[[0m[0mdebug[0m] [0m[0m        |30    17   40[0m
[0m[[0m[0mdebug[0m] [0m[0m        |31    17   50[0m
[0m[[0m[0mdebug[0m] [0m[0m        |32    18   42[0m
[0m[[0m[0mdebug[0m] [0m[0m        |33    18   56[0m
[0m[[0m[0mdebug[0m] [0m[0m        |34    18   76[0m
[0m[[0m[0mdebug[0m] [0m[0m        |35    18   84[0m
[0m[[0m[0mdebug[0m] [0m[0m        |36    19   36[0m
[0m[[0m[0mdebug[0m] [0m[0m        |37    19   46[0m
[0m[[0m[0mdebug[0m] [0m[0m        |38    19   68[0m
[0m[[0m[0mdebug[0m] [0m[0m        |39    20   32[0m
[0m[[0m[0mdebug[0m] [0m[0m        |40    20   48[0m
[0m[[0m[0mdebug[0m] [0m[0m        |41    20   52[0m
[0m[[0m[0mdebug[0m] [0m[0m        |42    20   56[0m
[0m[[0m[0mdebug[0m] [0m[0m        |43    20   64[0m
[0m[[0m[0mdebug[0m] [0m[0m        |44    22   66[0m
[0m[[0m[0mdebug[0m] [0m[0m        |45    23   54[0m
[0m[[0m[0mdebug[0m] [0m[0m        |46    24   70[0m
[0m[[0m[0mdebug[0m] [0m[0m        |47    24   92[0m
[0m[[0m[0mdebug[0m] [0m[0m        |48    24   93[0m
[0m[[0m[0mdebug[0m] [0m[0m        |49    24  120[0m
[0m[[0m[0mdebug[0m] [0m[0m        |50    25   85[0m
[0m[[0m[0mdebug[0m] [0m[0m      """.stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m    val parsed = s[0m
[0m[[0m[0mdebug[0m] [0m[0m      .trim()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .split("\n")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map(l => l.split("\\s+").drop(1).map(_.toDouble))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = parsed.map(_(0))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val phenotypes = parsed.map(_(1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariates = genotypes.map(_ => Array(1d))[0m
[0m[[0m[0mdebug[0m] [0m[0m    TestData(Seq(genotypes), phenotypes, covariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("against R glm") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val golden = RegressionStats(3.932d, 0.4155d, 1.49e-12)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareRegressionStats([0m
[0m[[0m[0mdebug[0m] [0m[0m      golden,[0m
[0m[[0m[0mdebug[0m] [0m[0m      runRegression(cars.genotypes.head.toArray, cars.phenotypes.toArray, cars.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Sanity test to make sure that our OLS baseline is correct[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("against R glm (Apache OLS)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val golden = RegressionStats(3.932d, 0.4155d, 1.49e-12)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareRegressionStats([0m
[0m[[0m[0mdebug[0m] [0m[0m      golden,[0m
[0m[[0m[0mdebug[0m] [0m[0m      olsBaseline(cars.genotypes.head.toArray, cars.phenotypes.toArray, cars.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("intercept only, 1 site") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(10, 1, 0, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareToApacheOLS(testData, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("intercept only, many sites") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(10, 100, 0, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareToApacheOLS(testData, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("many covariates, many sites") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(30, 100, 26, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareToApacheOLS(testData, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("many covariates, many sites, with spark") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(30, 100, 26, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareToApacheOLS(testData, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("throws exception if matrix is singular") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val covariates = new DenseMatrix(3, 1, Array(1, 1, 1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ctx = ComputeQR.computeQR(covariates)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = Array(0d, 0d, 0d)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val phenotypes = Array(1d, 2d, 3d)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SingularMatrixException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      olsBaseline(genotypes, phenotypes, Array(Array(1), Array(1), Array(1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      LinearRegressionGwas.runRegression(genotypes, phenotypes, ctx)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.toLowerCase.contains("singular"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkIllegalArgumentException(rows: Seq[RegressionRow], error: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(rows)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("linreg", expr("linear_regression_gwas(genotypes, phenotypes, covariates)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.getMessage.contains(error))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("throws exception if more covariates than samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(26, 100, 30, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        testData.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkIllegalArgumentException(rows, "Number of covariates must be less than number of samples")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("throws exception if number of genotypes and phenotypes do not match") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(10, 1, 0, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.tail.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        testData.phenotypes.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkIllegalArgumentException([0m
[0m[[0m[0mdebug[0m] [0m[0m      rows,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "Number of samples differs between genotype and phenotype arrays")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("throws exception if number of samples does not match between genotypes and covariates") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val testData = generateTestData(10, 1, 0, true, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = testData.genotypes.map { g =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      RegressionRow([0m
[0m[[0m[0mdebug[0m] [0m[0m        g.tail.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        testData.phenotypes.tail.toArray,[0m
[0m[[0m[0mdebug[0m] [0m[0m        twoDArrayToSparkMatrix(testData.covariates.toArray))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkIllegalArgumentException([0m
[0m[[0m[0mdebug[0m] [0m[0m      rows,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "Number of samples differs between genotype array and covariate matrix")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{AnalysisException, DataFrame, Row}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.expr[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{VCFRow, VariantSchemas}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass SampleQcExprsSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testVcf = s"$testDataHome/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val na12878 = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy private val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("sample_call_summary_stats high level test")(Seq(true, false)) { sampleIds =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = readVcf(na12878, sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(qc[0])")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[TestSampleCallStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Golden value is from Hail 0.2.12[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = TestSampleCallStats([0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1075,[0m
[0m[[0m[0mdebug[0m] [0m[0m      0,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      893,[0m
[0m[[0m[0mdebug[0m] [0m[0m      181,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1082,[0m
[0m[[0m[0mdebug[0m] [0m[0m      85,[0m
[0m[[0m[0mdebug[0m] [0m[0m      98,[0m
[0m[[0m[0mdebug[0m] [0m[0m      659,[0m
[0m[[0m[0mdebug[0m] [0m[0m      423,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      659.toDouble / 423,[0m
[0m[[0m[0mdebug[0m] [0m[0m      85.toDouble / 98,[0m
[0m[[0m[0mdebug[0m] [0m[0m      893.toDouble / 181[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class GenotypeFields(calls: Array[Int], sampleId: String, depth: Option[Int])[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class SimpleVariant([0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceAllele: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      alternateAlleles: Seq[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes: Seq[GenotypeFields])[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def makeDf(ref: String, alts: Seq[String], calls: Array[Int]): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gf = GenotypeFields(calls, "monkey", None)[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark.createDataFrame(Seq(SimpleVariant(ref, alts, Seq(gf))))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def makeDf(dp: Seq[Option[Int]]): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val data = dp.map { d =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      val genotype = GenotypeFields(null, "monkey", d)[0m
[0m[[0m[0mdebug[0m] [0m[0m      SimpleVariant("A", Seq("T"), Seq(genotype))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark.createDataFrame(data)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def readVcf(path: String, includeSampleIds: Boolean = true): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", includeSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(4)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def toCallStats(df: DataFrame): Seq[TestSampleCallStats] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.selectExpr("sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles) as qc")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("explode(qc) as sqc")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(sqc)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[TestSampleCallStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("heterozygous only") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = toCallStats(makeDf("A", Seq("G"), Array(0, 1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.head.nHet == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.head.nTransition == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.head.nTransversion == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.head.rHetHomVar == Double.PositiveInfinity)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("empty dataframe") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = toCallStats(sess.createDataFrame(sess.sparkContext.emptyRDD[Row], VCFRow.schema))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.isEmpty) // No error expected[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("dp stats")(Seq(true, false)) { sampleIds =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = readVcf(na12878, includeSampleIds = sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("sample_dp_summary_stats(genotypes) as stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("explode(stats) as dp_stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(dp_stats)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get ~== 1 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get ~== 645 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get ~== 72.2 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get ~== 75.6 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("dp stats (with null)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = makeDf(Seq(Some(1), None, Some(3)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("sample_dp_summary_stats(genotypes) as stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("explode(stats) as stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(stats)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = ArraySummaryStats(Some(2), Some(Math.sqrt(2)), Some(1), Some(3))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("gq stats") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stats = readVcf(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("sample_gq_summary_stats(genotypes) as stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("explode(stats) as stats")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("expand_struct(stats)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[ArraySummaryStats][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.min.get ~== 3 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.max.get ~== 99 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.mean.get ~== 89.2 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stats.stdDev.get ~== 23.2 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val expressionsToTest = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    "expand_struct(sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles)[0])",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "expand_struct(sample_gq_summary_stats(genotypes)[0])",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "expand_struct(sample_dp_summary_stats(genotypes)[0])"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val testCases = expressionsToTest[0m
[0m[[0m[0mdebug[0m] [0m[0m    .flatMap(expr => Seq((expr, true), (expr, false)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("sample ids are propagated if included")(testCases) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case (expr, sampleIds) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m      val df = readVcf(na12878, sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr(expr)[0m
[0m[[0m[0mdebug[0m] [0m[0m      val outputSchema = df.schema[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(outputSchema.exists(_.name == VariantSchemas.sampleIdField.name) == sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m      if (sampleIds) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(df.select("sampleId").as[String].head == "NA12878")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val typeTransformations = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    ("genotypes", "referenceAllele", "Genotypes field must be an array of structs"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ([0m
[0m[[0m[0mdebug[0m] [0m[0m      "genotypes",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "transform(genotypes, gt -> subset_struct(gt, 'sampleId'))",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "Genotype struct was missing required fields"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ("referenceAllele", "alternateAlleles", "Reference allele must be a string"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ("alternateAlleles", "referenceAllele", "Alternate alleles must be an array of strings")[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("type check failures")(typeTransformations) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case (colName, colExpr, error) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m        readVcf(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m          .withColumn(colName, expr(colExpr))[0m
[0m[[0m[0mdebug[0m] [0m[0m          .selectExpr("sample_call_summary_stats(genotypes, referenceAllele, alternateAlleles)")[0m
[0m[[0m[0mdebug[0m] [0m[0m          .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(e.getMessage.contains(error))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class TestSampleCallStats([0m
[0m[[0m[0mdebug[0m] [0m[0m    callRate: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nCalled: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nUncalled: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nHomRef: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nHet: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nHomVar: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nSnp: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nInsertion: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nDeletion: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nTransition: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nTransversion: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    nSpanningDeletion: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m    rTiTv: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m    rInsertionDeletion: Double,[0m
[0m[[0m[0mdebug[0m] [0m[0m    rHetHomVar: Double)[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.AnalysisException[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass AggregateByIndexSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  private lazy val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("basic") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val results = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(Tuple1(Seq(1L, 2L, 3L))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("aggregate_by_index(_1, 0l, (x, y) -> x + y, (x, y) -> x + y, x -> x) as agg")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Long]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(results == Seq(1L, 2L, 3L))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("with types changes") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val results = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(Tuple1(Seq(1L, 2L, 3L))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("""[0m
[0m[[0m[0mdebug[0m] [0m[0m          |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m          |_1,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |0d,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(x, y) -> cast(x as double) + cast(y as double),[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(x, y) -> x + y,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |x -> cast(x as string))[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[String]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(results == Seq("1.0", "2.0", "3.0"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("with grouping") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        ("a", Seq(1, 2, 3)),[0m
[0m[[0m[0mdebug[0m] [0m[0m        ("b", Seq(4, 5, 6))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val results = df[0m
[0m[[0m[0mdebug[0m] [0m[0m      .groupBy("_1")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .agg(expr("aggregate_by_index(_2, 0, (x, y) -> x + y, (x, y) -> x + y, x -> x)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("_1")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[(String, Seq[Int])][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(results == Seq(("a", Seq(1, 2, 3)), ("b", Seq(4, 5, 6))))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("mean function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val result = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .range(10000)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("values", array_repeat(col("id"), 100))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr([0m
[0m[[0m[0mdebug[0m] [0m[0m        """[0m
[0m[[0m[0mdebug[0m] [0m[0m          |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m          |values,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |struct(0l, 0l),[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(state, el) -> struct(state.col1 + 1, state.col2 + el),[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(state1, state2) -> struct(state1.col1 + state2.col1, state1.col2 + state2.col2),[0m
[0m[[0m[0mdebug[0m] [0m[0m          |state -> if(state.col1 = 0, null, state.col2 / state.col1))[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Double]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(result == Seq.fill(100)(4999.5d))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("type error") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .range(10000)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .withColumn("values", array_repeat(col("id"), 100))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("""[0m
[0m[[0m[0mdebug[0m] [0m[0m            |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m            |values,[0m
[0m[[0m[0mdebug[0m] [0m[0m            |1,[0m
[0m[[0m[0mdebug[0m] [0m[0m            |(acc, el) -> size(acc), -- type error since size expects a map or array[0m
[0m[[0m[0mdebug[0m] [0m[0m            |(acc1, acc2) -> acc1 + acc2)[0m
[0m[[0m[0mdebug[0m] [0m[0m          """.stripMargin)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("no evaluate function") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val results = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .range(10)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("values", expr("transform(array_repeat(0, 5), (el, idx) -> idx)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("""[0m
[0m[[0m[0mdebug[0m] [0m[0m          |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m          |values,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |0,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc, el) -> acc + el,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc1, acc2) -> acc1 + acc2)[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    results.foreach { row =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(row == Seq(0, 10, 20, 30, 40))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("empty dataframe") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val result = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyDataset[Tuple1[Seq[Int]]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("""[0m
[0m[[0m[0mdebug[0m] [0m[0m          |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m          |_1,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |0,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc, el) -> acc + el,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc1, acc2) -> acc1 + acc2)[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(result == Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("null array") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val result = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(Tuple1(null), Tuple1(Seq(1))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("""[0m
[0m[[0m[0mdebug[0m] [0m[0m          |aggregate_by_index([0m
[0m[[0m[0mdebug[0m] [0m[0m          |_1,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |0,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc, el) -> acc + el,[0m
[0m[[0m[0mdebug[0m] [0m[0m          |(acc1, acc2) -> acc1 + acc2)[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Option[Int]]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(result == Seq(None))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.tertiary[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.ml.linalg.{DenseMatrix, DenseVector, SparseVector, Vector}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{AnalysisException, DataFrame, Row}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{DoubleType, IntegerType, StringType, StructField, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.unsafe.types.UTF8String[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.expressions.{VariantType, VariantUtilExprs}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VariantUtilExprsSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class SimpleGenotypeFields(calls: Seq[Int])[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class SimpleVariant(genotypes: Seq[SimpleGenotypeFields])[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def makeGenotypesDf(calls: Seq[Seq[Int]]): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypes = calls.map(c => SimpleGenotypeFields(c))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val variant = SimpleVariant(genotypes)[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark.createDataFrame(Seq(variant))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private lazy val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("simple cases") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val states = makeGenotypesDf(Seq(Seq(0, 0), Seq(1, 0), Seq(2, 2)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("genotype_states(genotypes)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(states == Seq(0, 1, 4))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("-1 if any -1 appears in call array") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val states = makeGenotypesDf(Seq(Seq(0, -1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("genotype_states(genotypes)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(states == Seq(-1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("-1 if call array is empty") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val states = makeGenotypesDf(Seq(Seq(), Seq(1, 1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("genotype_states(genotypes)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(states == Seq(-1, 2))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class TestCase(ref: String, alt: String, vt: VariantType)[0m
[0m[[0m[0mdebug[0m] [0m[0m  val bases = Seq("A", "C", "G", "T")[0m
[0m[[0m[0mdebug[0m] [0m[0m  val testCases = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("", "", VariantType.Unknown),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("ACG", "ATG", VariantType.Transition),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("AG", "ATG", VariantType.Insertion),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("AG", "ATCTCAG", VariantType.Insertion),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("ATG", "A", VariantType.Deletion),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("ACTGGGG", "AG", VariantType.Deletion),[0m
[0m[[0m[0mdebug[0m] [0m[0m      TestCase("A", "*", VariantType.SpanningDeletion)[0m
[0m[[0m[0mdebug[0m] [0m[0m    ) ++[0m
[0m[[0m[0mdebug[0m] [0m[0m    bases.flatMap(b1 => bases.map((b1, _))).collect {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (b1, b2) if b1 != b2 =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val sortedBases = Seq(b1, b2).sorted[0m
[0m[[0m[0mdebug[0m] [0m[0m        if (sortedBases == Seq("A", "G") || sortedBases == Seq("C", "T")) {[0m
[0m[[0m[0mdebug[0m] [0m[0m          TestCase(b1, b2, VariantType.Transition)[0m
[0m[[0m[0mdebug[0m] [0m[0m        } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m          TestCase(b1, b2, VariantType.Transversion)[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("variant type")(testCases) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case TestCase(ref, alt, expected) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      val t = VariantUtilExprs.variantType(UTF8String.fromString(ref), UTF8String.fromString(alt))[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(t == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("add struct field has correct schema") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(Outer(Inner(1, "monkey"))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val added = df.selectExpr("add_struct_fields(inner, 'number', 1, 'string', 'blah') as struct")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inner = added.schema.find(_.name == "struct").get.dataType.asInstanceOf[StructType][0m
[0m[[0m[0mdebug[0m] [0m[0m    val fields = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m      ("one", IntegerType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ("two", StringType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ("number", IntegerType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ("string", StringType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(inner.length == fields.size)[0m
[0m[[0m[0mdebug[0m] [0m[0m    fields.foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (name, typ) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(inner.exists(f => f.name == name && f.dataType == typ))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("add struct field has correct values") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val value = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(Outer(Inner(1, "monkey"))))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr([0m
[0m[[0m[0mdebug[0m] [0m[0m        "expand_struct(add_struct_fields(inner, 'three', " +[0m
[0m[[0m[0mdebug[0m] [0m[0m        "cast(3.14159 as double), 'four', true))"[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BigInner][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(value == BigInner(1, "monkey", 3.14159, true))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val hcTestCases = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(Seq(0.0, 0.0, 1.0), Some(1), Seq(1, 1), Some(false), None, "unphased"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0.1, 0.1, 0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(-1, -1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "unphased, below threshold"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0.1, 0.9, 0.8, 0.2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(1, -1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "phased, 1 below threshold"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0.1, 0.9, 0.8, 0.2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(1, 0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "phased, lower threshold"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(Seq(0, 1, 0, 0, 0, 1), Some(2), Seq(1, 2), Some(true), Some(0.8), "phased, 2 alts"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0.1, 0.9, 0.0, 0.1, 0.1, 0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(1, 2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "phased 2 alts (2)"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0.1, 0.1, 0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(1, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(0.8),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "unphased, lower threshold"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(Seq(0, 0, 0, 0, 1, 0), Some(2), Seq(2, 1), Some(false), None, "unphased, 2 alts"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0, 0, 0, 1, 0, 0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(2, 0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "unphased, 2 alts (2)"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(0, 0, 0, 0, 0, 0, 0, 1, 0, 0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(3),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(3, 1),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "unphased, 3 alts"[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(null, Some(1), null, Some(false), None, "null probabilities"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(Seq(1, 2), None, null, Some(false), None, "null num alts"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    HCTestCase(Seq(1, 2), Some(1), null, None, None, "null phasing")[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("hard calls")(hcTestCases) { testCase =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val thresholdStr = testCase.threshold.map(d => s", $d").getOrElse("")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input = spark.createDataFrame(Seq(testCase))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDF = input[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("calls", expr(s"hard_calls(probabilities, numAlts, phased $thresholdStr)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = outputDF[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[HCTestCase][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.calls == testCase.calls)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("hard calls casts input") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val res = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .range(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("hard_calls(array(0, 0, 1), cast(1 as bigint), false, 0.8)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Int]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(res == Seq(1, 1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("hard calls threshold must be constant") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .range(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("hard_calls(array(0, 0, 1), cast(1 as bigint), false, rand())")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Threshold must be a constant value"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val arrays: Seq[Seq[Double]] = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(1, 2, 3),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(Double.PositiveInfinity, Double.NegativeInfinity, Double.NaN),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(-1, -1.23, 3.14159)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("to/from sparse vector")(arrays) { array =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vectorDf = sess[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(DoubleArrayWrapper(array)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("array_to_sparse_vector(features) as features")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vector = vectorDf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VectorWrapper][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m      .features[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vector.isInstanceOf[SparseVector])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertSeqsMatch(array, vector.toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedArray = vectorDf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("vector_to_array(features) as features")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[DoubleArrayWrapper][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m      .features[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertSeqsMatch(array, convertedArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("to/from dense vector")(arrays) { array =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vectorDf = sess[0m
[0m[[0m[0mdebug[0m] [0m[0m      .createDataFrame(Seq(DoubleArrayWrapper(array)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("array_to_dense_vector(features) as features")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vector = vectorDf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VectorWrapper][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m      .features[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vector.isInstanceOf[DenseVector])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertSeqsMatch(array, vector.toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedArray = vectorDf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("vector_to_array(features) as features")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[DoubleArrayWrapper][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m      .features[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertSeqsMatch(array, convertedArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("cast input when converting to vector") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val seq = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .range(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("array_to_sparse_vector(cast(array(1, 2, 3) as array<int>)) as features")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("vector_to_array(features)")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Double]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(seq == Seq(1d, 2d, 3d))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def assertSeqsMatch(s1: Seq[Double], s2: Seq[Double]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    s1.zip(s2).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (d1, d2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(d1 == d2 || (d1.isNaN && d2.isNaN))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val baseMatrix = new DenseMatrix(4, 3, (1 to 12).map(_.toDouble).toArray)[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val matrices = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    ("dense col major", baseMatrix.toDenseColMajor),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ("dense row major", baseMatrix.toDenseRowMajor),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ("sparse col major", baseMatrix.toSparseColMajor),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ("sparse row major", baseMatrix.toSparseRowMajor)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("explode matrix")(matrices) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case (_, matrix) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m      val exploded = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(Seq(Tuple1(matrix)))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr("explode_matrix(_1)")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[Seq[Double]][0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      val expected = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(1, 5, 9),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(2, 6, 10),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(3, 7, 11),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(4, 8, 12)[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(exploded.toSeq == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("explode matrix (null)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(spark.sql("select explode_matrix(null)").count() == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("subset struct") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(BigOuter(BigInner(1, "monkey", 2.5, false))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val subsetted =[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.selectExpr("subset_struct(bigInner, 'one', 'three') as struct")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      subsetted.schema.find(_.name == "struct").get.dataType.asInstanceOf[StructType] == StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m        Seq(StructField("one", IntegerType), StructField("three", DoubleType))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(subsetted.select("struct").collect.head == Row(Row(1, 2.5)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("expand struct only works on structs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.createDataFrame(Seq(Outer(Inner(1, "monkey"))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[AnalysisException] { df.selectExpr("expand_struct(inner.two)").show() }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Only structs can be expanded"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class HCTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m    probabilities: Seq[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    numAlts: Option[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    calls: Seq[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    phased: Option[Boolean],[0m
[0m[[0m[0mdebug[0m] [0m[0m    threshold: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    name: String) {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def toString: String = name[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class Inner(one: Int, two: String)[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class BigInner(one: Int, two: String, three: Double, four: Boolean)[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class Outer(inner: Inner)[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class BigOuter(bigInner: BigInner)[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class DoubleArrayWrapper(features: Seq[Double])[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class VectorWrapper(features: Vector)[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.sources._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.broadinstitute.hellbender.utils.SimpleInterval[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GlowLogging, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.vcf.TabixIndexHelper._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass TabixHelperSuite extends GlowBaseTest with GlowLogging {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val sourceName: String = "vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val tabixTestVcf: String = s"$testDataHome/tabix-test-vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testVcf = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testBigVcf = s"$tabixTestVcf/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val multiAllelicVcf = s"$tabixTestVcf/combined.chr20_18210071_18210093.g.vcf.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testNoTbiVcf = s"$tabixTestVcf/NA12878_21_10002403NoTbi.vcf.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def sparkConf: SparkConf = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    super[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.hadoop.io.compression.codecs", "org.seqdoop.hadoop_bam.util.BGZFCodec")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def printFilterContig(filterContig: FilterContig): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    filterContig.getContigName.foreach(i => logger.debug(s"$i"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def printFilterInterval(filterInterval: FilterInterval): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    filterInterval[0m
[0m[[0m[0mdebug[0m] [0m[0m      .getSimpleInterval[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach(i => logger.debug(s"${i.getContig}, ${i.getStart}, ${i.getEnd}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def printParsedFilterResult(parsedFilterResult: ParsedFilterResult): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    printFilterContig(parsedFilterResult.contig)[0m
[0m[[0m[0mdebug[0m] [0m[0m    printFilterInterval(parsedFilterResult.startInterval)[0m
[0m[[0m[0mdebug[0m] [0m[0m    printFilterInterval(parsedFilterResult.endInterval)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def isSameOptionSimpleInterval(i: Option[SimpleInterval], j: Option[SimpleInterval]): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    (i, j) match {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (Some(i), Some(j)) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        if (i.getContig == "") {[0m
[0m[[0m[0mdebug[0m] [0m[0m          i.getContig == j.getContig[0m
[0m[[0m[0mdebug[0m] [0m[0m        } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m          i.getContig == j.getContig && i.getStart == j.getStart && i.getEnd == j.getEnd[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (None, _) => j.isEmpty[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (_, None) => i.isEmpty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def isSameParsedFilterResult(i: ParsedFilterResult, j: ParsedFilterResult): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    i.contig.isSame(j.contig) &&[0m
[0m[[0m[0mdebug[0m] [0m[0m    i.startInterval.isSame(j.startInterval) && i.endInterval.isSame(j.endInterval)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Tests to ensure getSmallestQueryInterval produces the correct interval given different[0m
[0m[[0m[0mdebug[0m] [0m[0m   * start and end interval situations[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testGetSmallestQueryInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      ss: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      se: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      es: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ee: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      xs: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      xe: Long): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val actual = getSmallestQueryInterval(new FilterInterval(ss, se), new FilterInterval(es, ee))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = new FilterInterval(xs, xe)[0m
[0m[[0m[0mdebug[0m] [0m[0m    actual[0m
[0m[[0m[0mdebug[0m] [0m[0m      .getSimpleInterval[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach(i => logger.debug(s"${i.getContig}, ${i.getStart}, ${i.getEnd}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    printFilterInterval(actual)[0m
[0m[[0m[0mdebug[0m] [0m[0m    printFilterInterval(expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(actual.isSame(expected))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: non-overlapping start and end intervals") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(1000, 2000, 3000, 4000, 3000, 3000)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: non-overlapping touching") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // note that start will be incremented by one before overlapping[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(1000, 2000, 2001, 4000, 2001, 2001)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: overlapping by one") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Note that start will be incremented by one before overlapping)[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(1000, 2001, 2001, 4000, 2001, 2001)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: overlapping by more than 1") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(1000, 2000, 1500, 4000, 1500, 2000)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: start after end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(2000, 4000, 1000, 1999, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: start almost after end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(2000, 4000, 1000, 2000, 2000, 2000)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: no start") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(2, 1, 1000, 2001, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("getSmallestQueryInterval: no end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testGetSmallestQueryInterval(2000, 4000, 2, 1, 2, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Tests to ensure parseFilter returns the correct ParsedFilterResult given different[0m
[0m[[0m[0mdebug[0m] [0m[0m   * filter situations.[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      filters: Seq[Filter],[0m
[0m[[0m[0mdebug[0m] [0m[0m      contigName: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ss: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      se: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      es: Long,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ee: Long): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val actual = parseFilter(filters)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = ParsedFilterResult([0m
[0m[[0m[0mdebug[0m] [0m[0m      new FilterContig(contigName),[0m
[0m[[0m[0mdebug[0m] [0m[0m      new FilterInterval(ss, se),[0m
[0m[[0m[0mdebug[0m] [0m[0m      new FilterInterval(es, ee))[0m
[0m[[0m[0mdebug[0m] [0m[0m    printParsedFilterResult(actual)[0m
[0m[[0m[0mdebug[0m] [0m[0m    printParsedFilterResult(expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(isSameParsedFilterResult(actual, expected))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: contig, start <, end >") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThan("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThan("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004770,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004776,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: contig, start <=, end >=") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: contig, start >=, end <=") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: contig, start >, end <") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThan("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThan("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004772,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004774[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: contig, start =, end =") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: No contig, No Start, No End") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter(Seq(), "", 1, Int.MaxValue, 1, Int.MaxValue)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: inconsistent contig") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("end", 10004775L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "12")[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      null,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: unsupported conditions on contig") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Note: The detection of empty result set is deferred to Spark filtering.[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("end", 10004775L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "12")[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "12",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: And with equals") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        And(EqualTo("start", 10004770L), EqualTo("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: And with inequalities non-overlapping") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        And(LessThanOrEqual("start", 10004770L), GreaterThanOrEqual("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: Or with equals") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or(EqualTo("start", 10004770L), EqualTo("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: Or with inequalities non-overlapping") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or(LessThanOrEqual("start", 10004770L), GreaterThanOrEqual("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: Or with inequalities non-overlapping touching") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or(LessThanOrEqual("start", 10004774L), GreaterThanOrEqual("end", 10004775L))),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: Or with inequalities overlapping") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or(LessThanOrEqual("start", 10004775L), GreaterThanOrEqual("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: Or with inequalities overlapping reverse") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or(GreaterThanOrEqual("start", 10004775L), LessThanOrEqual("end", 10004775L))[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Int.MaxValue)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: And nested in Or") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or([0m
[0m[[0m[0mdebug[0m] [0m[0m          And(GreaterThan("start", 10004223L), LessThan("end", 10004500L)),[0m
[0m[[0m[0mdebug[0m] [0m[0m          And(GreaterThan("start", 10003500L), LessThan("end", 10004000L))[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10003502,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004499,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10003502,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004499[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parseFilter: And nested in Or nested in Or") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParseFilter([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        Or([0m
[0m[[0m[0mdebug[0m] [0m[0m          Or([0m
[0m[[0m[0mdebug[0m] [0m[0m            And(GreaterThan("start", 10004223L), LessThan("end", 10004500L)),[0m
[0m[[0m[0mdebug[0m] [0m[0m            And(GreaterThan("start", 10003500L), LessThan("end", 10004000L))[0m
[0m[[0m[0mdebug[0m] [0m[0m          ),[0m
[0m[[0m[0mdebug[0m] [0m[0m          EqualTo("end", 10004725L)[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10003502,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004725,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10003502,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004725[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Tests to ensure makeFilteredInterval returns the correct Option[SimpleInterval][0m
[0m[[0m[0mdebug[0m] [0m[0m   * given different filter situations[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      filters: Seq[Filter],[0m
[0m[[0m[0mdebug[0m] [0m[0m      useFilterParser: Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      useIndex: Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      contigName: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      s: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      e: Int): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val actual = makeFilteredInterval(filters, useFilterParser, useIndex)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = Option(new SimpleInterval(contigName, s, e))[0m
[0m[[0m[0mdebug[0m] [0m[0m    actual.foreach(i => logger.debug(s"${i.getContig}, ${i.getStart}, ${i.getEnd}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    expected.foreach(i => logger.debug(s"${i.getContig}, ${i.getStart}, ${i.getEnd}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(isSameOptionSimpleInterval(actual, expected))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      filters: Seq[Filter],[0m
[0m[[0m[0mdebug[0m] [0m[0m      useFilterParser: Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      useIndex: Boolean): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val actual = makeFilteredInterval(filters, useFilterParser, useIndex)[0m
[0m[[0m[0mdebug[0m] [0m[0m    actual.foreach(i => logger.debug(s"${i.getContig}, ${i.getStart}, ${i.getEnd}"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(isSameOptionSimpleInterval(actual, None))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start and end intervals non-overlapping") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThan("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThan("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004776,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004776[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start and end intervals non-overlapping touching ") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("start", 10004769L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("end", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start and end intervals non-overlapping by 1 ") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("end", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start and end intervals overlapping more than 1") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004775L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004775[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start after end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004770L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: start almost after end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: no contig") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      1[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: inconsistent contig") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        GreaterThanOrEqual("start", 10004770L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004771L),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "21")[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: no start") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("end", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004771[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("makeFilteredInterval: no end") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testMakeFilteredInterval([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("contigName"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("end"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        IsNotNull("start"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        EqualTo("contigName", "20"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        LessThanOrEqual("start", 10004771L)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      10004772[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Tests to ensure simultaneously setting useTabixIndex to true and useFilterParser to false results in an exception.[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("useFilterParser = false while useTabixIndex = true") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    try {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val dfWithTabix = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("useTabixIndex", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("useFilterParser", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(testBigVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .filter("contigName= '20' and start < 10004770 and end > 10004775")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      dfWithTabix.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case e: IllegalArgumentException => assert(true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      case _: Throwable => assert(false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Test to ensure no filter does not cause errors[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("no filter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfEmptyFilter = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter("contigName >= 20 ")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfEmptyFilter.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val withEmptyFilter = dfEmptyFilter.orderBy("contigName", "start").as[VCFRow].collect().toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfNoFilter = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfNoFilter.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val withNoFilter = dfNoFilter.orderBy("contigName", "start").as[VCFRow].collect().toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (dfEmptyFilter.count() == dfNoFilter.count()) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withEmptyFilter.zip(withNoFilter).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (ef, nf) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(ef.contigName == nf.contigName)[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(ef.start == nf.start)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      fail()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Tests to ensure invalid BGZ and absence of tbi does not cause errors[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("invalid BGZ") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter("contigName= '20' and start < 10004770 and end > 10004775")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No index file found") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testNoTbiVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter("contigName= '21' and start = 10002435")[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Tests that the variants returned for different filter statements are the same in[0m
[0m[[0m[0mdebug[0m] [0m[0m   * the three following cases:[0m
[0m[[0m[0mdebug[0m] [0m[0m   * 1. Filter parser and Tabix index are both used.[0m
[0m[[0m[0mdebug[0m] [0m[0m   * 2. Filter parser is used but tabix index is not.[0m
[0m[[0m[0mdebug[0m] [0m[0m   * 3. Neither is used.[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testParserAndTabix(fileName: String, condition: String, splitToBiallelic: Boolean): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfFT = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("splitToBiallelic", splitToBiallelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(fileName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(condition)[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfFT.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val withFT = dfFT.orderBy("contigName", "start").as[VCFRow].collect().toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfFN = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("splitToBiallelic", splitToBiallelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("useTabixIndex", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(fileName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(condition)[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfFN.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val withFN = dfFN.orderBy("contigName", "start").as[VCFRow].collect().toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfNN = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("splitToBiallelic", splitToBiallelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("useTabixIndex", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("useFilterParser", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(fileName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .filter(condition)[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfNN.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val withNN = dfNN.orderBy("contigName", "start").as[VCFRow].collect().toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (dfNN.count() == dfFT.count() && dfNN.count() == dfFN.count()) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      withFT.zip(withFN).zip(withNN).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case ((ft, fn), nn) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          logger.debug(s"${ft.contigName}, ${ft.start}")[0m
[0m[[0m[0mdebug[0m] [0m[0m          logger.debug(s"${fn.contigName}, ${fn.start}")[0m
[0m[[0m[0mdebug[0m] [0m[0m          logger.debug(s"${nn.contigName}, ${nn.start}")[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(ft.contigName == nn.contigName && fn.contigName == nn.contigName)[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(ft.start == nn.start && fn.start == nn.start)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      fail()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testParserAndTabix(fileName: String, condition: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(fileName, condition, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Parser/Tabix vs Not") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start > 0")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start >= 0")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Filter parser skips negative parameters and defers to spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start > -1")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start = 10004193 and end > -12")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Some corner cases[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start > 10004193")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start >= 10004193")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start <= 10004768 and end >= 10004779")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start <= 10004768 and end > 10004779")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start < 10004768 and end >= 10004779")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and start > 10001433 and end < 10001445")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBigVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "contigName = '20' and ((start>10004223 and end <10004500) or " +[0m
[0m[[0m[0mdebug[0m] [0m[0m      "(start > 10003500 and end < 10004000))")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBigVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "contigName= '20' and ((start>10004223 and end <10004500) or " +[0m
[0m[[0m[0mdebug[0m] [0m[0m      "(start > 10003500 and end < 10004000) or (end= 10004725))")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix(testBigVcf, "contigName= '20' and (start=10000211 or end=10003817)")[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBigVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "contigName= '20' and ((start>10004223 and end <10004500) or " +[0m
[0m[[0m[0mdebug[0m] [0m[0m      "(start > 10003500 and end < 10004000)) and contigName='20'")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // FilterParser unsupported logical operators must be handled correctly as well.[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBigVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "contigName= '20' and (not(start>10004223 and end <10004500) " +[0m
[0m[[0m[0mdebug[0m] [0m[0m      "or not(start > 10003500 and end < 10004000))")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Test multi-allelic case works as well[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Parser/Tabix vs Not: Multi-allelic ") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testParserAndTabix([0m
[0m[[0m[0mdebug[0m] [0m[0m      multiAllelicVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "contigName= 'chr20' and (start = 18210074 or end = 18210084)",[0m
[0m[[0m[0mdebug[0m] [0m[0m      true[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Files[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.VCFConstants[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.io.FileUtils[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.hadoop.fs.Path[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.{InternalRow, ScalaReflection}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.execution.datasources.PartitionedFile[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.execution.datasources.csv.CSVFileFormat[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{StringType, StructField, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{DataFrame, Dataset}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.{SparkConf, SparkException}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GenotypeFields, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFDatasourceSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val sourceName = "vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testVcf = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val multiAllelicVcf = s"$testDataHome/combined.chr20_18210071_18210093.g.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val tgpVcf = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val stringInfoFieldsVcf = s"$testDataHome/test.chr17.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def sparkConf: SparkConf = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    super[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.hadoop.io.compression.codecs", "org.seqdoop.hadoop_bam.util.BGZFCodec")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def makeVcfRow(strSeq: Seq[String]): String = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    (Seq("1", "1", "id", "C", "T,GT", "1", ".") ++ strSeq).mkString("\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("default schema") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.read.format(sourceName).load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.schema.exists(_.name.startsWith("INFO_")))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.where(expr("size(filter(genotypes, g -> g.sampleId is null)) > 0")).count() == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parse VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val datasource = spark.read.format(sourceName).load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    datasource.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("no sample ids") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("g", expr("genotypes[0]"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("g.*")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!schema.exists(_.name == "sampleId"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("with sample ids") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val datasource = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val size = datasource.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(datasource.where("genotypes[0].sampleId = 'NA12878'").count() == size)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("check parsed row") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val datasource = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = VCFRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      9999995,[0m
[0m[[0m[0mdebug[0m] [0m[0m      9999996,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("ACT"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(3775.73),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "AC" -> "2",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "AF" -> "1.00",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "AN" -> "2",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "DP" -> "84",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "ExcessHet" -> "3.0103",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "FS" -> "0.000",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "MLEAC" -> "2",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "MLEAF" -> "1.00",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "MQ" -> "60.44",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "QD" -> "25.36",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "SOR" -> "1.075"[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        GenotypeFields([0m
[0m[[0m[0mdebug[0m] [0m[0m          Some("NA12878"),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(Seq(1, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Option(84),[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(Seq(3813, 256, 0)),[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Option(99),[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(Seq(0, 84)),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      false[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareRows(datasource.orderBy("contigName", "start").as[VCFRow].head(), expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("multiple genotype fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.read.format(sourceName).load(input)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.selectExpr("size(genotypes)").as[Int].head == 2504)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("filter with and without pushdown returns same results") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    def checkResultsMatch(f: DataFrame => Long): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val withPushdown = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("enablePredicatePushdown", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      val withoutPushdown = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("enablePredicatePushdown", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(f(withPushdown) == f(withoutPushdown))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkResultsMatch(_.count())[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkResultsMatch { df =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.where(expr("qual > 1"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .agg(expr("max(size(genotypes))"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[Long][0m
[0m[[0m[0mdebug[0m] [0m[0m        .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkResultsMatch { df =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.where(expr("size(filter(genotypes, g -> g.calls[0] = 0)) > 50")).count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def parseVcfContents(row: String, nSamples: Int = 1): Dataset[VCFRow] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = Files.createTempFile("test-vcf", ".vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val samples = (1 to nSamples).map(n => s"sample_$n").mkString("\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headers =[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"##fileformat=VCFv4.2\n" +[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\t$samples\n"[0m
[0m[[0m[0mdebug[0m] [0m[0m    FileUtils.writeStringToFile(file.toFile, headers + row)[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(file.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uncalled genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("AC=2", "GT", "."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(row.genotypes.head.calls.get == Seq(-1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("split uncalled genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("AC=2", "GT", "./."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(row.genotypes.head.calls.get == Seq(-1, -1))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("phased genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("AC=2", "GT", "0|0"))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotype = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(genotype.calls.get == Seq(0, 0))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(genotype.phased.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("AC=2", "GT", "0/0"))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotype = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(genotype.calls.get == Seq(0, 0))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!genotype.phased.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("format field flags") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("DB", "GT", "1|2"))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val attributes = row.attributes[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(attributes.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(attributes.get("DB").contains(""))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing info values") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("AC=2", "GT:MIN_DP:SB", "1|2:.:3,4,5,6"))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val otherFields = row.genotypes.head.otherFields[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(otherFields.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!otherFields.contains("MIN_DP"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing format values") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents([0m
[0m[[0m[0mdebug[0m] [0m[0m      makeVcfRow(Seq("AC=2", "GT:DP:FT:GL:PL:GP:GQ:HQ:EC:MQ:AD", ".:.:.:.:.:.:.:.:.:.:."))[0m
[0m[[0m[0mdebug[0m] [0m[0m    ).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.depth.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.filters.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.genotypeLikelihoods.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.phredLikelihoods.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.posteriorProbabilities.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.conditionalQuality.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.haplotypeQualities.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.expectedAlleleCounts.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.mappingQuality.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.alleleDepths.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("dropped trailing format values") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents([0m
[0m[[0m[0mdebug[0m] [0m[0m      makeVcfRow(Seq("AC=2", "GT:MIN_DP:DP:FT:GL:PL:GP:GQ:HQ:EC:MQ:AD:SB", ".:5"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    ).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.depth.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.filters.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.genotypeLikelihoods.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.phredLikelihoods.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.posteriorProbabilities.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.conditionalQuality.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.haplotypeQualities.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.expectedAlleleCounts.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.mappingQuality.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.alleleDepths.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.otherFields.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.otherFields.contains("SB"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing GT format field") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq(".", "GL", "."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.phased.contains(false)) // By default, HTSJDK parses VCs as unphased[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.calls.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing calls are -1 (zero present") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq(".", "GT", "./."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.calls.get == Seq(-1, -1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.phased.contains(false))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing calls are -1 (only one present)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq(".", "GT", "1|."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = row.genotypes.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.calls.get == Seq(1, -1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.phased.contains(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("set END field") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = parseVcfContents(makeVcfRow(Seq("END=200", "GT", "."))).head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(row.end == 200)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("read VCFv4.3") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testDataHome/vcf/VCFv4.3.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(input.count == 5)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("split to biallelic variant contexts") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("splitToBiallelic", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(multiAllelicVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.collect.foreach { vc =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(vc.alternateAlleles.length < 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      if (vc.start < 18210074) {[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(!vc.splitFromMultiAllelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m      } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc.splitFromMultiAllelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = makeVcfRow(Seq("AC=monkey"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = Files.createTempFile("test-vcf", ".vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headers =[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"""##fileformat=VCFv4.2[0m
[0m[[0m[0mdebug[0m] [0m[0m         |##INFO=<ID=AC,Number=1,Type=Integer,Description="">[0m
[0m[[0m[0mdebug[0m] [0m[0m         |#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m    FileUtils.writeStringToFile(file.toFile, headers + row)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("validationStringency", "strict")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(file.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException](ds.collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("invalid validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("validationStringency", "fakeStringency")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def parseDF([0m
[0m[[0m[0mdebug[0m] [0m[0m      row: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      nSamples: Int = 1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      extraHeaderLines: String = ""): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = Files.createTempFile("test-vcf", ".vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val samples = (1 to nSamples).map(n => s"sample_$n").mkString("\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headers =[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"##fileformat=VCFv4.2\n" + extraHeaderLines +[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\t$samples\n"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rowStr = headers + row[0m
[0m[[0m[0mdebug[0m] [0m[0m    FileUtils.writeStringToFile(file.toFile, rowStr)[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(file.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("flatten INFO fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = ds[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select(ds.colRegex("`INFO.*`"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[INFOFields][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_AC == Seq(Some(2)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_AF == Seq(Some(1.00)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_AN.contains(2))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_BaseQRankSum.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_DP.contains(84))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_DS.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_ExcessHet.get ~== 3.0103 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_FS.get ~== 0.000 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_MLEAC == Seq(Some(2)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_MLEAF.head.get ~== 1.00 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_MQ.get ~== 60.44 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_MQRankSum.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_QD.get ~== 25.36 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_ReadPosRankSum.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.INFO_SOR.get ~== 1.075 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("flattened INFO fields schema does not include END key") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(tgpVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!schema.fieldNames.contains(VCFConstants.END_KEY))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("flattened INFO fields schema merged for multiple files") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf, tgpVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.fieldNames.length == 49)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.fieldNames.contains("INFO_MQRankSum")) // only in CEUTrio[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.fieldNames.contains("INFO_EX_TARGET")) // only in 1KG[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("prune a flattened INFO field") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dpDf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select(avg("INFO_DP"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Double][0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(dpDf.head ~== 75.50232558139535 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("parse string INFO fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val platformNamesDf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(stringInfoFieldsVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select("INFO_platformnames")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[String]][0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(platformNamesDf.head == Seq("Illumina", "CG", "10X", "Solid"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("partitioned file without all of header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // The real header is >7KB, so this contains a truncated header[0m
[0m[[0m[0mdebug[0m] [0m[0m    val partitionedFile = PartitionedFile(InternalRow(), testVcf, 0L, 6000L)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfFileFormat = new VCFFileFormat()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rowIter = vcfFileFormat.buildReader([0m
[0m[[0m[0mdebug[0m] [0m[0m      spark,[0m
[0m[[0m[0mdebug[0m] [0m[0m      StructType(Seq(StructField("value", StringType))),[0m
[0m[[0m[0mdebug[0m] [0m[0m      StructType(Seq.empty),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ScalaReflection.schemaFor[VCFRow].dataType.asInstanceOf[StructType],[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("validationStringency" -> "SILENT"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.sparkContext.hadoopConfiguration[0m
[0m[[0m[0mdebug[0m] [0m[0m    )(partitionedFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rowIter.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("gzip'd files are not considered to be splittable") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fileFormat = new VCFFileFormat()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testDataHome/vcf/1row_not_bgz.vcf.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!fileFormat.isSplitable(spark, Map.empty, new Path(path)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("misnumbered fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testDataHome/vcf/misnumbered_info.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .rdd[0m
[0m[[0m[0mdebug[0m] [0m[0m      .count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rows == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("multiple rows") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect() // Should not get an error[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("infer non standard format fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headerLines = s"""##FORMAT=<ID=MONKEY,Number=1,Type=String,Description="">[0m
[0m[[0m[0mdebug[0m] [0m[0m                         |##FORMAT=<ID=NUMBERS,Number=5,Type=Float,Description="">[0m
[0m[[0m[0mdebug[0m] [0m[0m                         |""".stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rowStr = makeVcfRow(Seq(".", "MONKEY:NUMBERS", "banana:1,2,3"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val value = parseDF(rowStr, extraHeaderLines = headerLines)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("genotypes[0].MONKEY")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[String][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(value == "banana")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val value2 = parseDF(rowStr, extraHeaderLines = headerLines)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .selectExpr("genotypes[0].NUMBERS")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[Seq[Double]][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(value2 == Seq(1, 2, 3))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class WeirdSchema(animal: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("be permissive if schema includes fields that can't be derived from VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(ScalaReflection.schemaFor[WeirdSchema].dataType.asInstanceOf[StructType])[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect() // No error expected[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("add BGZ codec when reading VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gzPath = new Path(s"$testDataHome/vcf/1row_bgz.vcf.gz")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val bgzPath = new Path(s"$testDataHome/vcf/1row.vcf.bgz")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfFormat = new VCFFileFormat()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val csvFormat = new CSVFileFormat()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfFormat.isSplitable(spark, Map.empty, gzPath))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!csvFormat.isSplitable(spark, Map.empty, gzPath))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfFormat.isSplitable(spark, Map.empty, bgzPath))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(csvFormat.isSplitable(spark, Map.empty, bgzPath))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Tolerate lower-case nan's") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfRows = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testDataHome/vcf/test_withNanQual.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcfRows.foreach { vc =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(vc.qual.get.isNaN)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareRows(r1: VCFRow, r2: VCFRow): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(r1.copy(qual = None) == r2.copy(qual = None))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(r1.qual.isDefined == r2.qual.isDefined)[0m
[0m[[0m[0mdebug[0m] [0m[0m    for {[0m
[0m[[0m[0mdebug[0m] [0m[0m      q1 <- r1.qual[0m
[0m[[0m[0mdebug[0m] [0m[0m      q2 <- r2.qual[0m
[0m[[0m[0mdebug[0m] [0m[0m    } {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(q1 ~== q2 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m// For testing only: schema based on CEUTrio VCF header[0m
[0m[[0m[0mdebug[0m] [0m[0mprivate case class INFOFields([0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_AC: Seq[Option[Int]],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_AF: Seq[Option[Double]],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_AN: Option[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_BaseQRankSum: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_DP: Option[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_DS: Option[Boolean],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_ExcessHet: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_FS: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_InbreedingCoeff: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_MLEAC: Seq[Option[Int]],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_MLEAF: Seq[Option[Double]],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_MQ: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_MQRankSum: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_QD: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_ReadPosRankSum: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m    INFO_SOR: Option[Double])[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.File[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.lang.{Double => JDouble, Integer => JInteger}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.util.{ArrayList => JArrayList, HashSet => JHashSet}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.ValidationStringency[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.variantcontext.{Allele, GenotypeBuilder, VariantContextBuilder}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.{VCFFileReader, VCFHeader}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GenotypeFields, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VariantContextToVCFRowConverterSuite extends GlowBaseTest with VCFConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val NA12878 = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val TGP = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val GVCF = s"$testDataHome/NA12878_21_10002403.g.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val defaultHeader = new VCFHeader()[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val defaultConverter = new VariantContextToVCFRowConverter(defaultHeader)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def compareVcfRows(vcf: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new VariantContextToVCFRowConverter(header)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sparkVcfRowList = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = new File(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val reader = new VCFFileReader(file, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val htsjdkVcList = reader.iterator.toList.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    val htsjdkVcfRowList = htsjdkVcList.map(converter.convert)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sparkVcfRowList.length == htsjdkVcfRowList.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    sparkVcfRowList.zip(htsjdkVcfRowList).map {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (sparkVcfRow, htsjdkVcfRow) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          sparkVcfRow.copy(qual = None) == htsjdkVcfRow.copy(qual = None),[0m
[0m[[0m[0mdebug[0m] [0m[0m          s"\nVC1 $sparkVcfRow\nVC2 $htsjdkVcfRow"[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m        if (sparkVcfRow.qual.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert([0m
[0m[[0m[0mdebug[0m] [0m[0m            sparkVcfRow.qual.get ~== htsjdkVcfRow.qual.get relTol 0.2,[0m
[0m[[0m[0mdebug[0m] [0m[0m            s"VC1 qual ${sparkVcfRow.qual.get} VC2 qual ${htsjdkVcfRow.qual.get}"[0m
[0m[[0m[0mdebug[0m] [0m[0m          )[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Single sample") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfRows(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Multiple samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfRows(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("GVCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfRows(GVCF)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Default VariantContext") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcb = new VariantContextBuilder()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.chr("")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.start(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.stop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refAllele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.alleles(Seq(refAllele).asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val defaultVc = vcb.make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfRow = defaultConverter.convert(defaultVc)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedDefaultVc = defaultVcfRow.copy(end = 1, referenceAllele = "A", genotypes = Nil)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfRow == convertedDefaultVc)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Default Genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcb = new VariantContextBuilder()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.chr("").start(1).stop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refAllele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.alleles(Seq(refAllele).asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.genotypes(new GenotypeBuilder().make)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcWithDefaultGt = vcb.make.fullyDecode(defaultHeader, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfRow = defaultConverter.convert(vcWithDefaultGt)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedVcWithDefaultGt = defaultVcfRow.copy([0m
[0m[[0m[0mdebug[0m] [0m[0m      end = 1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceAllele = "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes = Seq(defaultGenotypeFields.copy(phased = Some(false)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfRow == convertedVcWithDefaultGt)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Set VariantContext and Genotypes") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refAllele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val altAllele1 = Allele.create("T")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val altAllele2 = Allele.create("C")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt1Alleles = new JArrayList[Allele]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt1Alleles.add(refAllele)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt1Alleles.add(altAllele1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gb1 = new GenotypeBuilder("sample1", gt1Alleles)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.phased(true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.GQ(3)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.DP(4)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.AD(Array(10, 11, 12))[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.PL(Array(20, 21, 22, 23, 24, 25, 26))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt1Filters = new JArrayList[String]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt1Filters.add("gtFilter1")[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt1Filters.add("gtFilter2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.filters(gt1Filters)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("GP", "2.1,2.2,2.3,2.4,2.5,2.6")[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("HQ", Array(31, 32))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ecArrayList = new JArrayList[JInteger]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ecArrayList.add(41)[0m
[0m[[0m[0mdebug[0m] [0m[0m    ecArrayList.add(42)[0m
[0m[[0m[0mdebug[0m] [0m[0m    ecArrayList.add(43)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("EC", ecArrayList)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("MQ", "5")[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("NullKey", null)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("MissingKey", ".")[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("BoolKey", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("IntKey", new JInteger(150))[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("DoubleKey", 1.5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb1.attribute("StringKey", "gtStringVal")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt1 = gb1.make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt2Alleles = new JArrayList[Allele]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt2Alleles.add(altAllele2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    gt2Alleles.add(Allele.NO_CALL)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt2 = new GenotypeBuilder("sample2", gt2Alleles).make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcAlleles = new JArrayList[Allele]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcAlleles.add(refAllele)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcAlleles.add(altAllele1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcAlleles.add(altAllele2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcb = new VariantContextBuilder("source", "contigName", 101, 101, vcAlleles)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.log10PError(-1.0).id("id1;id2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcFilters = new JHashSet[String]()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcFilters.add("filter1")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcFilters.add("filter2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.filters(vcFilters)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("NullKey", null)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("MissingKey", ".")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("BoolKey", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("IntKey", 50)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("DoubleKey", new JDouble(0.5))[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("StringKey", "stringVal")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.genotypes(gt1, gt2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = vcb.make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfRow = defaultConverter.convert(vc)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedGt1 = GenotypeFields([0m
[0m[[0m[0mdebug[0m] [0m[0m      sampleId = Some("sample1"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      phased = Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      calls = Some(Seq(0, 1)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      depth = Some(4),[0m
[0m[[0m[0mdebug[0m] [0m[0m      filters = Some(Seq("gtFilter1", "gtFilter2")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypeLikelihoods = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      phredLikelihoods = Some(Seq(20, 21, 22, 23, 24, 25, 26)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      posteriorProbabilities = Some(Seq(2.1, 2.2, 2.3, 2.4, 2.5, 2.6)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      conditionalQuality = Some(3),[0m
[0m[[0m[0mdebug[0m] [0m[0m      haplotypeQualities = Some(Seq(31, 32)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedAlleleCounts = Some(Seq(41, 42, 43)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      mappingQuality = Some(5),[0m
[0m[[0m[0mdebug[0m] [0m[0m      alleleDepths = Some(Seq(10, 11, 12)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      otherFields = Map("IntKey" -> "150", "DoubleKey" -> "1.5", "StringKey" -> "gtStringVal")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedGt2 = defaultGenotypeFields.copy([0m
[0m[[0m[0mdebug[0m] [0m[0m      sampleId = Some("sample2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      phased = Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      calls = Some(Seq(2, -1))[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val convertedVc = VCFRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      contigName = "contigName",[0m
[0m[[0m[0mdebug[0m] [0m[0m      start = 100,[0m
[0m[[0m[0mdebug[0m] [0m[0m      end = 101,[0m
[0m[[0m[0mdebug[0m] [0m[0m      names = Seq("id1", "id2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceAllele = "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      alternateAlleles = Seq("T", "C"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      qual = Some(10.0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      filters = Seq("filter1", "filter2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      attributes = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "NullKey" -> "",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "MissingKey" -> "",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "BoolKey" -> "",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "IntKey" -> "50",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "DoubleKey" -> "0.5",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "StringKey" -> "stringVal"[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes = Seq(convertedGt1, convertedGt2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcfRow == convertedVc, s"\n$vcfRow\n$convertedVc")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw for missing INFO header line with strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcb = new VariantContextBuilder()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.chr("")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.start(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.stop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refAllele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.alleles(Seq(refAllele).asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.attribute("Key", "Value")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = vcb.make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new VariantContextToVCFRowConverter(defaultHeader, ValidationStringency.STRICT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](converter.convert(vc))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw for missing FORMAT header line with strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcb = new VariantContextBuilder()[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.chr("")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.start(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.stop(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refAllele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.alleles(Seq(refAllele).asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gb = new GenotypeBuilder()[0m
[0m[[0m[0mdebug[0m] [0m[0m    gb.attribute("Key", "Value")[0m
[0m[[0m[0mdebug[0m] [0m[0m    vcb.genotypes(gb.make)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = vcb.make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new VariantContextToVCFRowConverter(defaultHeader, ValidationStringency.STRICT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](converter.convert(vc))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.File[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Files[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.ValidationStringency[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.variantcontext.GenotypeLikelihoods[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.{VCFFileReader, VCFHeader}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.io.FileUtils[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GenotypeFields, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFRowToVariantContextConverterSuite extends GlowBaseTest with VCFConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val NA12878 = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val TGP = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val GVCF = s"$testDataHome/NA12878_21_10002403.g.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val defaultHeader = new VCFHeader(VCFRowHeaderLines.allHeaderLines.toSet.asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val defaultConverter = new VCFRowToVariantContextConverter(defaultHeader)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vcfRow: VCFRow = defaultVcfRow.copy(referenceAllele = "A", end = defaultVcfRow.start + 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def writeVcStr(vcStr: String): String = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = Files.createTempFile("test-vcf", ".vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    FileUtils.writeStringToFile(file.toFile, vcStr)[0m
[0m[[0m[0mdebug[0m] [0m[0m    file.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def compareVcs(vcf: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new VCFRowToVariantContextConverter(header)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sparkVcfRowList = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sparkVcList = sparkVcfRowList.map(converter.convert)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = new File(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val reader = new VCFFileReader(file, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val htsjdkVcList = reader.iterator.toList.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(htsjdkVcList.length == sparkVcList.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    htsjdkVcList.zip(sparkVcList).map {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (htsjdkVc, sparkVc) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val htsjdkVcStr = htsjdkVc.fullyDecode(header, false).toString[0m
[0m[[0m[0mdebug[0m] [0m[0m        val sparkVcStr = sparkVc.fullyDecode(header, false).toString[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(htsjdkVcStr == sparkVcStr, s"\nVC1 $htsjdkVcStr\nVC2 $sparkVcStr")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    (sparkVcList, htsjdkVcList, reader.getFileHeader)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Single sample") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcs(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Multiple samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcs(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("GVCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcs(GVCF)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Default VCF row") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = defaultConverter.convert(vcfRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getContig == defaultVcfRow.contigName)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getStart == defaultVcfRow.start + 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getEnd == vcfRow.end)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.emptyID)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getReference.getDisplayString == vcfRow.referenceAllele)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getAlternateAlleles.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!vc.hasLog10PError)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.isNotFiltered)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getAttributes.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt = vc.getGenotypesOrderedByName.asScala.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.getSampleName == "")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.getAlleles.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.isPhased)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.hasDP)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.isFiltered)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.hasPL)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.hasGQ)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!gt.hasAD)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt.getExtendedAttributes.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Set VCF row") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeFields1: GenotypeFields = GenotypeFields([0m
[0m[[0m[0mdebug[0m] [0m[0m      sampleId = Some("sample1"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      phased = Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      calls = Some(Seq(-1, 0)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      depth = Some(3),[0m
[0m[[0m[0mdebug[0m] [0m[0m      filters = Some(Seq("gtFilter1", "gtFilter2")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypeLikelihoods = Some(Seq(0.11, 0.12, 0.13, 0.14, 0.15, 0.16)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      phredLikelihoods = Some(Seq(10, 12, 13, 14, 15, 16)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      posteriorProbabilities = Some(Seq(0.1, 0.2, 0.3, 0.4, 0.5, 0.6)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      conditionalQuality = Some(4),[0m
[0m[[0m[0mdebug[0m] [0m[0m      haplotypeQualities = Some(Seq(20, 21, 22, 23, 24, 25, 26)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedAlleleCounts = Some(Seq(5, 6)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      mappingQuality = Some(7),[0m
[0m[[0m[0mdebug[0m] [0m[0m      alleleDepths = Some(Seq(30, 31, 32)),[0m
[0m[[0m[0mdebug[0m] [0m[0m      otherFields = Map("GT_KEY" -> "val")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeFields2 =[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultGenotypeFields.copy([0m
[0m[[0m[0mdebug[0m] [0m[0m        sampleId = Some("sample2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        phased = Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m        calls = Some(Seq(1, 2))[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val setVcfRow = VCFRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      contigName = "contigName",[0m
[0m[[0m[0mdebug[0m] [0m[0m      start = 100,[0m
[0m[[0m[0mdebug[0m] [0m[0m      end = 200,[0m
[0m[[0m[0mdebug[0m] [0m[0m      names = Seq("name1", "name2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceAllele = "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      alternateAlleles = Seq("T", "C"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      qual = Some(50.0),[0m
[0m[[0m[0mdebug[0m] [0m[0m      filters = Seq("filter1", "filter2"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      attributes = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "END" -> "200",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "FLAG_WO_HEADER_KEY" -> "",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "SINGLE_KEY" -> "single",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "ARRAY_KEY" -> "array1,array2"[0m
[0m[[0m[0mdebug[0m] [0m[0m      ),[0m
[0m[[0m[0mdebug[0m] [0m[0m      genotypes = Seq(genotypeFields1, genotypeFields2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = defaultConverter.convert(setVcfRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getContig == "contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getStart == 101)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getEnd == 200)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getID == "name1;name2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getReference.getDisplayString == "A")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val altAlleles = vc.getAlternateAlleles.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(altAlleles.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(altAlleles.head.getDisplayString == "T")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(altAlleles(1).getDisplayString == "C")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getLog10PError ~== -5 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val filters = vc.getFilters.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(filters.size == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(filters.contains("filter1"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(filters.contains("filter2"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcAttributes = vc.getAttributes.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcAttributes.size == 4)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcAttributes.get("END").contains("200"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcAttributes.get("FLAG_WO_HEADER_KEY").contains("."))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vcAttributes.get("SINGLE_KEY").contains("single"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val arrayVal = vc.getAttributeAsString("ARRAY_KEY", "")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(arrayVal == "array1,array2")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gtSeq = vc.getGenotypesOrderedByName.asScala.toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gtSeq.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt1 = gtSeq.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getSampleName == "sample1")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val alleles1 = gt1.getAlleles.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles1.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles1.head.isNoCall)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles1(1).getDisplayString == "A")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.isPhased)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getDP == 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getFilters == "gtFilter1;gtFilter2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getPL sameElements Array(10, 12, 13, 14, 15, 16))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getGQ == 4)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt1.getAD sameElements Array(30, 31, 32))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val extendedAttributes = gt1.getExtendedAttributes[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.size == 5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.get("GP") == "0.1,0.2,0.3,0.4,0.5,0.6")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.get("HQ") == "20,21,22,23,24,25,26")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.get("EC") == "5,6")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.get("MQ") == "7")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(extendedAttributes.get("GT_KEY") == "val")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gt2 = gtSeq(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gt2.getSampleName == "sample2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val alleles2 = gt2.getAlleles.asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles2.length == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles2.head.getDisplayString == "T")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(alleles2(1).getDisplayString == "C")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Set GL field") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gl = Seq(-1.5, -10.5, -100.5, -1000.5, -10000.5, -100000.5)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeField = defaultGenotypeFields.copy(genotypeLikelihoods = Some(gl))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val setVcfRow = vcfRow.copy(genotypes = Seq(genotypeField))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = defaultConverter.convert(setVcfRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val glAsPl = GenotypeLikelihoods.fromLog10Likelihoods(gl.toArray).getAsPLs[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gtSeq = vc.getGenotypesOrderedByName.asScala.toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gtSeq.head.getPL sameElements glAsPl)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No genotypes") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = defaultConverter.convert(vcfRow.copy(genotypes = Nil))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(vc.getGenotypesOrderedByName.asScala.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No GT field") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = defaultConverter.convert(vcfRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gtSeq = vc.getGenotypesOrderedByName.asScala.toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(gtSeq.head.getAlleles.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throws IllegalArgumentException with no reference allele") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](defaultConverter.convert(defaultVcfRow))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throws ArrayIndexOutOfBoundsException with allele index out of range") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeField = defaultGenotypeFields.copy(calls = Some(Seq(3)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val setVcfRow = vcfRow.copy(genotypes = Seq(genotypeField))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IndexOutOfBoundsException](defaultConverter.convert(setVcfRow))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw for missing INFO header line with strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val setVcfRow = vcfRow.copy(attributes = Map("Key" -> "Value"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val strictConverter =[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFRowToVariantContextConverter(defaultHeader, ValidationStringency.STRICT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](strictConverter.convert(setVcfRow))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throw for missing FORMAT header line with strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeField = defaultGenotypeFields.copy(otherFields = Map("Key" -> "Value"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val setVcfRow = vcfRow.copy(genotypes = Seq(genotypeField))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val strictConverter =[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFRowToVariantContextConverter(defaultHeader, ValidationStringency.STRICT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](strictConverter.convert(setVcfRow))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{ByteArrayOutputStream, StringReader}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.tribble.TribbleException.InvalidHeader[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.variantcontext.{Allele, GenotypeBuilder, VariantContextBuilder}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.{VCFCodec, VCFHeader, VCFHeaderLine}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.io.IOUtils[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFStreamWriterSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  val refA: Allele = Allele.create("A", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  val altT: Allele = Allele.create("T", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  val headerLines: Set[VCFHeaderLine] = VCFRowHeaderLines.allHeaderLines.toSet[0m
[0m[[0m[0mdebug[0m] [0m[0m  val actualSampleIds: Seq[String] = Seq("SampleA", "SampleB")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("VC to infer from has mixed missing and non-missing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new ByteArrayOutputStream()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer =[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFStreamWriter(stream, headerLines, InferSampleIds, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val gts = Seq("", "SampleA", "").map { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      new GenotypeBuilder(s).alleles(Seq(refA, altT).asJava).make[0m
[0m[[0m[0mdebug[0m] [0m[0m    }.asJava[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcBuilder = new VariantContextBuilder().chr("1").alleles("A", "T")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      writer.write(vcBuilder.genotypes(gts).make)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Cannot mix missing and non-missing sample IDs"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkInfer(firstRowSampleIds: Seq[String], secondRowSampleIds: Seq[String]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new ByteArrayOutputStream()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer =[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFStreamWriter(stream, headerLines, InferSampleIds, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val firstGts = firstRowSampleIds.map { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      new GenotypeBuilder(s).alleles(Seq(refA, altT).asJava).make[0m
[0m[[0m[0mdebug[0m] [0m[0m    }.asJava[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcBuilder = new VariantContextBuilder().chr("1").alleles("A", "T")[0m
[0m[[0m[0mdebug[0m] [0m[0m    writer.write(vcBuilder.genotypes(firstGts).make)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val secondGts = secondRowSampleIds.map { s =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      new GenotypeBuilder(s).alleles(Seq(refA, altT).asJava).make[0m
[0m[[0m[0mdebug[0m] [0m[0m    }.asJava[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      writer.write(vcBuilder.genotypes(secondGts).make)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      e.getMessage.contains("Cannot infer sample ids because they are not the same in every row"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Check for new sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkInfer(actualSampleIds, Seq("SampleC"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Saw present sample IDs when inferred missing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkInfer(Seq("", "", ""), actualSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Saw present sample IDs when inferred missing, same number of samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkInfer(Seq("", ""), actualSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Number of missing does not match") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkInfer(Seq("", "", ""), Seq("", ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Unexpected missing sample ID") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkInfer(actualSampleIds, Seq("", ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Don't write header with VC if told not to") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new ByteArrayOutputStream()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer =[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFStreamWriter(stream, headerLines, SampleIds(actualSampleIds), false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vc = new VariantContextBuilder().chr("1").alleles("A").make[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    writer.write(vc)[0m
[0m[[0m[0mdebug[0m] [0m[0m    writer.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stringReader = new StringReader(stream.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val lineIterator = new LineIteratorImpl(IOUtils.lineIterator(stringReader).asScala)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val codec = new VCFCodec()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[InvalidHeader](codec.readActualHeader(lineIterator).asInstanceOf[VCFHeader])[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Don't write header for empty stream if told not to") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new ByteArrayOutputStream()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer = new VCFStreamWriter(stream, headerLines, SampleIds(actualSampleIds), false)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    writer.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(stream.size == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Empty partition") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new ByteArrayOutputStream()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer = new VCFStreamWriter(stream, headerLines, InferSampleIds, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalStateException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      writer.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Cannot infer header for empty partition"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.FileNotFoundException[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.tribble.TribbleException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.VCFHeader[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.StructType[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFHeaderUtilsSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  val vcf = s"$testDataHome/NA12878_21_10002403.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val schema: StructType = spark.read.format("vcf").load(vcf).schema[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vcfMetadataLines: String =[0m
[0m[[0m[0mdebug[0m] [0m[0m    """[0m
[0m[[0m[0mdebug[0m] [0m[0m      |##fileformat=VCFv4.2[0m
[0m[[0m[0mdebug[0m] [0m[0m      |##FORMAT=<ID=PL,Number=G,Type=Integer,Description="">[0m
[0m[[0m[0mdebug[0m] [0m[0m      |##contig=<ID=monkey,length=42>[0m
[0m[[0m[0mdebug[0m] [0m[0m      |##source=DatabricksIsCool[0m
[0m[[0m[0mdebug[0m] [0m[0m      |""".stripMargin.trim // write CHROM line by itself to preserve tabs[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def getHeaderNoSamples([0m
[0m[[0m[0mdebug[0m] [0m[0m      options: Map[String, String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultOpt: Option[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      schema: StructType) = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (headerLines, _) = VCFHeaderUtils.parseHeaderLinesAndSamples([0m
[0m[[0m[0mdebug[0m] [0m[0m      options,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultOpt,[0m
[0m[[0m[0mdebug[0m] [0m[0m      schema,[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    new VCFHeader(headerLines.asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def getSchemaLines(header: VCFHeader) =[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getInfoHeaderLines.asScala.toSet ++ header.getFormatHeaderLines.asScala.toSet[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def getAllLines(header: VCFHeader) =[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getContigLines.asScala.toSet ++[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getFilterLines.asScala.toSet ++[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getFormatHeaderLines.asScala.toSet ++[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getInfoHeaderLines.asScala.toSet ++[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getOtherHeaderLines.asScala.toSet[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("fall back") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val oldHeader = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = getHeaderNoSamples(Map.empty, Some("infer"), schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getSchemaLines(header) == VCFSchemaInferrer.headerLinesFromSchema(schema).toSet)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getSchemaLines(header) == getSchemaLines(oldHeader))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("infer header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val oldHeader = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = getHeaderNoSamples(Map("vcfHeader" -> "infer"), None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getSchemaLines(header) == VCFSchemaInferrer.headerLinesFromSchema(schema).toSet)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getSchemaLines(header) == getSchemaLines(oldHeader))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("use header path") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = getHeaderNoSamples(Map("vcfHeader" -> vcf), None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      getAllLines(header) ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      getAllLines(VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, vcf)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("use literal header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val contents = vcfMetadataLines + "\n#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA12878\n"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = getHeaderNoSamples(Map("vcfHeader" -> contents), None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val parsed = VCFHeaderUtils.parseHeaderFromString(contents)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getAllLines(header).nonEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getAllLines(header) == getAllLines(parsed))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("throw on bad literal header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // no #CHROM line[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e =[0m
[0m[[0m[0mdebug[0m] [0m[0m      intercept[IllegalArgumentException](VCFHeaderUtils.parseHeaderFromString(vcfMetadataLines))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Unable to parse VCF header"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("no vcf header arg and no default header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      getHeaderNoSamples(Map.empty, None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Must specify a method to determine VCF header"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("invalid path") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[FileNotFoundException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      getHeaderNoSamples(Map("vcfHeader" -> "fake.vcf"), None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("invalid VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[TribbleException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      getHeaderNoSamples(Map("vcfHeader" -> s"$testDataHome/no_header.csv"), None, schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Files[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.commons.io.FileUtils[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.lit[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.{SparkException, TaskContext}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.VCFRow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.transformers.pipe.ProcessHelper[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFPiperSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val na12878 = s"$testDataHome/NA12878_21_10002403.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val TGP = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def afterEach(): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform("pipe_cleanup", spark.emptyDataFrame)[0m
[0m[[0m[0mdebug[0m] [0m[0m    super.afterEach()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def readVcf(vcf: String): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def pipeScript(vcf: String, script: String): (DataFrame, DataFrame) = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> "infer",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["$script"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform("pipe", inputDf, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    (inputDf, outputDf)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Prepend chr") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (_, df) = pipeScript([0m
[0m[[0m[0mdebug[0m] [0m[0m      na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/vcf/scripts/prepend-chr.sh"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.cache()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Prepends chr[0m
[0m[[0m[0mdebug[0m] [0m[0m    val distinctContigNames = df.select("contigName").as[String].distinct.collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(distinctContigNames.length == 1 && distinctContigNames.head == "chr21")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Include sample names[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleSeq = df.select("genotypes.sampleId").as[Seq[String]].head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleSeq == Seq("NA12878"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Flattens INFO fields[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sorSeq = df.select("INFO_SOR").as[Double].collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sorSeq.min ~== 0.551 relTol 0.2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.unpersist()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Remove INFO") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (_, df) = pipeScript([0m
[0m[[0m[0mdebug[0m] [0m[0m      na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/vcf/scripts/remove-info.sh"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!df.schema.fieldNames.exists(_.startsWith("INFO_")))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Remove non-header rows") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (inputDf, outputDf) = pipeScript([0m
[0m[[0m[0mdebug[0m] [0m[0m      na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/vcf/scripts/remove-rows.sh"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(inputDf.schema == outputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val baseTextOptions = Map("inputFormatter" -> "vcf", "outputFormatter" -> "text")[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("environment variables") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = baseTextOptions ++ Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "in_vcfHeader" -> "infer",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "cmd" -> """["printenv"]""",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "env_animal" -> "monkey",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "env_a" -> "b",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "env_c" -> "D",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "envE" -> "F")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = readVcf(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = Glow[0m
[0m[[0m[0mdebug[0m] [0m[0m      .transform("pipe", df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[String][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.contains("animal=monkey"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.contains("a=b"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.contains("c=D"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.contains("e=F"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("empty partition") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = readVcf(na12878).repartition(8)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.count == 4)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = baseTextOptions ++ Map("cmd" -> """["wc", "-l"]""", "in_vcfHeader" -> na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = Glow.transform("pipe", df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count() == 8)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("empty partition and missing samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = readVcf(na12878).repartition(8)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.count == 4)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = baseTextOptions ++ Map("cmd" -> """["wc", "-l"]""", "in_vcfHeader" -> "infer")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException](Glow.transform("pipe", df, options))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("stdin and stderr threads are cleaned up for successful commands") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    pipeScript(na12878, "cat")[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDIN_WRITER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDERR_READER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("command doesn't exist") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeScript(na12878, "totallyfakecommandthatdoesntexist")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.contains("No such file or directory"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // threads should still be cleaned up[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDIN_WRITER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDERR_READER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("header only") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = readVcf(na12878).limit(0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "text",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["cat", "-"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = Glow.transform("pipe", df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count == 28)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("task context is defined in each thread") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = input.map { el =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      require(TaskContext.get != null)[0m
[0m[[0m[0mdebug[0m] [0m[0m      el[0m
[0m[[0m[0mdebug[0m] [0m[0m    }.toDF[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["cat", "-"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = Glow.transform("pipe", df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count() == 4)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing sample names") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> "infer",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["cat", "-"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform("pipe", inputDf.toDF, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    inputDf.as[SimpleVcfRow].collect.zip(outputDf.as[SimpleVcfRow].collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        var missingSampleIdx = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m        val gtsWithSampleIds = vc1.genotypes.map { gt =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          missingSampleIdx += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m          gt.copy(sampleId = Some(s"sample_$missingSampleIdx"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        val vc1WithSampleIds = vc1.copy(genotypes = gtsWithSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1WithSampleIds.equals(vc2), s"VC1 $vc1WithSampleIds VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("input validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("INFO_fake", lit("foobar"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "inVcfHeader" -> TGP,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "inValidationStringency" -> "STRICT",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["cat", "-"]"""[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](Glow.transform("pipe", inputDf, options))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("output validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = Seq("1", "1", "id", "C", "T,GT", "1", ".", "AC=monkey").mkString("\t")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val file = Files.createTempFile("test-vcf", ".vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header =[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"""##fileformat=VCFv4.2[0m
[0m[[0m[0mdebug[0m] [0m[0m         |##INFO=<ID=AC,Number=1,Type=Integer,Description="">[0m
[0m[[0m[0mdebug[0m] [0m[0m         |#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO[0m
[0m[[0m[0mdebug[0m] [0m[0m        """.stripMargin[0m
[0m[[0m[0mdebug[0m] [0m[0m    FileUtils.writeStringToFile(file.toFile, header + row)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "inVcfHeader" -> "infer",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outValidationStringency" -> "STRICT",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["cat", "$file"]"""[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException](Glow.transform("pipe", inputDf, options))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class SimpleVcfRow(contigName: String, start: Long, genotypes: Seq[SimpleGenotype])[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class SimpleGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m    sampleId: Option[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m    phased: Option[Boolean],[0m
[0m[[0m[0mdebug[0m] [0m[0m    calls: Option[Seq[Int]])[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{BufferedInputStream, File}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.{Files, Path, Paths}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.util.stream.Collectors[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport com.google.common.io.ByteStreams[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.ValidationStringency[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.util.{BlockCompressedInputStream, BlockCompressedStreamConstants}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.variantcontext.writer.VCFHeaderWriter[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf.{VCFCompoundHeaderLine, VCFHeader, VCFHeaderLine}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.expr[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.{SparkConf, SparkException}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{VCFRow, VariantSchemas, WithUtils}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mabstract class VCFFileWriterSuite(val sourceName: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m    extends GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m    with VCFConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val NA12878 = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val TGP = s"$testDataHome/1000genomes-phase3-1row.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  val readSourceName = "vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def sparkConf: SparkConf = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Verify that tests correctly set BGZF codecs[0m
[0m[[0m[0mdebug[0m] [0m[0m    super.sparkConf.set("spark.hadoop.io.compression.codecs", "")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def createTempVcf: Path = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempDir = Files.createTempDirectory("test-vcf-dir")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = tempDir.resolve("test.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    logger.info(s"Writing VCF to path ${path.toAbsolutePath.toString}")[0m
[0m[[0m[0mdebug[0m] [0m[0m    path[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def writeAndRereadWithDBParser([0m
[0m[[0m[0mdebug[0m] [0m[0m      vcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      readSampleIds: Boolean = true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      rereadSampleIds: Boolean = true,[0m
[0m[[0m[0mdebug[0m] [0m[0m      schemaOption: (String, String),[0m
[0m[[0m[0mdebug[0m] [0m[0m      partitions: Option[Int] = None): (DataFrame, DataFrame) = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", readSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option(schemaOption._1, schemaOption._2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val repartitioned = partitions.map(p => ds.repartition(p)).getOrElse(ds)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (readSampleIds) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val originalHeader = scala[0m
[0m[[0m[0mdebug[0m] [0m[0m        .io[0m
[0m[[0m[0mdebug[0m] [0m[0m        .Source[0m
[0m[[0m[0mdebug[0m] [0m[0m        .fromFile(vcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .getLines()[0m
[0m[[0m[0mdebug[0m] [0m[0m        .takeWhile(_.startsWith("#"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .mkString("\n")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      repartitioned[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", originalHeader)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      repartitioned[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", rereadSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option(schemaOption._1, schemaOption._2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    (ds, rewrittenDs)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val schemaOptions = Seq(("vcfRowSchema", "true"), ("flattenInfoFields", "true"), ("", ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("Read single sample VCF with VCF parser")(schemaOptions) { schema =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (ds, rewrittenDs) = writeAndRereadWithDBParser(NA12878, schemaOption = schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    rewrittenDs.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.collect.zip(rewrittenDs.collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1.equals(vc2), s"VC1\n$vc1\nVC2\n$vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("Read multi-sample VCF with VCF parser")(schemaOptions) { schema =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (ds, rewrittenDs) = writeAndRereadWithDBParser(TGP, schemaOption = schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.collect.zip(rewrittenDs.collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1.equals(vc2), s"VC1 $vc1 VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Use VCF parser without sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (ds, rewrittenDs) = writeAndRereadWithDBParser([0m
[0m[[0m[0mdebug[0m] [0m[0m      TGP,[0m
[0m[[0m[0mdebug[0m] [0m[0m      readSampleIds = false,[0m
[0m[[0m[0mdebug[0m] [0m[0m      schemaOption = ("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.as[VCFRow].collect.zip(rewrittenDs.as[VCFRow].collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        var missingSampleIdx = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m        val gtsWithSampleIds = vc1.genotypes.map { gt =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          missingSampleIdx += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m          gt.copy(sampleId = Some(s"sample_$missingSampleIdx"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        val vc1WithSampleIds = vc1.copy(genotypes = gtsWithSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1WithSampleIds.equals(vc2), s"VC1 $vc1WithSampleIds VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Use VCF parser without sample IDs (many partitions)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (ds, rewrittenDs) = writeAndRereadWithDBParser([0m
[0m[[0m[0mdebug[0m] [0m[0m      NA12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      readSampleIds = false,[0m
[0m[[0m[0mdebug[0m] [0m[0m      schemaOption = ("vcfRowSchema", "true"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      partitions = Some(100)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val orderedDs1 = ds.orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val orderedDs2 = rewrittenDs.orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    orderedDs1.as[VCFRow].collect.zip(orderedDs2.as[VCFRow].collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        var missingSampleIdx = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m        val gtsWithSampleIds = vc1.genotypes.map { gt =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          missingSampleIdx += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m          gt.copy(sampleId = Some(s"sample_$missingSampleIdx"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        val vc1WithSampleIds = vc1.copy(genotypes = gtsWithSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1WithSampleIds.equals(vc2), s"VC1 $vc1WithSampleIds VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Use VCF parser with sample IDs (many partitions)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val (ds, rewrittenDs) = writeAndRereadWithDBParser([0m
[0m[[0m[0mdebug[0m] [0m[0m      NA12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      schemaOption = ("vcfRowSchema", "true"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      partitions = Some(100)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val orderedDs1 = ds.orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val orderedDs2 = rewrittenDs.orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    orderedDs1.as[VCFRow].collect.zip(orderedDs2.as[VCFRow].collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) => assert(vc1.equals(vc2), s"VC1 $vc1 VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Strict validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("flattenInfoFields", false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Contains INFO and FORMAT keys (eg. INFO AC) that can't be inferred from attributes map[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write.format(sourceName).option("validationStringency", "strict").save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  Seq(("bgzf", ".bgz"), ("gzip", ".gz")).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case (codecName, extension) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      test(s"Output $codecName compressed file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m        val tempFilePath = createTempVcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m        val ds = spark.read.format(readSourceName).option("vcfRowSchema", true).load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m        val outpath = tempFilePath.toString + extension[0m
[0m[[0m[0mdebug[0m] [0m[0m        ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m          .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m          .option("compression", codecName)[0m
[0m[[0m[0mdebug[0m] [0m[0m          .save(outpath)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m        val outFile = Paths.get(outpath)[0m
[0m[[0m[0mdebug[0m] [0m[0m        val filesWritten = if (outFile.toFile.isDirectory) {[0m
[0m[[0m[0mdebug[0m] [0m[0m          Files.list(Paths.get(outpath)).collect(Collectors.toList[Path]).asScala.map(_.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m        } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m          Seq(outpath)[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(filesWritten.exists(s => s.endsWith(extension)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("variant context validation settings obey stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    def parseRow(stringency: ValidationStringency): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val data =[0m
[0m[[0m[0mdebug[0m] [0m[0m        VCFRow(null, 0, 1, Seq.empty, null, Seq.empty, None, Seq.empty, Map.empty, Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .createDataFrame(Seq(data))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .drop("contigName")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .mode("overwrite")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("validationStringency", stringency.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(Files.createTempDirectory("vcf").resolve("vcf").toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    parseRow(ValidationStringency.SILENT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    parseRow(ValidationStringency.LENIENT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      parseRow(ValidationStringency.STRICT)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def writeVcfHeader(df: DataFrame, vcfHeaderOpt: Option[String]): VCFHeader = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFileStr = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (vcfHeaderOpt.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", vcfHeaderOpt.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFileStr)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      df.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFileStr)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = new File(tempFileStr)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fileToRead = if (tempFile.isDirectory) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      tempFile.listFiles().filter(_.getName.endsWith(".vcf")).head.getAbsolutePath[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      tempFileStr[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, fileToRead)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def getSchemaLines(header: VCFHeader): Set[VCFCompoundHeaderLine] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    header.getInfoHeaderLines.asScala.toSet ++ header.getFormatHeaderLines.asScala.toSet[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Provided header is sorted") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headerLine1 = new VCFHeaderLine("fakeHeaderKey", "fakeHeaderValue")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headerLine2 = new VCFHeaderLine("secondFakeHeaderKey", "secondFakeHeaderValue")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headerLines = Set(headerLine1, headerLine2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val extraHeader = new VCFHeader(headerLines.asJava, Seq("sample1", "NA12878").asJava)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfHeader = VCFHeaderWriter.writeHeaderAsString(extraHeader)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writtenHeader =[0m
[0m[[0m[0mdebug[0m] [0m[0m      writeVcfHeader(spark.read.format(readSourceName).load(NA12878), Some(vcfHeader))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(headerLines.subsetOf(writtenHeader.getMetaDataInInputOrder.asScala))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(writtenHeader.getGenotypeSamples.asScala == Seq("NA12878", "sample1"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Path header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writtenHeader = writeVcfHeader(spark.read.format(readSourceName).load(NA12878), Some(TGP))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tgpHeader = VCFMetadataLoader.readVcfHeader(spark.sparkContext.hadoopConfiguration, TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(tgpHeader.getMetaDataInInputOrder == writtenHeader.getMetaDataInInputOrder)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(tgpHeader.getGenotypeSamples == writtenHeader.getGenotypeSamples)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Infer header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.read.format(readSourceName).load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writtenHeader = writeVcfHeader(df, Some("infer"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val oldHeader = VCFMetadataLoader.readVcfHeader(spark.sparkContext.hadoopConfiguration, NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      getSchemaLines(writtenHeader) == VCFSchemaInferrer.headerLinesFromSchema(df.schema).toSet)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(getSchemaLines(writtenHeader) == getSchemaLines(oldHeader)) // Includes descriptions[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Empty file with inferred header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .limit(0)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Cannot infer sample IDs without rows[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Empty file with determined header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkContext[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyRDD[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .toDS[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfHeader", NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rewrittenDs.collect.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No genotypes column") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.write.format(sourceName).save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rereadDf = spark.read.format(readSourceName).load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!rereadDf.schema.contains("genotypes"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No sample IDs column") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("subsetGenotypes", expr("slice(genotypes, 1, 3)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumnRenamed("subsetGenotypes", "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.write.format(sourceName).save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rereadDf = spark.read.format(readSourceName).load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = rereadDf.select("genotypes.sampleId").distinct().as[Seq[String]].collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleIds.length == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleIds.head == Seq("sample_1", "sample_2", "sample_3"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass MultiFileVCFWriterSuite extends VCFFileWriterSuite("vcf") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Corrupted header lines are not written") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val extraHeaderStr = VCFHeaderWriter.writeHeaderAsString(new VCFHeader())[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", extraHeaderStr.substring(0, extraHeaderStr.length - 10))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.getMessage.contains("Unable to parse VCF header"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Invalid validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("validationStringency", "fakeStringency")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some empty partitions and infer sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .limit(2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(5)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testInferredSampleIds(row1HasSamples: Boolean, row2HasSamples: Boolean): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Samples: HG00096 HG00097	HG00099[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds1 = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", row1HasSamples)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("subsetGenotypes", expr("slice(genotypes, 1, 3)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumnRenamed("subsetGenotypes", "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Samples: HG00099 HG00100	HG00101	HG00102[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds2 = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", row2HasSamples)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("subsetGenotypes", expr("slice(genotypes, 3, 4)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumnRenamed("subsetGenotypes", "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = ds1.union(ds2).repartition(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", "infer")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getCause.getCause.getCause.isInstanceOf[IllegalArgumentException])[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      e.getCause[0m
[0m[[0m[0mdebug[0m] [0m[0m        .getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains("Cannot infer sample ids because they are not the same in every row"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Fails if inferred present sample IDs but row missing sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testInferredSampleIds(true, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Fails if inferred present sample IDs but row has different sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testInferredSampleIds(true, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Fails if injected missing sample IDs don't match number of samples") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testInferredSampleIds(false, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Fails if injected missing sample IDs but has sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testInferredSampleIds(false, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass SingleFileVCFWriterSuite extends VCFFileWriterSuite("bigvcf") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Corrupted header lines are not written") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val extraHeaderStr = VCFHeaderWriter.writeHeaderAsString(new VCFHeader())[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", extraHeaderStr.substring(0, extraHeaderStr.length - 10))[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Unable to parse VCF header"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Invalid validation stringency") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("validationStringency", "fakeStringency")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Check BGZF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(100) // Force multiple partitions[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outPath = createTempVcf.toString + ".bgz"[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.write.format(sourceName).save(outPath)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = new org.apache.hadoop.fs.Path(outPath)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fs = path.getFileSystem(spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    WithUtils.withCloseable(new BufferedInputStream(fs.open(path))) { is =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      // Contains block gzip header[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(BlockCompressedInputStream.isValidFile(is))[0m
[0m[[0m[0mdebug[0m] [0m[0m      val bytes = ByteStreams.toByteArray(is)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      // Empty gzip block only occurs once[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        bytes.indexOfSlice(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK) ==[0m
[0m[[0m[0mdebug[0m] [0m[0m        bytes.lastIndexOfSlice(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK)[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m      // Empty gzip block is at end of file[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(bytes.endsWith(BlockCompressedStreamConstants.EMPTY_GZIP_BLOCK))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some empty partitions and by default infer sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .limit(2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(5)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rereadDs = spark.read.format("vcf").load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rereadDs.sort("start").collect sameElements ds.sort("start").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some empty partitions and explicitly infer sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .limit(2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(5)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    ds.write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfHeader", "infer")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rereadDs = spark.read.format("vcf").load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rereadDs.sort("start").collect sameElements ds.sort("start").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Bigvcf header check for empty file with determined header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkContext[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyRDD[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .toDS[0m
[0m[[0m[0mdebug[0m] [0m[0m      .repartition(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfHeader", NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val truthHeader = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writtenHeader = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(truthHeader.getMetaDataInInputOrder.equals(writtenHeader.getMetaDataInInputOrder))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(truthHeader.getGenotypeSamples == writtenHeader.getGenotypeSamples)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Bigvcf 0 partitions exception check") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .sparkContext[0m
[0m[[0m[0mdebug[0m] [0m[0m        .emptyRDD[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m        .toDS[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfHeader", NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def sliceInferredSampleIds([0m
[0m[[0m[0mdebug[0m] [0m[0m      start: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      numPresentSampleIds: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      numMissingSampleIds: Int): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val presentSampleIds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("genotypesWithSampleIds", expr(s"slice(genotypes, $start, $numPresentSampleIds)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val missingSampleIds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(TGP)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn([0m
[0m[[0m[0mdebug[0m] [0m[0m        "genotypesWithoutSampleIds",[0m
[0m[[0m[0mdebug[0m] [0m[0m        expr(s"slice(genotypes, $start, $numMissingSampleIds)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes", "attributes")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    presentSampleIds[0m
[0m[[0m[0mdebug[0m] [0m[0m      .join(missingSampleIds, VariantSchemas.vcfBaseSchema.map(_.name))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("genotypes", expr("concat(genotypesWithSampleIds, genotypesWithoutSampleIds)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypesWithSampleIds", "genotypesWithoutSampleIds")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkWithInferredSampleIds(df: DataFrame, expectedSampleIds: Seq[String]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Should be written with all samples with no-calls if sample is missing[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfHeader", "infer")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rereadDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(readSourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("vcfRowSchema", "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Make sure there is only one set of sample IDs and they match the expected ones[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIdRows = rereadDf.select("genotypes.sampleId").distinct().as[Seq[String]].collect[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleIdRows.length == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sampleIdRows.head == expectedSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Compare the called genotypes[0m
[0m[[0m[0mdebug[0m] [0m[0m    val calledRereadDf = rereadDf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("calledGenotypes", expr("filter(genotypes, gt -> gt.calls[0] != -1)"))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumnRenamed("calledGenotypes", "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.as[VCFRow].collect.zip(calledRereadDf.as[VCFRow].collect).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (vc1, vc2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        var missingSampleIdx = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m        val gtsWithSampleIds = vc1.genotypes.map { gt =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          if (gt.sampleId.isEmpty) {[0m
[0m[[0m[0mdebug[0m] [0m[0m            missingSampleIdx += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m            gt.copy(sampleId = Some(s"sample_$missingSampleIdx"))[0m
[0m[[0m[0mdebug[0m] [0m[0m          } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m            gt[0m
[0m[[0m[0mdebug[0m] [0m[0m          }[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        val vc1WithSampleIds = vc1.copy(genotypes = gtsWithSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(vc1WithSampleIds.equals(vc2), s"VC1 $vc1WithSampleIds VC2 $vc2")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Unions inferred sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Samples: HG00096	HG00097	HG00099	HG00100	HG00101[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds1 = sliceInferredSampleIds(1, 5, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Samples: HG00099	HG00100	HG00101	HG00102[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds2 = sliceInferredSampleIds(3, 4, 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkWithInferredSampleIds([0m
[0m[[0m[0mdebug[0m] [0m[0m      ds1.union(ds2),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("HG00096", "HG00097", "HG00099", "HG00100", "HG00101", "HG00102"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Matching number of missing sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // 3 missing samples[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds1 = sliceInferredSampleIds(1, 0, 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds2 = sliceInferredSampleIds(2, 0, 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkWithInferredSampleIds(ds1.union(ds2), Seq("sample_1", "sample_2", "sample_3"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Mixed inferred and missing sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // 3 missing samples[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds1 = sliceInferredSampleIds(1, 3, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds2 = sliceInferredSampleIds(2, 3, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempFile = createTempVcf.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      ds1.union(ds2).write.option("vcfHeader", "infer").format(sourceName).save(tempFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Cannot mix missing and non-missing sample IDs"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Non-matching number of missing sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // 2 missing samples[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds1 = sliceInferredSampleIds(1, 0, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // 4 missing samples[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds2 = sliceInferredSampleIds(2, 0, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      ds1.union(ds2).write.format(sourceName).save(createTempVcf.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Rows contain varying number of missing samples"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.reflect.runtime.universe._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.bdgenomics.adam.util.PhredUtils[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GenotypeFields, TestUtils, VCFRow}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mtrait VCFConverterBaseTest extends TestUtils {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultContigName = ""[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultStart = 0L[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultEnd = 0L[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultNames = Seq.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultReferenceAllele = ""[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultAlternateAlleles = Seq.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultQual = None[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  final val defaultVcfRow = VCFRow([0m
[0m[[0m[0mdebug[0m] [0m[0m    contigName = defaultContigName,[0m
[0m[[0m[0mdebug[0m] [0m[0m    start = defaultStart,[0m
[0m[[0m[0mdebug[0m] [0m[0m    end = defaultEnd,[0m
[0m[[0m[0mdebug[0m] [0m[0m    names = defaultNames,[0m
[0m[[0m[0mdebug[0m] [0m[0m    referenceAllele = defaultReferenceAllele,[0m
[0m[[0m[0mdebug[0m] [0m[0m    alternateAlleles = defaultAlternateAlleles,[0m
[0m[[0m[0mdebug[0m] [0m[0m    qual = defaultQual,[0m
[0m[[0m[0mdebug[0m] [0m[0m    filters = Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m    attributes = Map.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m    genotypes = Seq(defaultGenotypeFields)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultGenotypeFields = GenotypeFields([0m
[0m[[0m[0mdebug[0m] [0m[0m    sampleId = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    phased = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    calls = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    depth = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    filters = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    genotypeLikelihoods = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    phredLikelihoods = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    posteriorProbabilities = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    conditionalQuality = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    haplotypeQualities = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    expectedAlleleCounts = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    mappingQuality = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    alleleDepths = None,[0m
[0m[[0m[0mdebug[0m] [0m[0m    otherFields = Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultAlleles = Seq(defaultReferenceAllele) ++ defaultAlternateAlleles[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultAlternateAllele = None[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultAlleleIdx = None[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultNonRefAlleleIdx = None[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val defaultSplitFromMultiAllelic = false[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def phredToLog(p: Int): Double = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    PhredUtils.phredToLogProbability(p)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def phredToLogFloat(p: Int): Float = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    PhredUtils.phredToLogProbability(p).toFloat[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // It is ok for f1 to be defined but f2 to be empty.[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareOptionalFloats(f1: Option[Float], f2: Option[Float], field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (!(f1.isDefined && f2.isEmpty)) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      try {[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          f1.isDefined.equals(f2.isDefined),[0m
[0m[[0m[0mdebug[0m] [0m[0m          "defined %s %s".format(f1.isDefined, f2.isDefined)[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          f1.getOrElse(0f) ~== f2.getOrElse(0f) relTol 0.2,[0m
[0m[[0m[0mdebug[0m] [0m[0m          "values %f %f".format(f1.getOrElse(0f), f2.getOrElse(0f))[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case t: Throwable =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          throw t[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // It is ok for d1 to be defined but d2 to be empty.[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareOptionalDoubles([0m
[0m[[0m[0mdebug[0m] [0m[0m      d1: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      d2: Option[Double],[0m
[0m[[0m[0mdebug[0m] [0m[0m      field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (!(d1.isDefined && d2.isEmpty)) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      try {[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          d1.isDefined.equals(d2.isDefined),[0m
[0m[[0m[0mdebug[0m] [0m[0m          "defined %s %s".format(d1.isDefined, d2.isDefined)[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          d1.getOrElse(0d) ~== d2.getOrElse(0d) relTol 0.2,[0m
[0m[[0m[0mdebug[0m] [0m[0m          "values %f %f".format(d1.getOrElse(0d), d2.getOrElse(0d))[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case t: Throwable =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          throw t[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareSeqFloats(sf1: Seq[Float], sf2: Seq[Float], field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    try {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(sf1.length.equals(sf2.length), "length %d %d".format(sf1.length, sf2.length))[0m
[0m[[0m[0mdebug[0m] [0m[0m      sf1.zip(sf2).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (f1: Float, f2: Float) => assert(f1 ~== f2 relTol 0.2, "values %f %f".format(f1, f2))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case t: Throwable =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw t[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareSeqDoubles(sd1: Seq[Double], sd2: Seq[Double], field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    try {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(sd1.length.equals(sd2.length), "length %d %d".format(sd1.length, sd2.length))[0m
[0m[[0m[0mdebug[0m] [0m[0m      sd1.zip(sd2).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (d1: Double, d2: Double) => assert(d1 ~== d2 relTol 0.2, "values %f %f".format(d1, d2))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case t: Throwable =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw t[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareStringDoubles(s1: String, s2: String, field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(s1.equals(s2) || (s1.toDouble ~== s2.toDouble relTol 0.2), s"$field $s1 $s2")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // It is ok for keys to be defined in m1 but not in m2.[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareMapStrings([0m
[0m[[0m[0mdebug[0m] [0m[0m      m1: scala.collection.Map[String, String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      m2: scala.collection.Map[String, String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      field: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    try {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(m2.keySet.subsetOf(m1.keySet))[0m
[0m[[0m[0mdebug[0m] [0m[0m      val keySet = m2.keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m      keySet.foreach { k =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        compareStringDoubles(m1(k), m2(k), k)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case t: Throwable =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        throw t[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def getClassFields[T: TypeTag]: Seq[String] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    typeOf[T].members.sorted.collect {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case m: MethodSymbol if m.isParamAccessor => m.name.toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.ValidationStringency[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{ArrayType, DataType, MapType, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass InternalRowToVariantContextConverterSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val NA12878 = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val header = VCFMetadataLoader.readVcfHeader(sparkContext.hadoopConfiguration, NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val headerLines = header.getMetaDataInInputOrder.asScala.toSet[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val optionsSeq = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    Map("flattenInfoFields" -> "true", "includeSampleIds" -> "true"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Map("flattenInfoFields" -> "true", "includeSampleIds" -> "false"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Map("flattenInfoFields" -> "false", "includeSampleIds" -> "false"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    Map("splitToBiallelic" -> "true", "includeSampleIds" -> "true")[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("common schema options pass strict validation")(optionsSeq) { options =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.read.format("vcf").options(options).load(NA12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    new InternalRowToVariantContextConverter([0m
[0m[[0m[0mdebug[0m] [0m[0m      toggleNullability(df.schema, true),[0m
[0m[[0m[0mdebug[0m] [0m[0m      headerLines,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ValidationStringency.STRICT).validate()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    new InternalRowToVariantContextConverter([0m
[0m[[0m[0mdebug[0m] [0m[0m      toggleNullability(df.schema, false),[0m
[0m[[0m[0mdebug[0m] [0m[0m      headerLines,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ValidationStringency.STRICT).validate()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def toggleNullability[T <: DataType](dt: T, nullable: Boolean): T = dt match {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case at: ArrayType => at.copy(containsNull = nullable).asInstanceOf[T][0m
[0m[[0m[0mdebug[0m] [0m[0m    case st: StructType =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      val fields = st.map { f =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        f.copy(dataType = toggleNullability(f.dataType, nullable), nullable = nullable)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m      StructType(fields).asInstanceOf[T][0m
[0m[[0m[0mdebug[0m] [0m[0m    case mt: MapType => mt.copy(valueContainsNull = nullable).asInstanceOf[T][0m
[0m[[0m[0mdebug[0m] [0m[0m    case other => other[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.vcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.vcf._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.VariantSchemas[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VCFSchemaInferrerSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("includes base fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(false, false, Seq.empty, Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    VariantSchemas.vcfBaseSchema.foreach { field =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(schema.contains(field))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("includes attributes field if not flattening info fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(false, false, Seq.empty, Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.exists(_.name == "attributes"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("sampleId field")(Seq(true, false)) { includeSampleIds =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(includeSampleIds, false, Seq.empty, Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypesField = schema[0m
[0m[[0m[0mdebug[0m] [0m[0m      .find(_.name == "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .get[0m
[0m[[0m[0mdebug[0m] [0m[0m      .dataType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[ArrayType][0m
[0m[[0m[0mdebug[0m] [0m[0m      .elementType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[StructType][0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(genotypesField.exists(_.name == "sampleId") == includeSampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class VCFField([0m
[0m[[0m[0mdebug[0m] [0m[0m      vcfType: VCFHeaderLineType,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vcfCount: Option[VCFHeaderLineCount], // None implies count=1[0m
[0m[[0m[0mdebug[0m] [0m[0m      sqlType: DataType,[0m
[0m[[0m[0mdebug[0m] [0m[0m      description: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val infoFields = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField(VCFHeaderLineType.Character, None, StringType, "descriptionOne"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField(VCFHeaderLineType.String, None, StringType, "descriptionTwo"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField(VCFHeaderLineType.Integer, None, IntegerType, "descriptionThree"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField(VCFHeaderLineType.Float, None, DoubleType, "descriptionFour"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Note: FLAG fields usually have count 0, but for this test it doesn't matter[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField(VCFHeaderLineType.Flag, None, BooleanType, "descriptionFive"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField([0m
[0m[[0m[0mdebug[0m] [0m[0m      VCFHeaderLineType.String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(VCFHeaderLineCount.G),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ArrayType(StringType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "descriptionSix"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField([0m
[0m[[0m[0mdebug[0m] [0m[0m      VCFHeaderLineType.Integer,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(VCFHeaderLineCount.G),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ArrayType(IntegerType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "descriptionSeven"),[0m
[0m[[0m[0mdebug[0m] [0m[0m    VCFField([0m
[0m[[0m[0mdebug[0m] [0m[0m      VCFHeaderLineType.Float,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(VCFHeaderLineCount.G),[0m
[0m[[0m[0mdebug[0m] [0m[0m      ArrayType(DoubleType),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "descriptionEight")[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val formatFields = infoFields.filter(_.vcfType != VCFHeaderLineType.Flag)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("flatten info field")(infoFields) { field =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val infoHeader = field.vcfCount match {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case Some(t) => new VCFInfoHeaderLine("field", t, field.vcfType, field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m      case None => new VCFInfoHeaderLine("field", 1, field.vcfType, field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(false, true, Seq(infoHeader), Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sqlField = schema.find(_.name == "INFO_field").get[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.dataType == field.sqlType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.metadata.contains("vcf_header_description"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.metadata.getString("vcf_header_description") == field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("infer format fields")(formatFields) { field =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val formatHeader = field.vcfCount match {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case Some(t) => new VCFFormatHeaderLine("field", t, field.vcfType, field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m      case None => new VCFFormatHeaderLine("field", 1, field.vcfType, field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(false, false, Seq.empty, Seq(formatHeader))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val genotypeSchema = schema[0m
[0m[[0m[0mdebug[0m] [0m[0m      .find(_.name == "genotypes")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .get[0m
[0m[[0m[0mdebug[0m] [0m[0m      .dataType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[ArrayType][0m
[0m[[0m[0mdebug[0m] [0m[0m      .elementType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[StructType][0m
[0m[[0m[0mdebug[0m] [0m[0m    val sqlField = genotypeSchema.find(_.name == "field").get[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.dataType == field.sqlType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.metadata.contains("vcf_header_description"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sqlField.metadata.getString("vcf_header_description") == field.description)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("validate headers") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val field1 = new VCFInfoHeaderLine("f1", 1, VCFHeaderLineType.Integer, "monkey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val field2 = new VCFInfoHeaderLine("f1", 1, VCFHeaderLineType.Float, "monkey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      VCFSchemaInferrer.inferSchema(false, false, Seq(field1, field2), Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      infoHeaderLines: Seq[VCFInfoHeaderLine],[0m
[0m[[0m[0mdebug[0m] [0m[0m      formatHeaderLines: Seq[VCFFormatHeaderLine])[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val cases = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("a", 14, VCFHeaderLineType.String, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFFormatHeaderLine("b", 1, VCFHeaderLineType.String, ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("a", VCFHeaderLineCount.A, VCFHeaderLineType.String, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFFormatHeaderLine("a", VCFHeaderLineCount.A, VCFHeaderLineType.String, ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("a", VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.Integer, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFFormatHeaderLine("a", VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.Integer, ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("monkey", 0, VCFHeaderLineType.Flag, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      // Field that has pretty name in genotype schema[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("PL", 0, VCFHeaderLineType.Flag, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    ),[0m
[0m[[0m[0mdebug[0m] [0m[0m    ToFromSchemaTestCase([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFInfoHeaderLine("a", VCFHeaderLineCount.G, VCFHeaderLineType.Float, "")),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(new VCFFormatHeaderLine("b", VCFHeaderLineCount.R, VCFHeaderLineType.Float, ""))[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("to and from schema")(cases) { tc =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(true, true, tc.infoHeaderLines, tc.formatHeaderLines)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val lines = VCFSchemaInferrer.headerLinesFromSchema(schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val allInputLines: Seq[VCFHeaderLine] = tc.formatHeaderLines ++ tc.infoHeaderLines[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(lines.toSet == allInputLines.toSet)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("include count metadata (non-integer)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val line = new VCFInfoHeaderLine("a", VCFHeaderLineCount.A, VCFHeaderLineType.Integer, "")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(true, true, Seq(line), Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.exists { f =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      f.name == "INFO_a" && f.metadata.getString(VCFSchemaInferrer.VCF_HEADER_COUNT_KEY) == "A"[0m
[0m[[0m[0mdebug[0m] [0m[0m    })[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("include count metadata (integer") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val line = new VCFInfoHeaderLine("a", 102, VCFHeaderLineType.Integer, "")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = VCFSchemaInferrer.inferSchema(true, true, Seq(line), Seq.empty)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(schema.exists { f =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      f.name == "INFO_a" && f.metadata.getString(VCFSchemaInferrer.VCF_HEADER_COUNT_KEY) == "102"[0m
[0m[[0m[0mdebug[0m] [0m[0m    })[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("counts for fields without metadata") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("INFO_a", IntegerType),[0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("INFO_b", BooleanType),[0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("INFO_c", ArrayType(IntegerType))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFInfoHeaderLine("a", 1, VCFHeaderLineType.Integer, ""),[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFInfoHeaderLine("b", 0, VCFHeaderLineType.Flag, ""),[0m
[0m[[0m[0mdebug[0m] [0m[0m      new VCFInfoHeaderLine("c", VCFHeaderLineCount.UNBOUNDED, VCFHeaderLineType.Integer, "")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(VCFSchemaInferrer.headerLinesFromSchema(schema) == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("don't include sample ids or otherFields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField([0m
[0m[[0m[0mdebug[0m] [0m[0m          "genotypes",[0m
[0m[[0m[0mdebug[0m] [0m[0m          ArrayType([0m
[0m[[0m[0mdebug[0m] [0m[0m            StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m              Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m                StructField("sampleId", StringType),[0m
[0m[[0m[0mdebug[0m] [0m[0m                StructField("otherFields", MapType(StringType, StringType))[0m
[0m[[0m[0mdebug[0m] [0m[0m              ))))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(VCFSchemaInferrer.headerLinesFromSchema(schema).isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("don't return same key multiple times") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField([0m
[0m[[0m[0mdebug[0m] [0m[0m          "genotypes",[0m
[0m[[0m[0mdebug[0m] [0m[0m          ArrayType([0m
[0m[[0m[0mdebug[0m] [0m[0m            StructType([0m
[0m[[0m[0mdebug[0m] [0m[0m              Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m                StructField("calls", ArrayType(IntegerType)),[0m
[0m[[0m[0mdebug[0m] [0m[0m                StructField("phased", BooleanType)[0m
[0m[[0m[0mdebug[0m] [0m[0m              ))))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val expected = Seq(new VCFFormatHeaderLine("GT", 1, VCFHeaderLineType.String, "Genotype"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(VCFSchemaInferrer.headerLinesFromSchema(schema) == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass GlowSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkTransform(df: DataFrame): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.count() == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(df.as[String].collect.toSeq == Seq("camel", "snake"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uses service provider") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "dummy_transformer",[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark.emptyDataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Map("camel_animal" -> "camel", "snake_animal" -> "snake"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkTransform(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("transformer names are converted to snake case") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "dummyTransformer",[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark.emptyDataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Map("camel_animal" -> "camel", "snake_animal" -> "snake"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkTransform(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("options are converted to snake case") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "dummyTransformer",[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark.emptyDataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Map("camelAnimal" -> "camel", "snake_animal" -> "snake"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkTransform(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("java map options") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val javaMap = new java.util.HashMap[String, String][0m
[0m[[0m[0mdebug[0m] [0m[0m    javaMap.put("camelAnimal", "camel")[0m
[0m[[0m[0mdebug[0m] [0m[0m    javaMap.put("snake_animal", "snake")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Glow.transform("dummyTransformer", spark.emptyDataFrame, javaMap)[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkTransform(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("tuple options") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "dummyTransformer",[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark.emptyDataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m        ("camelAnimal", "camel"),[0m
[0m[[0m[0mdebug[0m] [0m[0m        ("snake_animal", "snake"))[0m
[0m[[0m[0mdebug[0m] [0m[0m    checkTransform(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyTransformer extends DataFrameTransformer {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def name: String = "dummy_transformer"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def transform(df: DataFrame, options: Map[String, String]): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val animals = Seq(options.get("camel_animal"), options.get("snake_animal")).flatten[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.sparkSession.createDataFrame(animals.map(StringWrapper)).sort()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mcase class StringWrapper(s: String)[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.functions.monotonically_increasing_id[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{LongType, StringType, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport picard.vcf.LiftoverVcf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.vcf.VCFConverterBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass LiftOverVariantsTransformerSuite extends GlowBaseTest with VCFConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  val picardTestDataHome = s"$testDataHome/liftover/picard"[0m
[0m[[0m[0mdebug[0m] [0m[0m  val CHAIN_FILE = s"$testDataHome/liftover/hg38ToHg19.over.chain.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m  val REFERENCE_FILE = s"$testDataHome/liftover/hg19.chr20.fa.gz"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val requiredBaseSchema: StructType = new StructType()[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("contigName", StringType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("start", LongType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("end", LongType)[0m
[0m[[0m[0mdebug[0m] [0m[0m    .add("referenceAllele", StringType)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def liftOverAndCompare([0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardLiftedDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardFailedDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      chainFile: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceFile: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt: Option[Double]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val minMatchMap = if (minMatchOpt.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("minMatchRatio" -> minMatchOpt.get.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow[0m
[0m[[0m[0mdebug[0m] [0m[0m      .transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "lift_over_variants",[0m
[0m[[0m[0mdebug[0m] [0m[0m        inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Map("chainFile" -> chainFile, "referenceFile" -> referenceFile) ++ minMatchMap)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val liftedDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .filter("liftOverStatus.success = true")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .select(picardLiftedDf.schema.fieldNames.head, picardLiftedDf.schema.fieldNames.tail: _*)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val failedDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .filter("liftOverStatus.success = false")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .select([0m
[0m[[0m[0mdebug[0m] [0m[0m          "contigName",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "start",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "end",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "referenceAllele",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "alternateAlleles",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "liftOverStatus.errorMessage")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val liftedRows = liftedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val picardLiftedRows = picardLiftedDf.orderBy("contigName", "start").collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(liftedRows.length == picardLiftedRows.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    liftedRows.zip(picardLiftedRows).foreach { case (r1, r2) => assert(r1 == r2) }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val failedRows = failedDf.collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val picardFailedRows =[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardFailedDf[0m
[0m[[0m[0mdebug[0m] [0m[0m        .selectExpr([0m
[0m[[0m[0mdebug[0m] [0m[0m          "contigName",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "start",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "end",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "referenceAllele",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "alternateAlleles",[0m
[0m[[0m[0mdebug[0m] [0m[0m          "filters[0]")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(failedRows.length == picardFailedRows.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    failedRows.zip(picardFailedRows).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (r1, r2) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val numFields = r1.length[0m
[0m[[0m[0mdebug[0m] [0m[0m        var i = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m        while (i < numFields - 1) {[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(r1(i) == r2(i))[0m
[0m[[0m[0mdebug[0m] [0m[0m          i += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(r1.getString(numFields - 1).contains(r2.getString(numFields - 1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def readVcf(vcfFile: String): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcfFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      testVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardLiftedVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardFailedVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      chainFile: String = CHAIN_FILE,[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceFile: String = REFERENCE_FILE,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt: Option[Double] = None): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val picardLiftedDf = readVcf(picardLiftedVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val picardFailedDf = readVcf(picardFailedVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m    liftOverAndCompare([0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardLiftedDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      picardFailedDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      chainFile,[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceFile,[0m
[0m[[0m[0mdebug[0m] [0m[0m      minMatchOpt)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Basic") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/combined.chr20_18210071_18210093.g.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.combined.chr20_18210071_18210093.g.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.combined.chr20_18210071_18210093.g.vcf" // No failures[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some failures") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/unlifted.test.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/lifted.test.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$testDataHome/liftover/failed.test.vcf" // NoTarget[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Don't change original fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(s"$testDataHome/liftover/unlifted.test.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .withColumn("id", monotonically_increasing_id)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow[0m
[0m[[0m[0mdebug[0m] [0m[0m      .transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "lift_over_variants",[0m
[0m[[0m[0mdebug[0m] [0m[0m        inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m        Map("chainFile" -> CHAIN_FILE, "referenceFile" -> REFERENCE_FILE))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("id")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.select("id").as[Long].collect sameElements inputDf.select("id").as[Long].collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  requiredBaseSchema.indices.foreach { idx =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    test(s"Try lifting with insufficient fields, drop ${requiredBaseSchema(idx)}") {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m        spark[0m
[0m[[0m[0mdebug[0m] [0m[0m          .read[0m
[0m[[0m[0mdebug[0m] [0m[0m          .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m          .schema([0m
[0m[[0m[0mdebug[0m] [0m[0m            new StructType(requiredBaseSchema[0m
[0m[[0m[0mdebug[0m] [0m[0m              .fields[0m
[0m[[0m[0mdebug[0m] [0m[0m              .take(idx) ++ requiredBaseSchema.fields.takeRight(requiredBaseSchema.size - idx - 1)))[0m
[0m[[0m[0mdebug[0m] [0m[0m          .load(s"$testDataHome/combined.chr20_18210071_18210093.g.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      assertThrows[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m        Glow[0m
[0m[[0m[0mdebug[0m] [0m[0m          .transform([0m
[0m[[0m[0mdebug[0m] [0m[0m            "lift_over_variants",[0m
[0m[[0m[0mdebug[0m] [0m[0m            inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m            Map("chainFile" -> CHAIN_FILE, "referenceFile" -> REFERENCE_FILE))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    "testLiftoverBiallelicIndels.vcf", // No failures[0m
[0m[[0m[0mdebug[0m] [0m[0m    "testLiftoverMultiallelicIndels.vcf", // ReverseComplementedIndel[0m
[0m[[0m[0mdebug[0m] [0m[0m    "testLiftoverFailingVariants.vcf" // MismatchedRefAllele[0m
[0m[[0m[0mdebug[0m] [0m[0m  ).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m    case baseVcf =>[0m
[0m[[0m[0mdebug[0m] [0m[0m      test(s"Liftover reverse strand $baseVcf") {[0m
[0m[[0m[0mdebug[0m] [0m[0m        compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m          s"$picardTestDataHome/$baseVcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m          s"$picardTestDataHome/lifted.$baseVcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m          s"$picardTestDataHome/failed.$baseVcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m          s"$picardTestDataHome/test.over.chain",[0m
[0m[[0m[0mdebug[0m] [0m[0m          s"$picardTestDataHome/dummy.reference.fasta",[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(1.0)[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Simple indels") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/testLiftoverIndelNoFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/lifted.testLiftoverIndelNoFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/failed.testLiftoverIndelNoFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/test.two.block.over.chain",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/dummy.two.block.reference.fasta",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(.95)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Indel flip") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/testLiftoverIndelFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/lifted.testLiftoverIndelFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/failed.testLiftoverIndelFlip.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/test.over.chain",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/mini.reference.fasta",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1.0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Missing chain file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(s"$testDataHome/combined.chr20_18210071_18210093.g.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m        Glow.transform("lift_over_variants", inputDf, Map("referenceFile" -> REFERENCE_FILE))[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.count[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.contains("Must provide chain file"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Missing reference file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(s"$testDataHome/combined.chr20_18210071_18210093.g.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m        Glow.transform("lift_over_variants", inputDf, Map("chainFile" -> CHAIN_FILE))[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.count[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.contains("Must provide reference file"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No matching refseq") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // chr20 refseq for chr1 interval[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = readVcf(s"$picardTestDataHome/testLiftoverBiallelicIndels.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m      "lift_over_variants",[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("chainFile" -> s"$picardTestDataHome/test.over.chain", "referenceFile" -> REFERENCE_FILE))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val distinctErrorMessages =[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.select("liftOverStatus.errorMessage").distinct.as[String].collect[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(distinctErrorMessages.length == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(distinctErrorMessages.head == LiftoverVcf.FILTER_NO_TARGET)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Reverse complemented bases don't match new reference") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareLiftedVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/testLiftoverBiallelicIndels.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/lifted.mismatchRefSeq.testLiftoverBiallelicIndels.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/failed.mismatchRefSeq.testLiftoverBiallelicIndels.vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/test.over.chain",[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"$picardTestDataHome/dummy2.reference.fasta",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Some(1.0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.pipe[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{InputStream, OutputStream}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.InternalRow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.expressions.GenericInternalRow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{StringType, StructField, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.unsafe.types.UTF8String[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass PipeTransformerSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("cleanup") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    sparkContext.getPersistentRDDs.values.foreach(_.unpersist(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq("dolphin").toDF.repartition(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.rdd.cache()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "dummy_in", "outputFormatter" -> "dummy_out", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(sparkContext.getPersistentRDDs.size == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform("pipe_cleanup", df, Map.empty[String, String])[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(sparkContext.getPersistentRDDs.size == 1) // Should cleanup the RDD cached by piping[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    df.rdd.unpersist()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("read input and output formatters from service loader") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq("dolphin").toDF.repartition(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "dummy_in", "outputFormatter" -> "dummy_out", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count() == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.schema.length == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.schema.exists(f => f.name == "animal" && f.dataType == StringType))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.where("animal = 'monkey'").count() == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform("pipe_cleanup", df, Map.empty[String, String])[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing input formatter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("outputFormatter" -> "dummy_out", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Missing pipe input formatter"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing output formatter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "dummy_in", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Missing pipe output formatter"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("could not find input formatter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "fake_in", "outputFormatter" -> "dummy_out", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Could not find an input formatter for fake_in"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("could not find output formatter") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark.emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "dummy_in", "outputFormatter" -> "fake_out", "cmd" -> """["cat"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[IllegalArgumentException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains("Could not find an output formatter for fake_out"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyInputFormatterFactory() extends InputFormatterFactory {[0m
[0m[[0m[0mdebug[0m] [0m[0m  def name: String = "dummy_in"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def makeInputFormatter(df: DataFrame, options: Map[String, String]): InputFormatter = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    new DummyInputFormatter()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyInputFormatter() extends InputFormatter {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def close(): Unit = ()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def write(record: InternalRow): Unit = ()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def init(stream: OutputStream): Unit = ()[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyOutputFormatterFactory() extends OutputFormatterFactory {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def name: String = "dummy_out"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def makeOutputFormatter(options: Map[String, String]): OutputFormatter = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    new DummyOutputFormatter()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyOutputFormatter() extends OutputFormatter {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def makeIterator(stream: InputStream): Iterator[Any] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val schema = StructType(Seq(StructField("animal", StringType)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val internalRow = new GenericInternalRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      Array(UTF8String.fromString("monkey")).asInstanceOf[Array[Any]])[0m
[0m[[0m[0mdebug[0m] [0m[0m    Iterator(schema, internalRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.pipe[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{StringType, StructField, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass TextPiperSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def afterEach(): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform("pipe_cleanup", spark.emptyDataFrame)[0m
[0m[[0m[0mdebug[0m] [0m[0m    super.afterEach()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def pipeText(df: DataFrame): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "text", "outputFormatter" -> "text", "cmd" -> """["cat", "-"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m    new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("text input and output") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = pipeText(Seq("hello", "world").toDF())[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count() == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.schema == StructType(Seq(StructField("text", StringType))))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.orderBy("text").as[String].collect.toSeq == Seq("hello", "world"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("text input requires one column") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq(Seq("hello", "world"), Seq("foo", "bar")).toDF()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](pipeText(df))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("text input requires string column") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq(Seq(5), Seq(6)).toDF()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException](pipeText(df))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("does not break on null row") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq("hello", null, "hello").toDF()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val output = pipeText(df)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.count() == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(output.filter("text = 'hello'").count == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("command fails") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq("hello", "world").toDF()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "inputFormatter" -> "text",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "outputFormatter" -> "text",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "cmd" -> """["bash", "-c", "exit 1"]""")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ex = intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      new PipeTransformer().transform(df, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ex.getMessage.contains("Subprocess exited with status 1"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // threads should still be cleaned up[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDIN_WRITER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        !Thread[0m
[0m[[0m[0mdebug[0m] [0m[0m          .getAllStackTraces[0m
[0m[[0m[0mdebug[0m] [0m[0m          .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m          .keySet[0m
[0m[[0m[0mdebug[0m] [0m[0m          .exists(_.getName.startsWith(ProcessHelper.STDERR_READER_THREAD_PREFIX)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.pipe[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.DataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{StringType, StructField}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass CSVPiperSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val saige = s"$testDataHome/saige_output.txt"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val csv = s"$testDataHome/no_header.csv"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def afterEach(): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform("pipe_cleanup", spark.emptyDataFrame)[0m
[0m[[0m[0mdebug[0m] [0m[0m    super.afterEach()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def pipeCsv([0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf: DataFrame,[0m
[0m[[0m[0mdebug[0m] [0m[0m      cmd: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDelimiter: Option[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDelimiter: Option[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputHeader: Option[Boolean],[0m
[0m[[0m[0mdebug[0m] [0m[0m      outputHeader: Option[Boolean]): DataFrame = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val baseOptions =[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "csv", "outputFormatter" -> "csv", "cmd" -> cmd)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDelimiterOption = if (inputDelimiter.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("in_delimiter" -> inputDelimiter.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDelimiterOption = if (outputDelimiter.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("out_delimiter" -> outputDelimiter.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputHeaderOption = if (inputHeader.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("in_header" -> inputHeader.get.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputHeaderOption = if (outputHeader.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("out_header" -> outputHeader.get.toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map.empty[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m      "pipe",[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      baseOptions ++ inputDelimiterOption ++ outputDelimiterOption ++ inputHeaderOption ++ outputHeaderOption)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Delimiter and header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark.read.option("delimiter", " ").option("header", "true").csv(saige)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeCsv(inputDf, """["sed", "s/:/ /g"]""", Some(":"), Some(" "), Some(true), Some(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.orderBy("CHR", "POS").collect.toSeq == inputDf.orderBy("CHR", "POS").collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some empty partitions with header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", " ").option("header", "true").csv(saige).repartition(20)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeCsv(inputDf, s"""["cat", "-"]""", Some(" "), Some(" "), Some(true), Some(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.orderBy("CHR", "POS").collect.toSeq == inputDf.orderBy("CHR", "POS").collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Single row with header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", " ").option("header", "true").csv(saige).limit(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeCsv(inputDf, s"""["cat", "-"]""", Some(" "), Some(" "), Some(true), Some(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.collect.toSeq == inputDf.collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No rows with header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", " ").option("header", "true").csv(saige).limit(0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeCsv(inputDf, s"""["cat", "-"]""", Some(" "), Some(" "), Some(true), Some(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Default options: comma delimiter and no header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark.read.option("delimiter", ",").option("header", "false").csv(csv)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = pipeCsv(inputDf, s"""["cat", "-"]""", None, None, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.orderBy("_c0").collect.toSeq == inputDf.orderBy("_c0").collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Some empty partitions without header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", ",").option("header", "false").csv(csv).repartition(20)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = pipeCsv(inputDf, s"""["cat", "-"]""", None, None, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.orderBy("_c0").collect.toSeq == inputDf.orderBy("_c0").collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Single row and no header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", ",").option("header", "false").csv(csv).limit(1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = pipeCsv(inputDf, s"""["cat", "-"]""", None, None, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.schema == inputDf.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.collect.toSeq == inputDf.collect.toSeq)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No rows and no header") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.option("delimiter", ",").option("header", "false").csv(csv).limit(0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalStateException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      pipeCsv(inputDf, s"""["cat", "-"]""", None, None, None, None))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("GWAS") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val na12878 = s"$testDataHome/NA12878_21_10002403.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format("vcf").load(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "csv",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "out_header" -> "true",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "out_delimiter" -> " ",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["$testDataHome/vcf/scripts/gwas.sh"]"""[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform("pipe", input, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.schema.fields.toSeq == Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("CHR", StringType, nullable = true),[0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("POS", StringType, nullable = true),[0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("pValue", StringType, nullable = true)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.count == input.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("pValue = 0.5").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Gene-based GWAS") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val na12878 = s"$testDataHome/NA12878_21_10002403.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format("vcf").load(na12878)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options = Map([0m
[0m[[0m[0mdebug[0m] [0m[0m      "inputFormatter" -> "vcf",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "outputFormatter" -> "csv",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "in_vcfHeader" -> na12878,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "out_header" -> "true",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "out_delimiter" -> " ",[0m
[0m[[0m[0mdebug[0m] [0m[0m      "cmd" -> s"""["python", "$testDataHome/vcf/scripts/gwas-region.py", "$testDataHome/vcf/scripts/group_file.txt"]"""[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform("pipe", input, options)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      outputDf.schema.fields.toSeq == Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("Gene", StringType, nullable = true),[0m
[0m[[0m[0mdebug[0m] [0m[0m        StructField("pValue", StringType, nullable = true)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.count == 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.filter("pValue = 0.5").count == outputDf.count)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Big file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf = spark.read.text(s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outputDf = Glow.transform([0m
[0m[[0m[0mdebug[0m] [0m[0m      "pipe",[0m
[0m[[0m[0mdebug[0m] [0m[0m      inputDf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Map("inputFormatter" -> "text", "outputFormatter" -> "csv", "cmd" -> """["cat", "-"]"""))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(outputDf.count() == 1103)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.util[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.scalatest.FunSuite[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass StringUtilsSuite extends FunSuite {[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def testSnakeConversion(name: String, input: String, expected: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    test(name) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(StringUtils.toSnakeCase(input) == expected)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  testSnakeConversion([0m
[0m[[0m[0mdebug[0m] [0m[0m    "doesn't change lower case string",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "monkey",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "monkey"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  testSnakeConversion([0m
[0m[[0m[0mdebug[0m] [0m[0m    "doesn't change lower case string with underscores",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "mon_key",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "mon_key"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  testSnakeConversion([0m
[0m[[0m[0mdebug[0m] [0m[0m    "simple camel to snake case",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "monKey",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "mon_key"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  testSnakeConversion([0m
[0m[[0m[0mdebug[0m] [0m[0m    "upper camel to snake",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "MonKey",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "mon_key"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  testSnakeConversion([0m
[0m[[0m[0mdebug[0m] [0m[0m    "mixed",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "MonKe_y",[0m
[0m[[0m[0mdebug[0m] [0m[0m    "mon_ke_y"[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("SnakeCaseMap") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val m = new SnakeCaseMap([0m
[0m[[0m[0mdebug[0m] [0m[0m      Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "AniMal" -> "MonKey"[0m
[0m[[0m[0mdebug[0m] [0m[0m      ))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(m("AniMal") == "MonKey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(m("aniMal") == "MonKey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(m("ani_mal") == "MonKey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!m.contains("animal"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("SnakeCaseMap (add / subtract)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val base = new SnakeCaseMap([0m
[0m[[0m[0mdebug[0m] [0m[0m      Map([0m
[0m[[0m[0mdebug[0m] [0m[0m        "AniMal" -> "MonKey",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "vegeTable" -> "carrot"[0m
[0m[[0m[0mdebug[0m] [0m[0m      ))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val added = base + ("kEy" -> "value")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(added("ani_mal") == "MonKey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(added("k_ey") == "value")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val subtracted = base - "vege_table"[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(subtracted("ani_mal") == "MonKey")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(subtracted.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.normalizevariants[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.GlowLogging[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass NormalizeVariantsTransformerSuite extends GlowBaseTest with GlowLogging {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val sourceName: String = "vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testFolder: String = s"$testDataHome/variantnormalizer-test"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // gatk test file (multiallelic)[0m
[0m[[0m[0mdebug[0m] [0m[0m  // The base of vcfs and reference in these test files were taken from gatk[0m
[0m[[0m[0mdebug[0m] [0m[0m  // LeftTrimAndLeftAlign test suite. The reference genome was trimmed to +/-400 bases around[0m
[0m[[0m[0mdebug[0m] [0m[0m  // each variant to generate a small reference fasta. The vcf variants were modified accordingly.[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestReference =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/Homo_sapiens_assembly38.20.21_altered.fasta"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcf =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfExpectedSplit =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_gatksplit.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfExpectedNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfExpectedSplitNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_gatksplit_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // These files are similar to above but contain symbolic variants.[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfSymbolic =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_symbolic.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfSymbolicExpectedSplit =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_symbolic_gatksplit.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfSymbolicExpectedNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_symbolic_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val gatkTestVcfSymbolicExpectedSplitNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/test_left_align_hg38_altered_symbolic_gatksplit_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // vt test files[0m
[0m[[0m[0mdebug[0m] [0m[0m  // The base of vcfs and reference in these test files were taken from vt[0m
[0m[[0m[0mdebug[0m] [0m[0m  // (https://genome.sph.umich.edu/wiki/Vt) normalization test suite. The vcf in this test suite[0m
[0m[[0m[0mdebug[0m] [0m[0m  // is biallelic. The reference genome was trimmed to +/-100 bases around each variant to[0m
[0m[[0m[0mdebug[0m] [0m[0m  // generate a small reference fasta. The vcf variants were modified accordingly.[0m
[0m[[0m[0mdebug[0m] [0m[0m  //[0m
[0m[[0m[0mdebug[0m] [0m[0m  // The multialleleic versions were generated by artificially adding more alleles and[0m
[0m[[0m[0mdebug[0m] [0m[0m  // corresponding genotypes to some of the variants.[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestReference = s"$testFolder/20_altered.fasta"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfBiallelic =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_biallelic.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfBiallelicExpectedNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_biallelic_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfMultiAllelic =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_multiallelic.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfMultiAllelicExpectedSplit =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_multiallelic_gatksplit.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfMultiAllelicExpectedNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_multiallelic_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestVcfMultiAllelicExpectedSplitNormalized =[0m
[0m[[0m[0mdebug[0m] [0m[0m    s"$testFolder/01_IN_altered_multiallelic_gatksplit_bcftoolsnormalized.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def sparkConf: SparkConf = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    super[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set([0m
[0m[[0m[0mdebug[0m] [0m[0m        "spark.hadoop.io.compression.codecs",[0m
[0m[[0m[0mdebug[0m] [0m[0m        "org.seqdoop.hadoop_bam.util.BGZFCodec"[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   *  Tests whether the transformed VCF matches the expected VCF[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      originalVCFFileName: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedVCFFileName: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceGenome: Option[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      mode: Option[String][0m
[0m[[0m[0mdebug[0m] [0m[0m  ): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val options: Map[String, String] = Map() ++ {[0m
[0m[[0m[0mdebug[0m] [0m[0m        referenceGenome match {[0m
[0m[[0m[0mdebug[0m] [0m[0m          case Some(r) => Map("referenceGenomePath" -> r)[0m
[0m[[0m[0mdebug[0m] [0m[0m          case None => Map()[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m      } ++ {[0m
[0m[[0m[0mdebug[0m] [0m[0m        mode match {[0m
[0m[[0m[0mdebug[0m] [0m[0m          case Some(m) => Map("mode" -> m)[0m
[0m[[0m[0mdebug[0m] [0m[0m          case None => Map()[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfOriginal = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(originalVCFFileName)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfNormalized = Glow[0m
[0m[[0m[0mdebug[0m] [0m[0m      .transform([0m
[0m[[0m[0mdebug[0m] [0m[0m        "normalize_variants",[0m
[0m[[0m[0mdebug[0m] [0m[0m        dfOriginal,[0m
[0m[[0m[0mdebug[0m] [0m[0m        options[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start", "end")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfNormalized.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dfExpected = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(expectedVCFFileName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start", "end")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfExpected.rdd.count()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(dfNormalized.count() == dfExpected.count())[0m
[0m[[0m[0mdebug[0m] [0m[0m    dfExpected[0m
[0m[[0m[0mdebug[0m] [0m[0m      .drop("splitFromMultiAllelic")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .zip(dfNormalized.drop("splitFromMultiAllelic").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (rowExp, rowNorm) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(rowExp.equals(rowNorm), s"Expected\n$rowExp\nNormalized\n$rowNorm")[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("normalization transform do-normalize-no-split no-reference") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // vcf containing multi-allelic variants[0m
[0m[[0m[0mdebug[0m] [0m[0m    try {[0m
[0m[[0m[0mdebug[0m] [0m[0m      testNormalizedvsExpected(vtTestVcfMultiAllelic, vtTestVcfMultiAllelic, None, None)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } catch {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case _: IllegalArgumentException => succeed[0m
[0m[[0m[0mdebug[0m] [0m[0m      case _: Throwable => fail()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("normalization transform do-normalize-no-split") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfBiallelic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfBiallelicExpectedNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(vtTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("normalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelicExpectedNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(vtTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfExpectedNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(gatkTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("normalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolicExpectedNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(gatkTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      None)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("normalization transform no-normalize-do-split") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected(vtTestVcfBiallelic, vtTestVcfBiallelic, None, Option("split"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelicExpectedSplit,[0m
[0m[[0m[0mdebug[0m] [0m[0m      None,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("split"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected(gatkTestVcf, gatkTestVcfExpectedSplit, None, Option("split"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolicExpectedSplit,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(gatkTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("split"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("normalization transform do-normalize-do-split") {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfBiallelic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfBiallelicExpectedNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(vtTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("splitAndNormalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestVcfMultiAllelicExpectedSplitNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(vtTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("splitAndNormalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcf,[0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfExpectedSplitNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(gatkTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("splitAndNormalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testNormalizedvsExpected([0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolic,[0m
[0m[[0m[0mdebug[0m] [0m[0m      gatkTestVcfSymbolicExpectedSplitNormalized,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option(gatkTestReference),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Option("splitAndNormalize"))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.transformers.normalizevariants[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Paths[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.variant.variantcontext.Allele[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.broadinstitute.hellbender.engine.ReferenceDataSource[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.GlowLogging[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.transformers.normalizevariants.VariantNormalizer._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass VariantNormalizerSuite extends GlowBaseTest with GlowLogging {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val sourceName: String = "vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val testFolder: String = s"$testDataHome/variantnormalizer-test"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vtTestReference = s"$testFolder/20_altered.fasta"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * Tests realignAlleles method for given alleles and compares with the provided expected[0m
[0m[[0m[0mdebug[0m] [0m[0m   * outcome[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  def testRealignAlleles([0m
[0m[[0m[0mdebug[0m] [0m[0m      referenceGenome: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      contig: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      origStart: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      origEnd: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      origAlleleStrings: Seq[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedStart: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedEnd: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      expectedAlleleString: Seq[String]): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val refGenomeDataSource = ReferenceDataSource.of(Paths.get(referenceGenome))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val alleles = origAlleleStrings.take(1).map(Allele.create(_, true)) ++[0m
[0m[[0m[0mdebug[0m] [0m[0m      origAlleleStrings.drop(1).map(Allele.create(_))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val reAlignedAlleles =[0m
[0m[[0m[0mdebug[0m] [0m[0m      realignAlleles(AlleleBlock(alleles, origStart, origEnd), refGenomeDataSource, contig)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(reAlignedAlleles.start == expectedStart)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(reAlignedAlleles.end == expectedEnd)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(reAlignedAlleles.alleles.length == expectedAlleleString.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    for (i <- 0 to expectedAlleleString.length - 1) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(expectedAlleleString(i) == reAlignedAlleles.alleles(i).getBaseString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("test realignAlleles") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    testRealignAlleles([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestReference,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      71,[0m
[0m[[0m[0mdebug[0m] [0m[0m      73,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("AA", "AAAA", "AAAAAA"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      67,[0m
[0m[[0m[0mdebug[0m] [0m[0m      68,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("T", "TAA", "TAAAA")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testRealignAlleles([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestReference,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      36,[0m
[0m[[0m[0mdebug[0m] [0m[0m      76,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("GAAGGCATAGCCATTACCTTTTAAAAAATTTTAAAAAAAGA", "GA"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      28,[0m
[0m[[0m[0mdebug[0m] [0m[0m      67,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("AAAAAAAAGAAGGCATAGCCATTACCTTTTAAAAAATTTT", "A")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testRealignAlleles([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestReference,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("GG", "GA"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("G", "A")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    testRealignAlleles([0m
[0m[[0m[0mdebug[0m] [0m[0m      vtTestReference,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "20",[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("GG", "TA"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("GG", "TA")[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.bgen[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.encoders.RowEncoder[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{BgenGenotype, BgenRow}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass BgenRowConverterSuite extends BgenConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val sourceName = "bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def compareVcfToBgen([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBgen: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      testVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      bitsPerProb: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPhasing: Boolean = false) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val bgenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testBgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new InternalRowToBgenRowConverter(vcfDs.schema, 10, 2, defaultPhasing)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val encoder = RowEncoder.apply(vcfDs.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    bgenDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sort("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .zip(vcfDs.sort("contigName", "start").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (br, vr) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          checkBgenRowsEqual(br, converter.convert(encoder.toRow(vr)), false, bitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 8 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfToBgen(s"$testRoot/example.8bits.bgen", s"$testRoot/example.8bits.vcf", 8)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 16 bit (with missing samples)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfToBgen(s"$testRoot/example.16bits.bgen", s"$testRoot/example.16bits.vcf", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 32 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfToBgen(s"$testRoot/example.32bits.bgen", s"$testRoot/example.32bits.vcf", 32)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("phased") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareVcfToBgen(s"$testRoot/phased.16bits.bgen", s"$testRoot/phased.16bits.vcf", 16, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def inferPhasingOrPloidy([0m
[0m[[0m[0mdebug[0m] [0m[0m      alternateAlleles: Seq[String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      numPosteriorProbabilities: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      phasedOpt: Option[Boolean],[0m
[0m[[0m[0mdebug[0m] [0m[0m      phased: Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      ploidyOpt: Option[Int],[0m
[0m[[0m[0mdebug[0m] [0m[0m      ploidy: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      shouldThrow: Boolean = false,[0m
[0m[[0m[0mdebug[0m] [0m[0m      maxPloidy: Int = 10,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPloidy: Int = 2,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPhasing: Boolean = false): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val v = BgenRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      "chr1",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10,[0m
[0m[[0m[0mdebug[0m] [0m[0m      11,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Nil,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      alternateAlleles,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        BgenGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          phasedOpt,[0m
[0m[[0m[0mdebug[0m] [0m[0m          ploidyOpt,[0m
[0m[[0m[0mdebug[0m] [0m[0m          (0 until numPosteriorProbabilities).map(_.toDouble)[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new InternalRowToBgenRowConverter([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRow.schema,[0m
[0m[[0m[0mdebug[0m] [0m[0m      maxPloidy,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPloidy,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPhasing[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    val encoder = RowEncoder.apply(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = Seq(v).toDF[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (shouldThrow) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assertThrows[IllegalStateException](converter.convert(encoder.toRow(df.collect.head)))[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      val row = converter.convert(encoder.toRow(df.collect.head))[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(row.genotypes.head.phased.get == phased)[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(row.genotypes.head.ploidy.get == ploidy)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Infer phasing from ploidy") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Unphased[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 3, None, false, Some(2), 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 10, None, false, Some(3), 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Phased[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 4, None, true, Some(2), 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 9, None, true, Some(3), 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Throw if nonsensical[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 2, None, false, Some(2), 2, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 5, None, true, Some(2), 2, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("If phasing ambiguous, use default phasing when inferring from ploidy") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Default phasing is false[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 0, None, false, Some(0), 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 2, None, false, Some(1), 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 2, None, true, Some(1), 1, defaultPhasing = true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Infer ploidy from phasing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Unphased[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 3, Some(false), false, None, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 10, Some(false), false, None, 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Phased[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 4, Some(true), true, None, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 9, Some(true), true, None, 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Throw if nonsensical[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 9, Some(false), false, None, 3, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 5, Some(true), true, None, 2, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("If ploidy ambiguous, use default ploidy when inferring from phasing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Default ploidy is 2[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 0, Some(true), true, None, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 0, Some(false), false, None, 3, defaultPloidy = 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("If no ploidy or phasing info provided, use default ploidy") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Default ploidy is 2[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 3, None, false, None, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 0, None, false, None, 0, defaultPloidy = 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 9, None, true, None, 3, defaultPloidy = 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Infer ploidy for unphased up to max") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Max is 10[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 66, Some(false), false, None, 10)[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T"), 12, Some(false), false, None, 11, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    inferPhasingOrPloidy(Seq("T", "C"), 66, Some(false), false, None, 10, maxPloidy = 12)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throws on mixed phasing") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val converter = new InternalRowToBgenRowConverter(BgenRow.schema, 10, 2, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val encoder = RowEncoder.apply(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val mixedRow = BgenRow([0m
[0m[[0m[0mdebug[0m] [0m[0m      "chr1",[0m
[0m[[0m[0mdebug[0m] [0m[0m      10,[0m
[0m[[0m[0mdebug[0m] [0m[0m      11,[0m
[0m[[0m[0mdebug[0m] [0m[0m      Nil,[0m
[0m[[0m[0mdebug[0m] [0m[0m      "A",[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq("T"),[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m        BgenGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(true),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(2),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Nil[0m
[0m[[0m[0mdebug[0m] [0m[0m        ),[0m
[0m[[0m[0mdebug[0m] [0m[0m        BgenGenotype([0m
[0m[[0m[0mdebug[0m] [0m[0m          None,[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(false),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Some(2),[0m
[0m[[0m[0mdebug[0m] [0m[0m          Nil[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val row = Seq(mixedRow).toDF.collect.head[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalStateException](converter.convert(encoder.toRow(row)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.bgen[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{File, FileInputStream}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Files[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport com.google.common.io.LittleEndianDataInputStream[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.BgenRow[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass BgenWriterSuite extends BgenConverterBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val sourceName = "bigbgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def roundTrip(testBgen: String, bitsPerProb: Int) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenDir = Files.createTempDirectory("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenFile = newBgenDir.resolve("temp.bgen").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val origDs =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format("bgen").schema(BgenRow.schema).load(testBgen).as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    origDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("bitsPerProbability", bitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Check that rows in new file are approximately equal[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newDs =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format("bgen").schema(BgenRow.schema).load(newBgenFile).as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    origDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sort("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .zip(newDs.sort("contigName", "start").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach { case (or, nr) => checkBgenRowsEqual(or, nr, true, bitsPerProb) }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Check that size of files are approximately equal (excluding free data area in header)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val origStream = new FileInputStream(testBgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val littleEndianOrigStream = new LittleEndianDataInputStream(origStream)[0m
[0m[[0m[0mdebug[0m] [0m[0m    littleEndianOrigStream.skipBytes(4) // Skip to size of header[0m
[0m[[0m[0mdebug[0m] [0m[0m    val headerLength = littleEndianOrigStream.readInt()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val freeDataSize = headerLength - 20[0m
[0m[[0m[0mdebug[0m] [0m[0m    origStream.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val origFileSize = new File(testBgen).length()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newFileSize = new File(newBgenFile).length()[0m
[0m[[0m[0mdebug[0m] [0m[0m    require([0m
[0m[[0m[0mdebug[0m] [0m[0m      origFileSize - freeDataSize ~= newFileSize relTol 0.1,[0m
[0m[[0m[0mdebug[0m] [0m[0m      s"Orig file size $origFileSize with free data size $freeDataSize, new file size $newFileSize"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def changeVariantIdAndRsid(variantId: Option[String], rsid: Option[String], testBgen: String) {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenDir = Files.createTempDirectory("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenFile = newBgenDir.resolve("temp.bgen").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val origDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testBgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .map { br =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        br.copy(names = Seq(variantId, rsid).flatten)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    origDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach { br =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.names == Seq(variantId.getOrElse(""), rsid.getOrElse("")))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  def roundTripVcf([0m
[0m[[0m[0mdebug[0m] [0m[0m      testBgen: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      testVcf: String,[0m
[0m[[0m[0mdebug[0m] [0m[0m      bitsPerProb: Int,[0m
[0m[[0m[0mdebug[0m] [0m[0m      defaultPhasing: Option[Boolean] = None) {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenDir = Files.createTempDirectory("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenFile = newBgenDir.resolve("temp.bgen").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val bgenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testBgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    val origVcfDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(testVcf)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writer = origVcfDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("bitsPerProbability", bitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writerWithDefaultPhasing = if (defaultPhasing.isDefined) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      writer.option("defaultInferredPhasing", defaultPhasing.get)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      writer[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    writerWithDefaultPhasing.save(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcfDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    bgenDs[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sort("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect[0m
[0m[[0m[0mdebug[0m] [0m[0m      .zip(vcfDs.sort("contigName", "start").collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m        case (br, vr) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m          checkBgenRowsEqual(br, vr, false, bitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 8 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/example.8bits.bgen", 8)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 16 bit (with missing samples)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/example.16bits.bgen", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 32 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/example.32bits.bgen", 32)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("phased") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/phased.16bits.bgen", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("complex 16 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/complex.16bits.bgen", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing sample IDs") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTrip(s"$testRoot/example.16bits.oxford.bgen", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("missing variant ID and rsid") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    changeVariantIdAndRsid(None, None, s"$testRoot/example.16bits.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("single name") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    changeVariantIdAndRsid(Some("fakeId"), None, s"$testRoot/example.16bits.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("invalid bitsPerProb option") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[SparkException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq[0m
[0m[[0m[0mdebug[0m] [0m[0m        .empty[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m        .toDF()[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("bitsPerProbability", 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(Files.createTempDirectory("bgen").resolve("out.bgen").toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Represent probabilities as int") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.calculateIntProbabilities(bitsPerProb = 2, Seq(0.5, 0.5)).sorted ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(1, 2) // Unrounded: 1.5, 1.5[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.calculateIntProbabilities(bitsPerProb = 8, Seq(0.99, 0.01)) ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(252, 3) // Unrounded: 252.45, 2.55[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.calculateIntProbabilities(bitsPerProb = 16, Seq(0.605, 0.283, 0.122)) ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(39649, 18547, 7995) // Unrounded: 39648.675, 18546.405, 7995.27[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.calculateIntProbabilities(bitsPerProb = 32, Seq(0.23, 0.27, 0.16, 0.34)) ==[0m
[0m[[0m[0mdebug[0m] [0m[0m      Seq(987842478, 1159641170, 687194767, 1460288881)[0m
[0m[[0m[0mdebug[0m] [0m[0m      // Unrounded: 987842477.85, 1159641169.65, 687194767.2, 1460288880.3[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Probability block sizes") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.getProbabilityBlockSize([0m
[0m[[0m[0mdebug[0m] [0m[0m        GenotypeCharacteristics(numAlleles = 3, phased = false, ploidy = 3)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ) == ProbabilityBlockSize(10, 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      BgenRecordWriter.getProbabilityBlockSize([0m
[0m[[0m[0mdebug[0m] [0m[0m        GenotypeCharacteristics(numAlleles = 4, phased = true, ploidy = 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      ) == ProbabilityBlockSize(4, 2)[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Empty file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenDir = Files.createTempDirectory("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenFile = newBgenDir.resolve("temp.bgen").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkContext[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyRDD[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .toDS[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rewrittenDs.collect.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 8 bit VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTripVcf(s"$testRoot/example.8bits.bgen", s"$testRoot/example.8bits.vcf", 8)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 16 bit (with missing samples) VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTripVcf(s"$testRoot/example.16bits.bgen", s"$testRoot/example.16bits.vcf", 16)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 32 bit VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTripVcf(s"$testRoot/example.32bits.bgen", s"$testRoot/example.32bits.vcf", 32)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("phased VCF") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    roundTripVcf(s"$testRoot/phased.16bits.bgen", s"$testRoot/phased.16bits.vcf", 16, Some(true))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("No genotype") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenDir = Files.createTempDirectory("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val newBgenFile = newBgenDir.resolve("temp.bgen").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val noGtRow = BgenRow("chr1", 10, 11, Nil, "A", Seq("T"), Nil)[0m
[0m[[0m[0mdebug[0m] [0m[0m    Seq(noGtRow)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toDS[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bigbgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenDs = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(BgenRow.schema)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(newBgenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(rewrittenDs.collect.head.genotypes.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.bgen[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.io.{BufferedReader, InputStreamReader}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport scala.collection.JavaConverters._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport com.google.common.io.LittleEndianDataInputStream[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.hadoop.fs.Path[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.SparkException[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.catalyst.ScalaReflection[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.types.{ArrayType, StructType}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common._[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass BgenReaderSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val sourceName = "bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m  private val testRoot = s"$testDataHome/bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def iterateFile(path: String): Seq[BgenRow] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val p = new Path(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fs = p.getFileSystem(spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val baseStream = fs.open(p)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new LittleEndianDataInputStream(baseStream)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = new BgenHeaderReader(stream).readHeader()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val iterator = new BgenFileIterator(header, stream, baseStream, 0, fs.getFileStatus(p).getLen)[0m
[0m[[0m[0mdebug[0m] [0m[0m    iterator.init()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ret = iterator.toList[0m
[0m[[0m[0mdebug[0m] [0m[0m    baseStream.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m    ret[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def compareBgenToVcf(bgenPath: String, vcfPath: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val bgen = iterateFile(bgenPath)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sortBy(r => (r.contigName, r.start))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map(r => r.copy(names = r.names.filter(_.nonEmpty).distinct.sorted))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val vcf = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("includeSampleIds", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("vcfRowSchema", true)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(vcfPath)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[VCFRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map { r =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        val names = r.names[0m
[0m[[0m[0mdebug[0m] [0m[0m        // QCTools incorrectly separates IDs with commas instead of semicolons when exporting to VCF[0m
[0m[[0m[0mdebug[0m] [0m[0m        r.copy(names = names.flatMap(_.split(",")))[0m
[0m[[0m[0mdebug[0m] [0m[0m      }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(bgen.size == vcf.size)[0m
[0m[[0m[0mdebug[0m] [0m[0m    bgen.zip(vcf).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (br, vr) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.contigName == vr.contigName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.start == vr.start)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.end == vr.end)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.names == vr.names)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.referenceAllele == vr.referenceAllele)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.alternateAlleles == vr.alternateAlleles)[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert(br.genotypes.length == vr.genotypes.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m        br.genotypes.zip(vr.genotypes).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m          case (bg, vg) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m            // Note: QCTools inserts a dummy "NA" sample ID if absent when exporting to VCF[0m
[0m[[0m[0mdebug[0m] [0m[0m            assert(bg.sampleId == vg.sampleId || vg.sampleId.get.startsWith("NA"))[0m
[0m[[0m[0mdebug[0m] [0m[0m            bg.posteriorProbabilities.indices.foreach { i =>[0m
[0m[[0m[0mdebug[0m] [0m[0m              bg.posteriorProbabilities(i) == vg.posteriorProbabilities.get(i)[0m
[0m[[0m[0mdebug[0m] [0m[0m            }[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def getSampleIds(path: String, colIdx: Int = 1): Seq[String] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val p = new Path(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fs = p.getFileSystem(spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = fs.open(p)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val streamReader = new InputStreamReader(stream)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val bufferedReader = new BufferedReader(streamReader)[0m
[0m[[0m[0mdebug[0m] [0m[0m    // The first two (2) lines in a .sample file are header lines[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = bufferedReader[0m
[0m[[0m[0mdebug[0m] [0m[0m      .lines()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .skip(2)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .iterator()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asScala[0m
[0m[[0m[0mdebug[0m] [0m[0m      .map(_.split(" ").apply(colIdx))[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toList[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    stream.close()[0m
[0m[[0m[0mdebug[0m] [0m[0m    sampleIds[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 8 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareBgenToVcf(s"$testRoot/example.8bits.bgen", s"$testRoot/example.8bits.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 16 bit (with missing samples)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareBgenToVcf(s"$testRoot/example.16bits.bgen", s"$testRoot/example.16bits.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("unphased 32 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareBgenToVcf(s"$testRoot/example.32bits.bgen", s"$testRoot/example.32bits.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("phased") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareBgenToVcf(s"$testRoot/phased.16bits.bgen", s"$testRoot/phased.16bits.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("complex 16 bit") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    compareBgenToVcf(s"$testRoot/complex.16bits.bgen", s"$testRoot/complex.16bits.vcf")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("skip entire file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val p = new Path(s"$testRoot/example.16bits.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fs = p.getFileSystem(spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val baseStream = fs.open(p)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new LittleEndianDataInputStream(baseStream)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = new BgenHeaderReader(stream).readHeader()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val iterator = new BgenFileIterator([0m
[0m[[0m[0mdebug[0m] [0m[0m      header,[0m
[0m[[0m[0mdebug[0m] [0m[0m      stream,[0m
[0m[[0m[0mdebug[0m] [0m[0m      baseStream,[0m
[0m[[0m[0mdebug[0m] [0m[0m      fs.getFileStatus(p).getLen,[0m
[0m[[0m[0mdebug[0m] [0m[0m      fs.getFileStatus(p).getLen[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    iterator.init()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!iterator.hasNext) // should skip entire file[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("skip to last record") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val p = new Path(s"$testRoot/complex.16bits.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fs = p.getFileSystem(spark.sparkContext.hadoopConfiguration)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val baseStream = fs.open(p)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val stream = new LittleEndianDataInputStream(baseStream)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val header = new BgenHeaderReader(stream).readHeader()[0m
[0m[[0m[0mdebug[0m] [0m[0m    val iterator = new BgenFileIterator([0m
[0m[[0m[0mdebug[0m] [0m[0m      header,[0m
[0m[[0m[0mdebug[0m] [0m[0m      stream,[0m
[0m[[0m[0mdebug[0m] [0m[0m      baseStream,[0m
[0m[[0m[0mdebug[0m] [0m[0m      774L, // last record start position[0m
[0m[[0m[0mdebug[0m] [0m[0m      fs.getFileStatus(p).getLen[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    iterator.init()[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(iterator.toSeq.size == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("read with spark") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testRoot/example.16bits.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fromSpark = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m      .toSeq[0m
[0m[[0m[0mdebug[0m] [0m[0m    val direct = iterateFile(path).sortBy(r => (r.contigName, r.start))[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(fromSpark.size == direct.size)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(fromSpark == direct)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("read with spark (no index)") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testRoot/example.16bits.noindex.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val fromSpark = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .collect()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(fromSpark.length == iterateFile(path).size)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("read only certain fields") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testRoot/example.16bits.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val allele = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .orderBy("start")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .select("referenceAllele")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[String][0m
[0m[[0m[0mdebug[0m] [0m[0m      .first[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(allele == "A")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class WeirdSchema(animal: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("be permissive if schema includes fields that can't be derived from BGEN") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testRoot/example.16bits.noindex.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .schema(ScalaReflection.schemaFor[WeirdSchema].dataType.asInstanceOf[StructType])[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .rdd[0m
[0m[[0m[0mdebug[0m] [0m[0m      .count() // No error expected[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Sample IDs present if .sample file is provided") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val basePath = s"$testRoot/example.16bits.oxford"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleFilePath", s"$basePath.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleIdColumn", "ID_2")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$basePath.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = getSampleIds(s"$basePath.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ds.genotypes.map(_.sampleId.get) == sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Sample IDs present if no sample column provided but matches default") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val basePath = s"$testRoot/example.16bits.oxford"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleFilePath", s"$basePath.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$basePath.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = getSampleIds(s"$basePath.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ds.genotypes.map(_.sampleId.get) == sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Returns all sample IDs provided in corrupted .sample file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val basePath = s"$testRoot/example.16bits.oxford"[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("sampleFilePath", s"$basePath.corrupted.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$basePath.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m        .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Only uses .sample file if no samples in bgen file") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = s"$testRoot/example.16bits.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleFilePath", s"$testRoot/example.fake.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(path)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ds.genotypes.forall(!_.sampleId.get.startsWith("fake")))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throws if wrong provided column name") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val basePath = s"$testRoot/example.16bits.oxford"[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("sampleFilePath", s"$basePath.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("sampleIdColumn", "FAKE")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$basePath.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m        .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Sample IDs present using provided column name") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val ds = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleFilePath", s"$testRoot/example.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleIdColumn", "ID_1")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testRoot/example.16bits.oxford.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m      .head[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sampleIds = getSampleIds(s"$testRoot/example.sample", 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(ds.genotypes.map(_.sampleId.get) == sampleIds)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Throws if default sample column doesn't match") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m    import sess.implicits._[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assertThrows[IllegalArgumentException]([0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option("sampleFilePath", s"$testRoot/example.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(s"$testRoot/example.16bits.oxford.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .as[BgenRow][0m
[0m[[0m[0mdebug[0m] [0m[0m        .head[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("Skip non-bgen files") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val input = s"$testRoot/example.8bits.*"[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark.read.format(sourceName).load(input).count() // No error[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    // Expect an error because we try to read non-bgen files as bgen[0m
[0m[[0m[0mdebug[0m] [0m[0m    intercept[SparkException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .read[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .option(BgenOptions.IGNORE_EXTENSION_KEY, true)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .load(input)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .count()[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  private def hasSampleIdField(schema: StructType): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    schema[0m
[0m[[0m[0mdebug[0m] [0m[0m      .find(_.name == VariantSchemas.genotypesFieldName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .get[0m
[0m[[0m[0mdebug[0m] [0m[0m      .dataType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[ArrayType][0m
[0m[[0m[0mdebug[0m] [0m[0m      .elementType[0m
[0m[[0m[0mdebug[0m] [0m[0m      .asInstanceOf[StructType][0m
[0m[[0m[0mdebug[0m] [0m[0m      .exists(_.name == VariantSchemas.sampleIdField.name)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("schema does not include sample id field if there are no ids") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testRoot/example.16bits.nosampleids.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!hasSampleIdField(df.schema))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("schema includes sample id if at least one file has ids") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testRoot/example.16bits*.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(hasSampleIdField(df.schema))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("schema includes sample id if sample id file is provided") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleFilePath", s"$testRoot/example.16bits.oxford.sample")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option("sampleIdColumn", "ID_2")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testRoot/example.16bits.nosampleids.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(hasSampleIdField(df.schema))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("schema does not include sample ids if `includeSampleIds` is false") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val df = spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .read[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format(sourceName)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .option(CommonOptions.INCLUDE_SAMPLE_IDS, false)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .load(s"$testRoot/example.16bits*.bgen")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(!hasSampleIdField(df.schema))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.bgen[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.BgenRow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mtrait BgenConverterBaseTest extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  val testRoot = s"$testDataHome/bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // The error in a probability stored using the rounding rule is 1/(2**B - 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  def checkBgenRowsEqual([0m
[0m[[0m[0mdebug[0m] [0m[0m      trueRow: BgenRow,[0m
[0m[[0m[0mdebug[0m] [0m[0m      testRow: BgenRow,[0m
[0m[[0m[0mdebug[0m] [0m[0m      strict: Boolean,[0m
[0m[[0m[0mdebug[0m] [0m[0m      bitsPerProb: Int): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tolGivenBitsPerProb = 1.0 / ((1L << bitsPerProb) - 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (strict) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(trueRow.names == testRow.names)[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      // QCTools incorrectly separates IDs with commas instead of semicolons when exporting to VCF[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert([0m
[0m[[0m[0mdebug[0m] [0m[0m        trueRow.names.flatMap(_.split(",")).filter(n => n.nonEmpty && n != ".").distinct.sorted ==[0m
[0m[[0m[0mdebug[0m] [0m[0m        testRow.names.flatMap(_.split(",")).filter(n => n.nonEmpty && n != ".").distinct.sorted[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(trueRow.copy(names = Nil, genotypes = Nil) == testRow.copy(names = Nil, genotypes = Nil))[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(trueRow.genotypes.length == testRow.genotypes.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m    trueRow.genotypes.zip(testRow.genotypes).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m      case (oGt, nGt) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m        assert([0m
[0m[[0m[0mdebug[0m] [0m[0m          oGt.sampleId == nGt.sampleId || (oGt.sampleId.isEmpty && nGt[0m
[0m[[0m[0mdebug[0m] [0m[0m            .sampleId[0m
[0m[[0m[0mdebug[0m] [0m[0m            .get[0m
[0m[[0m[0mdebug[0m] [0m[0m            .startsWith("NA"))[0m
[0m[[0m[0mdebug[0m] [0m[0m        )[0m
[0m[[0m[0mdebug[0m] [0m[0m        if (oGt.posteriorProbabilities.nonEmpty || nGt.posteriorProbabilities.nonEmpty) {[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(oGt.phased == nGt.phased)[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(oGt.ploidy == nGt.ploidy)[0m
[0m[[0m[0mdebug[0m] [0m[0m          assert(oGt.posteriorProbabilities.length == nGt.posteriorProbabilities.length)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m          oGt.posteriorProbabilities.zip(nGt.posteriorProbabilities).foreach {[0m
[0m[[0m[0mdebug[0m] [0m[0m            case (oPp, nPp) =>[0m
[0m[[0m[0mdebug[0m] [0m[0m              if (strict) {[0m
[0m[[0m[0mdebug[0m] [0m[0m                assert(oPp ~== nPp absTol tolGivenBitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m              } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m                // Account for truncation in VCF text representation[0m
[0m[[0m[0mdebug[0m] [0m[0m                val precision = math.min([0m
[0m[[0m[0mdebug[0m] [0m[0m                  oPp.toString.split(".").lift(1).map(_.length).getOrElse(0),[0m
[0m[[0m[0mdebug[0m] [0m[0m                  nPp.toString.split(".").lift(1).map(_.length).getOrElse(0)[0m
[0m[[0m[0mdebug[0m] [0m[0m                )[0m
[0m[[0m[0mdebug[0m] [0m[0m                val impO =[0m
[0m[[0m[0mdebug[0m] [0m[0m                  BigDecimal(oPp).setScale(precision, BigDecimal.RoundingMode.HALF_UP).toDouble[0m
[0m[[0m[0mdebug[0m] [0m[0m                val impN =[0m
[0m[[0m[0mdebug[0m] [0m[0m                  BigDecimal(nPp).setScale(precision, BigDecimal.RoundingMode.HALF_UP).toDouble[0m
[0m[[0m[0mdebug[0m] [0m[0m                assert(impO ~== impN absTol tolGivenBitsPerProb)[0m
[0m[[0m[0mdebug[0m] [0m[0m              }[0m
[0m[[0m[0mdebug[0m] [0m[0m          }[0m
[0m[[0m[0mdebug[0m] [0m[0m        }[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.sql[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.{Files, Path}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m// Sanity check that legacy DataSource names starting with "com.databricks." still work[0m
[0m[[0m[0mdebug[0m] [0m[0mclass ComDatabricksDataSourceSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val vcf = s"$testDataHome/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.vcf"[0m
[0m[[0m[0mdebug[0m] [0m[0m  lazy val bgen = s"$testDataHome/bgen/example.16bits.bgen"[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def createTempPath(extension: String): Path = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val tempDir = Files.createTempDirectory(s"test-$extension-dir")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val path = tempDir.resolve(s"test.$extension")[0m
[0m[[0m[0mdebug[0m] [0m[0m    logger.info(s"Writing $extension to path ${path.toAbsolutePath.toString}")[0m
[0m[[0m[0mdebug[0m] [0m[0m    path[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  case class DataSources(legacyDataSource: String, standardDataSource: String, file: String)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Legacy read DataSource, standard read DataSource, file[0m
[0m[[0m[0mdebug[0m] [0m[0m  val readDataSources: Seq[DataSources] = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    DataSources("com.databricks.vcf", "vcf", vcf),[0m
[0m[[0m[0mdebug[0m] [0m[0m    DataSources("com.databricks.bgen", "bgen", bgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("read")(readDataSources) { rds =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val legacyDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format(rds.legacyDataSource).load(rds.file).orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val standardDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format(rds.standardDataSource).load(rds.file).orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(legacyDf.collect sameElements standardDf.collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  // Legacy write source, standard read DataSource, file[0m
[0m[[0m[0mdebug[0m] [0m[0m  val writeDataSources: Seq[DataSources] = Seq([0m
[0m[[0m[0mdebug[0m] [0m[0m    DataSources("com.databricks.vcf", "vcf", vcf),[0m
[0m[[0m[0mdebug[0m] [0m[0m    DataSources("com.databricks.bigvcf", "vcf", vcf),[0m
[0m[[0m[0mdebug[0m] [0m[0m    DataSources("com.databricks.bigbgen", "bgen", bgen)[0m
[0m[[0m[0mdebug[0m] [0m[0m  )[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  gridTest("write")(writeDataSources) { wds =>[0m
[0m[[0m[0mdebug[0m] [0m[0m    val inputDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format(wds.standardDataSource).load(wds.file).orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenFile = createTempPath(wds.standardDataSource).toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    inputDf.write.format(wds.legacyDataSource).save(rewrittenFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    val rewrittenDf =[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark.read.format(wds.standardDataSource).load(rewrittenFile).orderBy("contigName", "start")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(inputDf.collect sameElements rewrittenDf.collect)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.sql[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport htsjdk.samtools.util.Log[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.SparkSession[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.test.SharedSparkSession[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.{DebugFilesystem, SparkConf}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.scalatest.concurrent.{AbstractPatienceConfiguration, Eventually}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.scalatest.time.{Milliseconds, Seconds, Span}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.scalatest.{Args, FunSuite, Status, Tag}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.Glow[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.common.{GlowLogging, TestUtils}[0m
[0m[[0m[0mdebug[0m] [0m[0mimport io.projectglow.sql.util.BGZFCodec[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mabstract class GlowBaseTest[0m
[0m[[0m[0mdebug[0m] [0m[0m    extends FunSuite[0m
[0m[[0m[0mdebug[0m] [0m[0m    with SharedSparkSession[0m
[0m[[0m[0mdebug[0m] [0m[0m    with GlowLogging[0m
[0m[[0m[0mdebug[0m] [0m[0m    with GlowTestData[0m
[0m[[0m[0mdebug[0m] [0m[0m    with TestUtils[0m
[0m[[0m[0mdebug[0m] [0m[0m    with JenkinsTestPatience {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override protected def sparkConf: SparkConf = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    super[0m
[0m[[0m[0mdebug[0m] [0m[0m      .sparkConf[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.driver.maxResultSize", "0")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.kryo.registrator", "org.broadinstitute.hellbender.engine.spark.GATKRegistrator")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.kryoserializer.buffer.max", "2047m")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set("spark.kryo.registrationRequired", "false")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .set([0m
[0m[[0m[0mdebug[0m] [0m[0m        "spark.hadoop.io.compression.codecs",[0m
[0m[[0m[0mdebug[0m] [0m[0m        classOf[BGZFCodec].getCanonicalName[0m
[0m[[0m[0mdebug[0m] [0m[0m      )[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def initializeSession(): Unit = ()[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override protected implicit def spark: SparkSession = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val sess = SparkSession.builder().config(sparkConf).master("local[2]").getOrCreate()[0m
[0m[[0m[0mdebug[0m] [0m[0m    Glow.register(sess)[0m
[0m[[0m[0mdebug[0m] [0m[0m    SparkSession.setActiveSession(sess)[0m
[0m[[0m[0mdebug[0m] [0m[0m    Log.setGlobalLogLevel(Log.LogLevel.ERROR)[0m
[0m[[0m[0mdebug[0m] [0m[0m    sess[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  protected def gridTest[A](testNamePrefix: String, testTags: Tag*)(params: Seq[A])([0m
[0m[[0m[0mdebug[0m] [0m[0m      testFun: A => Unit): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    for (param <- params) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      test(testNamePrefix + s" ($param)", testTags: _*)(testFun(param))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def afterEach(): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    DebugFilesystem.assertNoOpenStreams()[0m
[0m[[0m[0mdebug[0m] [0m[0m    eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sparkContext.getPersistentRDDs.isEmpty)[0m
[0m[[0m[0mdebug[0m] [0m[0m      assert(spark.sharedState.cacheManager.isEmpty, "Cache not empty.")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    super.afterEach()[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def runTest(testName: String, args: Args): Status = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    logger.info(s"Running test '$testName'")[0m
[0m[[0m[0mdebug[0m] [0m[0m    val res = super.runTest(testName, args)[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (res.succeeds()) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      logger.info(s"Done running test '$testName'")[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      logger.info(s"Done running test '$testName' with a failure")[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    res[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m/**[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unit-test patience config to use with unit tests that use scala test's eventually and other[0m
[0m[[0m[0mdebug[0m] [0m[0m * asynchronous checks. This will override the default timeout and check interval so they are[0m
[0m[[0m[0mdebug[0m] [0m[0m * more likely to pass in highly loaded CI environments.[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0mtrait JenkinsTestPatience extends AbstractPatienceConfiguration with Eventually {[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  /**[0m
[0m[[0m[0mdebug[0m] [0m[0m   * The total timeout to wait for `eventually` blocks to succeed[0m
[0m[[0m[0mdebug[0m] [0m[0m   */[0m
[0m[[0m[0mdebug[0m] [0m[0m  final override implicit val patienceConfig: PatienceConfig =[0m
[0m[[0m[0mdebug[0m] [0m[0m    if (sys.env.get("JENKINS_HOST").nonEmpty) {[0m
[0m[[0m[0mdebug[0m] [0m[0m      // increase the timeout on jenkins where parallelizing causes things to be very slow[0m
[0m[[0m[0mdebug[0m] [0m[0m      PatienceConfig(Span(10, Seconds), Span(50, Milliseconds))[0m
[0m[[0m[0mdebug[0m] [0m[0m    } else {[0m
[0m[[0m[0mdebug[0m] [0m[0m      // use the default timeout on local machines so failures don't hang for a long time[0m
[0m[[0m[0mdebug[0m] [0m[0m      PatienceConfig(Span(5, Seconds), Span(15, Milliseconds))[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.sql[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Files[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.hadoop.conf.Configuration[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.rdd.RDD[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass SingleFileWriterSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("uses service loader") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outDir = Files.createTempDirectory("writer")[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(DummyFileUploader.counter == 0)[0m
[0m[[0m[0mdebug[0m] [0m[0m    SingleFileWriter.write(sparkContext.emptyRDD[Array[Byte]], outDir.resolve("monkey").toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(DummyFileUploader.counter == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m    SingleFileWriter.write(sparkContext.emptyRDD[Array[Byte]], outDir.resolve("orangutan").toString)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(DummyFileUploader.counter == 1)[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyFileUploader extends BigFileUploader {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def canUpload(conf: Configuration, path: String): Boolean = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    path.contains("monkey")[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def upload(bytes: RDD[Array[Byte]], path: String): Unit = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    DummyFileUploader.counter += 1[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mobject DummyFileUploader {[0m
[0m[[0m[0mdebug[0m] [0m[0m  var counter = 0[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.sql[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.{Files, Paths}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.rdd.RDD[0m
[0m[[0m[0mdebug[0m] [0m[0mimport org.apache.spark.sql.{DataFrame, SaveMode}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass BigFileDatasourceSuite extends GlowBaseTest {[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("save mode: append") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outFile = Files.createTempFile("tmp", ".tmp").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[RuntimeException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .mode(SaveMode.Append)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format("io.projectglow.sql.DummyBigFileDatasource")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(outFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert([0m
[0m[[0m[0mdebug[0m] [0m[0m      e.getMessage[0m
[0m[[0m[0mdebug[0m] [0m[0m        .contains("Append mode is not supported by io.projectglow.sql.DummyBigFileDatasource"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("save mode: overwrite") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outDir = Files.createTempDirectory("tmp").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .mode(SaveMode.Overwrite)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("io.projectglow.sql.DummyBigFileDatasource")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(outDir)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val filePath = Paths.get(outDir)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(Files.isRegularFile(filePath))[0m
[0m[[0m[0mdebug[0m] [0m[0m    val writtenBytes = Files.readAllBytes(filePath)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(writtenBytes.toSeq == Seq(0, 1, 2).map(_.toByte))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("save mode: error if exists") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outFile = Files.createTempFile("tmp", ".tmp").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    val e = intercept[RuntimeException] {[0m
[0m[[0m[0mdebug[0m] [0m[0m      spark[0m
[0m[[0m[0mdebug[0m] [0m[0m        .emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m        .write[0m
[0m[[0m[0mdebug[0m] [0m[0m        .mode(SaveMode.ErrorIfExists)[0m
[0m[[0m[0mdebug[0m] [0m[0m        .format("io.projectglow.sql.DummyBigFileDatasource")[0m
[0m[[0m[0mdebug[0m] [0m[0m        .save(outFile)[0m
[0m[[0m[0mdebug[0m] [0m[0m    }[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(e.getMessage.contains(s"Path $outFile already exists"))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m  test("save mode: ignore") {[0m
[0m[[0m[0mdebug[0m] [0m[0m    val outDir = Files.createTempDirectory("tmp").toString[0m
[0m[[0m[0mdebug[0m] [0m[0m    spark[0m
[0m[[0m[0mdebug[0m] [0m[0m      .emptyDataFrame[0m
[0m[[0m[0mdebug[0m] [0m[0m      .write[0m
[0m[[0m[0mdebug[0m] [0m[0m      .mode(SaveMode.Ignore)[0m
[0m[[0m[0mdebug[0m] [0m[0m      .format("io.projectglow.sql.DummyBigFileDatasource")[0m
[0m[[0m[0mdebug[0m] [0m[0m      .save(outDir)[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    val dirPath = Paths.get(outDir)[0m
[0m[[0m[0mdebug[0m] [0m[0m    assert(Files.isDirectory(dirPath))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mclass DummyBigFileDatasource extends BigFileDatasource {[0m
[0m[[0m[0mdebug[0m] [0m[0m  override def serializeDataFrame([0m
[0m[[0m[0mdebug[0m] [0m[0m      options: Map[String, String],[0m
[0m[[0m[0mdebug[0m] [0m[0m      data: DataFrame): RDD[Array[Byte]] = {[0m
[0m[[0m[0mdebug[0m] [0m[0m    data.sqlContext.sparkContext.parallelize(Seq(Array(0, 1, 2).map(_.toByte)))[0m
[0m[[0m[0mdebug[0m] [0m[0m  }[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
[0m[[0m[0mdebug[0m] [0m[0mFirst line of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mText of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0m/*[0m
[0m[[0m[0mdebug[0m] [0m[0m * Copyright 2019 The Glow Authors[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Licensed under the Apache License, Version 2.0 (the "License");[0m
[0m[[0m[0mdebug[0m] [0m[0m * you may not use this file except in compliance with the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m * You may obtain a copy of the License at[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m *     http://www.apache.org/licenses/LICENSE-2.0[0m
[0m[[0m[0mdebug[0m] [0m[0m *[0m
[0m[[0m[0mdebug[0m] [0m[0m * Unless required by applicable law or agreed to in writing, software[0m
[0m[[0m[0mdebug[0m] [0m[0m * distributed under the License is distributed on an "AS IS" BASIS,[0m
[0m[[0m[0mdebug[0m] [0m[0m * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.[0m
[0m[[0m[0mdebug[0m] [0m[0m * See the License for the specific language governing permissions and[0m
[0m[[0m[0mdebug[0m] [0m[0m * limitations under the License.[0m
[0m[[0m[0mdebug[0m] [0m[0m */[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mpackage io.projectglow.sql[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mimport java.nio.file.Paths[0m
[0m[[0m[0mdebug[0m] [0m[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mtrait GlowTestData {[0m
[0m[[0m[0mdebug[0m] [0m[0m  final lazy val testDataHome = Paths[0m
[0m[[0m[0mdebug[0m] [0m[0m    .get([0m
[0m[[0m[0mdebug[0m] [0m[0m      sys.props.getOrElse("test.dir", ""),[0m
[0m[[0m[0mdebug[0m] [0m[0m      "test-data"[0m
[0m[[0m[0mdebug[0m] [0m[0m    )[0m
[0m[[0m[0mdebug[0m] [0m[0m    .toString[0m
[0m[[0m[0mdebug[0m] [0m[0m}[0m
[0m[[0m[0mdebug[0m] [0m[0mModified text of file is:[0m
[0m[[0m[0mdebug[0m] [0m[0mNone[0m
